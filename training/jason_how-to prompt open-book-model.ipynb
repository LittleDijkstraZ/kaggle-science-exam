{"cells":[{"cell_type":"markdown","metadata":{},"source":["# How To Train Model for Open Book Q&A Technique\n","In this notebook we demonstrate how to train a model to be used with top scoring Open Book Q&A method. The Open Book method was first presented by JJ (@jjinho) [here][1], then Quangteo (@quangbk) improved RAM usage [here][2], and Anil (@nlztrk) combined with Q&A [here][3]. Radek (@radek1) demonstrated the strength of Q&A [here][5]. Next Mgoksu (@mgoksu) demonstrated how to achieve top public LB=0.807 using this method [here][4] by finetuning DeBerta large on this method.\n","\n","In order to train a model for use with Open Book Q&A, we need a CSV that contains; `prompt` (i.e. question), `A, B, C, D, E` (i.e. answer choices), and we need a column of `context` extracted from wikipedia pages for each question. To generate the `context` column, we run Mgoksu's notebook [here][4]. In code cell #5, we load our CSV without `context` column with code `trn = pd.read_csv(OUR_DATASET.CSV)`. Then in code cell #21 our dataset is saved to disk as `test_context.csv` with the column `context` added.\n","\n","I have searched and concatenated all publicly shared datasets into one 60k CSV and then ran Mgoksu's notebook with `NUM_TITLES_INCLUDE = 5` and `NUM_SENTENCES_INCLUDE = 20`. This added an additional `context` column. I uploaded the resultant CSV file to a Kaggle dataset [here][6]. If you enjoy the notebook you are reading, please upvote the dataset too. Thanks! \n","\n","![](https://miro.medium.com/v2/resize:fit:800/format:webp/1*bTGY3fKIgNefQxNsOYpnBw.png)\n"," \n","(image source [here][7])\n","\n","[1]: https://www.kaggle.com/code/jjinho/open-book-llm-science-exam\n","[2]: https://www.kaggle.com/code/quangbk/open-book-llm-science-exam-reduced-ram-usage\n","[3]: https://www.kaggle.com/code/nlztrk/openbook-debertav3-large-baseline-single-model\n","[4]: https://www.kaggle.com/code/mgoksu/0-807-sharing-my-trained-with-context-model\n","[5]: https://www.kaggle.com/code/radek1/new-dataset-deberta-v3-large-training\n","[6]: https://www.kaggle.com/datasets/cdeotte/60k-data-with-context-v2\n","[7]: https://blog.gopenai.com/enrich-llms-with-retrieval-augmented-generation-rag-17b82a96b6f0"]},{"cell_type":"markdown","metadata":{},"source":["# Load CSV\n","We will load 60k CSV of `prompts`, `A,B,C,D,E`, and `context` from my Kaggle dataset [here][1]. This dataset is all publicly shared datasets concatenated then processed with Mgoksu's notebook [here][2] to create a `context` column. (To learn more about the datasets within read my discussion post). This Kaggle dataset also contains competition `train.csv` with added `context` column (to be used as a validation dataset).\n","\n","In this train notebook, we have internet turned on and can choose whatever model we wish to download and train. After we finetune this model, we will create a second notebook with the Open Book Q&A technique and load the finetuned model from the output of this notebook. The second notebook will have internet turned off so that it can be submitted to Kaggle's competition.\n","\n","[1]: https://www.kaggle.com/datasets/cdeotte/60k-data-with-context-v2\n","[2]: https://www.kaggle.com/code/mgoksu/0-807-sharing-my-trained-with-context-model"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2023-09-10T15:51:09.289992Z","iopub.status.busy":"2023-09-10T15:51:09.289234Z","iopub.status.idle":"2023-09-10T15:51:09.301973Z","shell.execute_reply":"2023-09-10T15:51:09.300654Z","shell.execute_reply.started":"2023-09-10T15:51:09.289938Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["VER: 18_256\n"]}],"source":["%load_ext autoreload\n","%autoreload 2\n","import os\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","\n","from typing import Optional, Union\n","import pandas as pd, numpy as np, torch\n","from datasets import Dataset # 这个的使用方法 hugging face 上面有教程\n","from dataclasses import dataclass\n","from transformers import AutoTokenizer\n","from transformers import EarlyStoppingCallback\n","from transformers.tokenization_utils_base import PreTrainedTokenizerBase, PaddingStrategy\n","from transformers import AutoModelForMultipleChoice, TrainingArguments, Trainer\n","\n","# TRAIN WITH SUBSET OF 60K\n","# NUM_TRAIN_SAMPLES = 1_024\n","NUM_TRAIN_SAMPLES = None\n","\n","# PARAMETER EFFICIENT FINE TUNING\n","# PEFT REQUIRES 1XP100 GPU NOT 2XT4\n","USE_PEFT = True\n","# USE_PEFT = True # 这个的全称是 pretrained efficient finetuning, hugging face 上面有教程\n","\n","# NUMBER OF LAYERS TO FREEZE \n","# DEBERTA LARGE HAS TOTAL OF 24 LAYERS\n","\n","# FREEZE_LAYERS = 18\n","# FREEZE_LAYERS = 24\n","# FREEZE_LAYERS = 22\n","# FREEZE_LAYERS = 12\n","FREEZE_LAYERS = 18\n","# FREEZE_LAYERS = 20\n","\n","\n","\n","\n","\n","# BOOLEAN TO FREEZE EMBEDDINGS\n","FREEZE_EMBEDDINGS = True\n","# FREEZE_EMBEDDINGS = False\n","\n","# LENGTH OF CONTEXT PLUS QUESTION ANSWER\n","# 我需要搞懂这个长度到底指的是什么，尤其是 context 和 question 的长度是怎么分配的。256 不可能 cover 全部。\n","# 因为如果模型没能在足够长的 input 中训练，那么positional encoding 很差的模型就不好 extrapolate\n","# MAX_INPUT = 256\n","MAX_INPUT = 256 # jason 的假设： 短的 input 会让模型尝试记忆 fintune 的数据，而不是基于 retrieved prompt 做出推断?\n","# MAX_INPUT = 512\n","# MAX_INPUT = 768 # 调整这个的大小的时候，每次都需要重新跑一下dataset\n","\n","# HUGGING FACE MODEL\n","# MODEL = 'microsoft/deberta-v3-large' # 0.75 -> 0.89\n","# MODEL = \"sileod/deberta-v3-large-tasksource-nli\" 0.75 -> 0.85\n","# MODEL = \"domenicrosati/deberta-v3-large-dapt-tapt-scientific-papers-pubmed-finetuned-DAGPap22\" 0.\n","# MODEL = \"deepset/roberta-base-squad2\" # 0.4 -> 0.70 @ 450\n","# MODEL = \"bert-large-cased-whole-word-masking-finetuned-squad\" # 0.4 -> 0.6 @ 80\n","# MODEL = \"Intel/bert-large-uncased-squadv1.1-sparse-90-unstructured\" # 0.4 -> 0.65 @ 300\n","# MODEL = \"OpenAssistant/reward-model-deberta-v3-large-v2\" 0.78 -> 0.88\n","# MODEL = \"google/flan-t5-xxl\"\n","MODEL = \"\"\n","# MODEL = \"ccore/Llama2-330m-32k-Rhetorical-Agents-QA-Builder\"\n","VER=f'{FREEZE_LAYERS}_{MAX_INPUT}'\n","print(f'VER: {VER}')\n","\n","checkpoint_folder = MODEL.split('/')[-1] + '_checkpoints'\n","dataset_folder = MODEL.split('/')[-1] + '_datasets'"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2023-09-10T15:50:47.929122Z","iopub.status.busy":"2023-09-10T15:50:47.928767Z","iopub.status.idle":"2023-09-10T15:50:48.010881Z","shell.execute_reply":"2023-09-10T15:50:48.009882Z","shell.execute_reply.started":"2023-09-10T15:50:47.929092Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Validation data size: (200, 8)\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>prompt</th>\n","      <th>context</th>\n","      <th>A</th>\n","      <th>B</th>\n","      <th>C</th>\n","      <th>D</th>\n","      <th>E</th>\n","      <th>answer</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Which of the following statements accurately d...</td>\n","      <td>The presence of a clustered thick disk-like co...</td>\n","      <td>MOND is a theory that reduces the observed mis...</td>\n","      <td>MOND is a theory that increases the discrepanc...</td>\n","      <td>MOND is a theory that explains the missing bar...</td>\n","      <td>MOND is a theory that reduces the discrepancy ...</td>\n","      <td>MOND is a theory that eliminates the observed ...</td>\n","      <td>D</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Which of the following is an accurate definiti...</td>\n","      <td>Many of these systems evolve in a self-similar...</td>\n","      <td>Dynamic scaling refers to the evolution of sel...</td>\n","      <td>Dynamic scaling refers to the non-evolution of...</td>\n","      <td>Dynamic scaling refers to the evolution of sel...</td>\n","      <td>Dynamic scaling refers to the non-evolution of...</td>\n","      <td>Dynamic scaling refers to the evolution of sel...</td>\n","      <td>A</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Which of the following statements accurately d...</td>\n","      <td>It is possible that this usage is related with...</td>\n","      <td>The triskeles symbol was reconstructed as a fe...</td>\n","      <td>The triskeles symbol is a representation of th...</td>\n","      <td>The triskeles symbol is a representation of a ...</td>\n","      <td>The triskeles symbol represents three interloc...</td>\n","      <td>The triskeles symbol is a representation of th...</td>\n","      <td>A</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>What is the significance of regularization in ...</td>\n","      <td>Renormalization is distinct from regularizatio...</td>\n","      <td>Regularizing the mass-energy of an electron wi...</td>\n","      <td>Regularizing the mass-energy of an electron wi...</td>\n","      <td>Regularizing the mass-energy of an electron wi...</td>\n","      <td>Regularizing the mass-energy of an electron wi...</td>\n","      <td>Regularizing the mass-energy of an electron wi...</td>\n","      <td>C</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Which of the following statements accurately d...</td>\n","      <td>Several qualitative observations can be made o...</td>\n","      <td>The angular spacing of features in the diffrac...</td>\n","      <td>The angular spacing of features in the diffrac...</td>\n","      <td>The angular spacing of features in the diffrac...</td>\n","      <td>The angular spacing of features in the diffrac...</td>\n","      <td>The angular spacing of features in the diffrac...</td>\n","      <td>D</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>195</th>\n","      <td>What is the relation between the three moment ...</td>\n","      <td>The second equation is more general as it does...</td>\n","      <td>The three moment theorem expresses the relatio...</td>\n","      <td>The three moment theorem is used to calculate ...</td>\n","      <td>The three moment theorem describes the relatio...</td>\n","      <td>The three moment theorem is used to calculate ...</td>\n","      <td>The three moment theorem is used to derive the...</td>\n","      <td>C</td>\n","    </tr>\n","    <tr>\n","      <th>196</th>\n","      <td>What is the throttling process, and why is it ...</td>\n","      <td>A throttle is the mechanism by which fluid flo...</td>\n","      <td>The throttling process is a steady flow of a f...</td>\n","      <td>The throttling process is a steady adiabatic f...</td>\n","      <td>The throttling process is a steady adiabatic f...</td>\n","      <td>The throttling process is a steady flow of a f...</td>\n","      <td>The throttling process is a steady adiabatic f...</td>\n","      <td>B</td>\n","    </tr>\n","    <tr>\n","      <th>197</th>\n","      <td>What happens to excess base metal as a solutio...</td>\n","      <td>Furthermore, this melting may begin at a tempe...</td>\n","      <td>The excess base metal will often solidify, bec...</td>\n","      <td>The excess base metal will often crystallize-o...</td>\n","      <td>The excess base metal will often dissolve, bec...</td>\n","      <td>The excess base metal will often liquefy, beco...</td>\n","      <td>The excess base metal will often evaporate, be...</td>\n","      <td>B</td>\n","    </tr>\n","    <tr>\n","      <th>198</th>\n","      <td>What is the relationship between mass, force, ...</td>\n","      <td>Newton first set out the definition of mass Th...</td>\n","      <td>Mass is a property that determines the weight ...</td>\n","      <td>Mass is an inertial property that determines a...</td>\n","      <td>Mass is an inertial property that determines a...</td>\n","      <td>Mass is an inertial property that determines a...</td>\n","      <td>Mass is a property that determines the size of...</td>\n","      <td>D</td>\n","    </tr>\n","    <tr>\n","      <th>199</th>\n","      <td>What did Arthur Eddington discover about two o...</td>\n","      <td>Eddington's criticism seems to have been based...</td>\n","      <td>Arthur Eddington showed that two of Einstein's...</td>\n","      <td>Arthur Eddington showed that two of Einstein's...</td>\n","      <td>Arthur Eddington showed that two of Einstein's...</td>\n","      <td>Arthur Eddington showed that two of Einstein's...</td>\n","      <td>Arthur Eddington showed that two of Einstein's...</td>\n","      <td>C</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>200 rows × 8 columns</p>\n","</div>"],"text/plain":["                                                prompt  \\\n","0    Which of the following statements accurately d...   \n","1    Which of the following is an accurate definiti...   \n","2    Which of the following statements accurately d...   \n","3    What is the significance of regularization in ...   \n","4    Which of the following statements accurately d...   \n","..                                                 ...   \n","195  What is the relation between the three moment ...   \n","196  What is the throttling process, and why is it ...   \n","197  What happens to excess base metal as a solutio...   \n","198  What is the relationship between mass, force, ...   \n","199  What did Arthur Eddington discover about two o...   \n","\n","                                               context  \\\n","0    The presence of a clustered thick disk-like co...   \n","1    Many of these systems evolve in a self-similar...   \n","2    It is possible that this usage is related with...   \n","3    Renormalization is distinct from regularizatio...   \n","4    Several qualitative observations can be made o...   \n","..                                                 ...   \n","195  The second equation is more general as it does...   \n","196  A throttle is the mechanism by which fluid flo...   \n","197  Furthermore, this melting may begin at a tempe...   \n","198  Newton first set out the definition of mass Th...   \n","199  Eddington's criticism seems to have been based...   \n","\n","                                                     A  \\\n","0    MOND is a theory that reduces the observed mis...   \n","1    Dynamic scaling refers to the evolution of sel...   \n","2    The triskeles symbol was reconstructed as a fe...   \n","3    Regularizing the mass-energy of an electron wi...   \n","4    The angular spacing of features in the diffrac...   \n","..                                                 ...   \n","195  The three moment theorem expresses the relatio...   \n","196  The throttling process is a steady flow of a f...   \n","197  The excess base metal will often solidify, bec...   \n","198  Mass is a property that determines the weight ...   \n","199  Arthur Eddington showed that two of Einstein's...   \n","\n","                                                     B  \\\n","0    MOND is a theory that increases the discrepanc...   \n","1    Dynamic scaling refers to the non-evolution of...   \n","2    The triskeles symbol is a representation of th...   \n","3    Regularizing the mass-energy of an electron wi...   \n","4    The angular spacing of features in the diffrac...   \n","..                                                 ...   \n","195  The three moment theorem is used to calculate ...   \n","196  The throttling process is a steady adiabatic f...   \n","197  The excess base metal will often crystallize-o...   \n","198  Mass is an inertial property that determines a...   \n","199  Arthur Eddington showed that two of Einstein's...   \n","\n","                                                     C  \\\n","0    MOND is a theory that explains the missing bar...   \n","1    Dynamic scaling refers to the evolution of sel...   \n","2    The triskeles symbol is a representation of a ...   \n","3    Regularizing the mass-energy of an electron wi...   \n","4    The angular spacing of features in the diffrac...   \n","..                                                 ...   \n","195  The three moment theorem describes the relatio...   \n","196  The throttling process is a steady adiabatic f...   \n","197  The excess base metal will often dissolve, bec...   \n","198  Mass is an inertial property that determines a...   \n","199  Arthur Eddington showed that two of Einstein's...   \n","\n","                                                     D  \\\n","0    MOND is a theory that reduces the discrepancy ...   \n","1    Dynamic scaling refers to the non-evolution of...   \n","2    The triskeles symbol represents three interloc...   \n","3    Regularizing the mass-energy of an electron wi...   \n","4    The angular spacing of features in the diffrac...   \n","..                                                 ...   \n","195  The three moment theorem is used to calculate ...   \n","196  The throttling process is a steady flow of a f...   \n","197  The excess base metal will often liquefy, beco...   \n","198  Mass is an inertial property that determines a...   \n","199  Arthur Eddington showed that two of Einstein's...   \n","\n","                                                     E answer  \n","0    MOND is a theory that eliminates the observed ...      D  \n","1    Dynamic scaling refers to the evolution of sel...      A  \n","2    The triskeles symbol is a representation of th...      A  \n","3    Regularizing the mass-energy of an electron wi...      C  \n","4    The angular spacing of features in the diffrac...      D  \n","..                                                 ...    ...  \n","195  The three moment theorem is used to derive the...      C  \n","196  The throttling process is a steady adiabatic f...      B  \n","197  The excess base metal will often evaporate, be...      B  \n","198  Mass is a property that determines the size of...      D  \n","199  Arthur Eddington showed that two of Einstein's...      C  \n","\n","[200 rows x 8 columns]"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["df_valid = pd.read_csv('../input/60k-data-with-context-v2/train_with_context2.csv')\n","print('Validation data size:', df_valid.shape )\n","df_valid"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2023-09-10T15:52:03.937771Z","iopub.status.busy":"2023-09-10T15:52:03.936994Z","iopub.status.idle":"2023-09-10T15:52:11.860278Z","shell.execute_reply":"2023-09-10T15:52:11.859288Z","shell.execute_reply.started":"2023-09-10T15:52:03.937732Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["size of dataset 60347\n","Train data size: (60347, 8)\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>prompt</th>\n","      <th>context</th>\n","      <th>A</th>\n","      <th>B</th>\n","      <th>C</th>\n","      <th>D</th>\n","      <th>E</th>\n","      <th>answer</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>In relation to Eunice Fay McKenzie's career, w...</td>\n","      <td>Eunice Fay McKenzie (February 19, 1918 – April...</td>\n","      <td>McKenzie showcased her singing talents in nume...</td>\n","      <td>McKenzie is primarily remembered for her starr...</td>\n","      <td>McKenzie gained recognition for her role as a ...</td>\n","      <td>McKenzie's collaborations with director Blake ...</td>\n","      <td>McKenzie's successful career in sound films co...</td>\n","      <td>B</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>How does Modified Newtonian Dynamics (MOND) im...</td>\n","      <td>The presence of a clustered thick disk-like co...</td>\n","      <td>MOND is a theory that increases the discrepanc...</td>\n","      <td>MOND explains the missing baryonic mass in gal...</td>\n","      <td>MOND is a theory that reduces the observed mis...</td>\n","      <td>MOND is a theory that eliminates the observed ...</td>\n","      <td>MOND's impact on the observed missing baryonic...</td>\n","      <td>E</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Which of the following statements accurately d...</td>\n","      <td>Woody Hartman is a retired American soccer goa...</td>\n","      <td>Ray Montgomerie is a former footballer who pla...</td>\n","      <td>Ray Montgomerie is a former footballer who pla...</td>\n","      <td>Ray Montgomerie is a former footballer who pla...</td>\n","      <td>Ray Montgomerie is a former footballer who pla...</td>\n","      <td>Ray Montgomerie is a former footballer who pla...</td>\n","      <td>B</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>What is the significance of the Museum of the ...</td>\n","      <td>The Museum of the Occupation of Latvia () is a...</td>\n","      <td>The Museum of the Occupation of Latvia is a me...</td>\n","      <td>The Museum of the Occupation of Latvia showcas...</td>\n","      <td>The Museum of the Occupation of Latvia was est...</td>\n","      <td>The Museum of the Occupation of Latvia primari...</td>\n","      <td>The Museum of the Occupation of Latvia is a mu...</td>\n","      <td>C</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>What was the previous name of the Christian Sc...</td>\n","      <td>It was named the Evangelical School for the De...</td>\n","      <td>The Christian School for the Deaf (CSD)</td>\n","      <td>The Christian School for the Blind (CSB)</td>\n","      <td>The Evangelical School and Chapel for the Deaf...</td>\n","      <td>The Evangelical School for the Deaf (ESD)</td>\n","      <td>The Evangelical School for the Blind (ESB)</td>\n","      <td>D</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                              prompt  \\\n","0  In relation to Eunice Fay McKenzie's career, w...   \n","1  How does Modified Newtonian Dynamics (MOND) im...   \n","2  Which of the following statements accurately d...   \n","3  What is the significance of the Museum of the ...   \n","4  What was the previous name of the Christian Sc...   \n","\n","                                             context  \\\n","0  Eunice Fay McKenzie (February 19, 1918 – April...   \n","1  The presence of a clustered thick disk-like co...   \n","2  Woody Hartman is a retired American soccer goa...   \n","3  The Museum of the Occupation of Latvia () is a...   \n","4  It was named the Evangelical School for the De...   \n","\n","                                                   A  \\\n","0  McKenzie showcased her singing talents in nume...   \n","1  MOND is a theory that increases the discrepanc...   \n","2  Ray Montgomerie is a former footballer who pla...   \n","3  The Museum of the Occupation of Latvia is a me...   \n","4            The Christian School for the Deaf (CSD)   \n","\n","                                                   B  \\\n","0  McKenzie is primarily remembered for her starr...   \n","1  MOND explains the missing baryonic mass in gal...   \n","2  Ray Montgomerie is a former footballer who pla...   \n","3  The Museum of the Occupation of Latvia showcas...   \n","4           The Christian School for the Blind (CSB)   \n","\n","                                                   C  \\\n","0  McKenzie gained recognition for her role as a ...   \n","1  MOND is a theory that reduces the observed mis...   \n","2  Ray Montgomerie is a former footballer who pla...   \n","3  The Museum of the Occupation of Latvia was est...   \n","4  The Evangelical School and Chapel for the Deaf...   \n","\n","                                                   D  \\\n","0  McKenzie's collaborations with director Blake ...   \n","1  MOND is a theory that eliminates the observed ...   \n","2  Ray Montgomerie is a former footballer who pla...   \n","3  The Museum of the Occupation of Latvia primari...   \n","4          The Evangelical School for the Deaf (ESD)   \n","\n","                                                   E answer  \n","0  McKenzie's successful career in sound films co...      B  \n","1  MOND's impact on the observed missing baryonic...      E  \n","2  Ray Montgomerie is a former footballer who pla...      B  \n","3  The Museum of the Occupation of Latvia is a mu...      C  \n","4         The Evangelical School for the Blind (ESB)      D  "]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["df_train = pd.read_csv('../input/60k-data-with-context-v2/all_12_with_context2.csv')\n","print('size of dataset', len(df_train))\n","df_train = df_train.drop(columns=\"source\")\n","df_train = df_train.fillna('')\n","if NUM_TRAIN_SAMPLES:\n","    df_train = df_train.sample(NUM_TRAIN_SAMPLES) # taken NUM_TRAIN_SAMPLES of samples here\n","print('Train data size:', df_train.shape)\n","df_train.head()"]},{"cell_type":"markdown","metadata":{},"source":["# Dataloader\n","Code is from Radek's notebook [here][1] with modifications to the tokenization process.\n","\n","[1]: https://www.kaggle.com/code/radek1/new-dataset-deberta-v3-large-training"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2023-09-10T15:52:22.561293Z","iopub.status.busy":"2023-09-10T15:52:22.560935Z","iopub.status.idle":"2023-09-10T15:52:22.575102Z","shell.execute_reply":"2023-09-10T15:52:22.574170Z","shell.execute_reply.started":"2023-09-10T15:52:22.561265Z"},"trusted":true},"outputs":[],"source":["option_to_index = {option: idx for idx, option in enumerate('ABCDE')}\n","index_to_option = {v: k for k,v in option_to_index.items()}\n","\n","# 等于说训练的时候模型是可以看到context的，因此与预测保持一致\n","def preprocess(example):\n","    first_sentence = [ \"[CLS] \" + example['context'] ] * 5\n","    second_sentences = [\" #### \" + example['prompt'] + \" [SEP] \" + example[option] + \" [SEP]\" for option in 'ABCDE']\n","    tokenized_example = tokenizer(first_sentence, second_sentences, truncation='only_first',\n","                                  max_length=MAX_INPUT, add_special_tokens=False)\n","    tokenized_example['label'] = option_to_index[example['answer']]\n","\n","    return tokenized_example\n","\n","@dataclass\n","class DataCollatorForMultipleChoice:\n","    tokenizer: PreTrainedTokenizerBase\n","    padding: Union[bool, str, PaddingStrategy] = True\n","    max_length: Optional[int] = None\n","    pad_to_multiple_of: Optional[int] = None\n","    \n","    def __call__(self, features):\n","        label_name = 'label' if 'label' in features[0].keys() else 'labels'\n","        labels = [feature.pop(label_name) for feature in features]\n","        batch_size = len(features)\n","        num_choices = len(features[0]['input_ids'])\n","        flattened_features = [\n","            [{k: v[i] for k, v in feature.items()} for i in range(num_choices)] for feature in features\n","        ]\n","        flattened_features = sum(flattened_features, [])\n","        \n","        # Huggingface tokenizer padding\n","        batch = self.tokenizer.pad(\n","            flattened_features,\n","            padding=self.padding,\n","            max_length=self.max_length,\n","            pad_to_multiple_of=self.pad_to_multiple_of, # this is related to mixed precision training\n","            return_tensors='pt',\n","        )\n","        # batch = self.tokenizer(\n","        #     flattened_features,\n","        #     padding=self.padding,\n","        #     max_length=self.max_length,\n","        #     pad_to_multiple_of=self.pad_to_multiple_of,\n","        #     return_tensors='pt',\n","        # )\n","        batch = {k: v.view(batch_size, num_choices, -1) for k, v in batch.items()}\n","        batch['labels'] = torch.tensor(labels, dtype=torch.int64)\n","        return batch"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2023-09-10T15:52:29.894379Z","iopub.status.busy":"2023-09-10T15:52:29.894012Z","iopub.status.idle":"2023-09-10T15:52:31.881652Z","shell.execute_reply":"2023-09-10T15:52:31.880578Z","shell.execute_reply.started":"2023-09-10T15:52:29.894346Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2f31a73176904557b93b00606ba1e1b9","version_major":2,"version_minor":0},"text/plain":["Downloading (…)okenizer_config.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"49eea4d1ca1d474589f14d40f149ee5c","version_major":2,"version_minor":0},"text/plain":["Downloading spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e586e9f1dba6405b88062e6b33c345c9","version_major":2,"version_minor":0},"text/plain":["Downloading (…)/main/tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"89e37376ffd242899ef3e7598d32235a","version_major":2,"version_minor":0},"text/plain":["Downloading (…)cial_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["T5TokenizerFast(name_or_path='google/flan-t5-xxl', vocab_size=32100, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>', 'additional_special_tokens': ['<extra_id_0>', '<extra_id_1>', '<extra_id_2>', '<extra_id_3>', '<extra_id_4>', '<extra_id_5>', '<extra_id_6>', '<extra_id_7>', '<extra_id_8>', '<extra_id_9>', '<extra_id_10>', '<extra_id_11>', '<extra_id_12>', '<extra_id_13>', '<extra_id_14>', '<extra_id_15>', '<extra_id_16>', '<extra_id_17>', '<extra_id_18>', '<extra_id_19>', '<extra_id_20>', '<extra_id_21>', '<extra_id_22>', '<extra_id_23>', '<extra_id_24>', '<extra_id_25>', '<extra_id_26>', '<extra_id_27>', '<extra_id_28>', '<extra_id_29>', '<extra_id_30>', '<extra_id_31>', '<extra_id_32>', '<extra_id_33>', '<extra_id_34>', '<extra_id_35>', '<extra_id_36>', '<extra_id_37>', '<extra_id_38>', '<extra_id_39>', '<extra_id_40>', '<extra_id_41>', '<extra_id_42>', '<extra_id_43>', '<extra_id_44>', '<extra_id_45>', '<extra_id_46>', '<extra_id_47>', '<extra_id_48>', '<extra_id_49>', '<extra_id_50>', '<extra_id_51>', '<extra_id_52>', '<extra_id_53>', '<extra_id_54>', '<extra_id_55>', '<extra_id_56>', '<extra_id_57>', '<extra_id_58>', '<extra_id_59>', '<extra_id_60>', '<extra_id_61>', '<extra_id_62>', '<extra_id_63>', '<extra_id_64>', '<extra_id_65>', '<extra_id_66>', '<extra_id_67>', '<extra_id_68>', '<extra_id_69>', '<extra_id_70>', '<extra_id_71>', '<extra_id_72>', '<extra_id_73>', '<extra_id_74>', '<extra_id_75>', '<extra_id_76>', '<extra_id_77>', '<extra_id_78>', '<extra_id_79>', '<extra_id_80>', '<extra_id_81>', '<extra_id_82>', '<extra_id_83>', '<extra_id_84>', '<extra_id_85>', '<extra_id_86>', '<extra_id_87>', '<extra_id_88>', '<extra_id_89>', '<extra_id_90>', '<extra_id_91>', '<extra_id_92>', '<extra_id_93>', '<extra_id_94>', '<extra_id_95>', '<extra_id_96>', '<extra_id_97>', '<extra_id_98>', '<extra_id_99>']}, clean_up_tokenization_spaces=True)"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["# 每个 tokenizer 都是 pretrained model 在 pretraining 的时候使用的 \n","# 在 tokenizer 的 special tokens 中, 注意到 [CLS] 同时是 bos 和 cls token; cls_token 是为了让模型知道要做 classification 了 \n","tokenizer = AutoTokenizer.from_pretrained(MODEL)\n","tokenizer"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2023-09-10T15:55:08.821857Z","iopub.status.busy":"2023-09-10T15:55:08.821159Z","iopub.status.idle":"2023-09-10T15:55:09.689547Z","shell.execute_reply":"2023-09-10T15:55:09.688542Z","shell.execute_reply.started":"2023-09-10T15:55:08.821820Z"},"trusted":true},"outputs":[{"data":{"text/plain":["Dataset({\n","    features: ['prompt', 'context', 'A', 'B', 'C', 'D', 'E', 'answer'],\n","    num_rows: 60347\n","})"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["dataset_valid = Dataset.from_pandas(df_valid)\n","dataset = Dataset.from_pandas(df_train)\n","if '__index_level_0__' in dataset._info.features: # 加了这行防爆\n","    print('removing __index_level_0__')\n","    dataset = dataset.remove_columns([\"__index_level_0__\"])\n","dataset"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2023-09-10T15:55:13.711687Z","iopub.status.busy":"2023-09-10T15:55:13.711294Z","iopub.status.idle":"2023-09-10T15:55:36.510447Z","shell.execute_reply":"2023-09-10T15:55:36.508573Z","shell.execute_reply.started":"2023-09-10T15:55:13.711657Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f6d615d89a0a4919a10ae36d9bcb91d6","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/200 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["getting tokenized_dataset_256 from disk\n"]},{"data":{"text/plain":["Dataset({\n","    features: ['input_ids', 'token_type_ids', 'attention_mask', 'label'],\n","    num_rows: 60347\n","})"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["tokenized_dataset_valid = dataset_valid.map(preprocess, remove_columns=['prompt', 'context', 'A', 'B', 'C', 'D', 'E', 'answer'])\n","if os.path.exists(f'./{dataset_folder}/tokenized_dataset_{MAX_INPUT}'):\n","    print(f'getting tokenized_dataset_{MAX_INPUT} from disk')\n","    tokenized_dataset = Dataset.load_from_disk(f'./{dataset_folder}/tokenized_dataset_{MAX_INPUT}')\n","else:\n","    tokenized_dataset = dataset.map(preprocess, remove_columns=['prompt', 'context', 'A', 'B', 'C', 'D', 'E', 'answer'])\n","    tokenized_dataset.save_to_disk(f'./{dataset_folder}/tokenized_dataset_{MAX_INPUT}')\n","\n","# changed by cxzheng, fuck! stucked!\n","\n","tokenized_dataset # 他跑到 21100 附近的时候会卡住，有点奇怪"]},{"cell_type":"markdown","metadata":{},"source":["# MAP@3 Metric\n","The competition metric is MAP@3 therefore we will make a custom code to add to Hugging Face's trainer. Discussion [here][1]\n","\n","[1]: https://www.kaggle.com/competitions/kaggle-llm-science-exam/discussion/435602"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2023-09-09T03:47:36.361768Z","iopub.status.busy":"2023-09-09T03:47:36.361320Z","iopub.status.idle":"2023-09-09T03:47:36.378492Z","shell.execute_reply":"2023-09-09T03:47:36.377185Z","shell.execute_reply.started":"2023-09-09T03:47:36.361728Z"},"trusted":true},"outputs":[],"source":["def map_at_3(predictions, labels):\n","    map_sum = 0\n","    pred = np.argsort(-1*np.array(predictions),axis=1)[:,:3]\n","    for x,y in zip(pred,labels):\n","        z = [1/i if y==j else 0 for i,j in zip([1,2,3],x)]\n","        map_sum += np.sum(z)\n","    return map_sum / len(predictions)\n","\n","def compute_metrics(p):\n","    predictions = p.predictions.tolist()\n","    labels = p.label_ids.tolist()\n","    return {\"map@3\": map_at_3(predictions, labels)}"]},{"cell_type":"markdown","metadata":{},"source":["# Train and Save \n","We will now train and save our model using Hugging Face's easy to use trainer. By adjusting the parameters in this notebook, we can achieve `CV MAP@3 = 0.915+` and corresponding single model `LB MAP@3 = 0.830+` wow!\n","\n","In we run this notebook outside of Kaggle then we can train longer and with more RAM. If we run this notebook on Kaggle, then we need to use tricks to train models efficiently. Here are some ideas:\n","* use fp16 (this speeds up T4 not P100)\n","* use gradient_accumlation_steps (this simulates larger batch sizes)\n","* use gradient_checkpointing (this uses disk to save RAM)\n","* use 2xT4 instead of 1xP100 (this doubles GPUs)\n","* freeze model embeddings (this reduces weights to train)\n","* freeze some model layers (this reduces weights to train)\n","* use PEFT (this reduces weights to train)\n","* increase LR and decrease epochs (this reduces work)\n","* use smaller models (this reduces weights to train)"]},{"cell_type":"markdown","metadata":{},"source":["We will use a Hugging Face AutoModelForMultipleChoice. For the list of possible models, see Hugging Face's repository [here][1]. We can optionally use PEFT to accelerate training and use less memory. However i have noticed that validation accuracy is less. (Note that PEFT requires us to use 1xP100 not 2xT4 GPU. I'm not sure why). We can also optionally freeze layers. This also accelerates training and uses less memory. However validation accuracy may become less.\n","\n","[1]: https://huggingface.co/models"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["# # NOTE PEFT REQUIRES US TO USE 1XP100 NOT 2XT4. I'M NOT SURE WHY.\n","# if USE_PEFT:\n","#     !pip install --no-index --no-deps ../input/llm-whls/peft-0.4.0-py3-none-any.whl"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["def get_model_general(model_name):\n","    from transformers import AutoTokenizer, pipeline\n","    tokenizer = AutoTokenizer.from_pretrained(model_name)\n","    pipe = pipeline(\n","        \"text-generation\",\n","        model=model_name,\n","        tokenizer=tokenizer,\n","        torch_dtype=torch.float16,\n","        trust_remote_code=True,\n","        device_map=\"cuda:0\",\n","    )\n","    tokenizer = pipe.tokenizer\n","    model = pipe.model\n","    return model, tokenizer\n","model, tokenizer = get_model_general('nlpcloud/instruct-gpt-j-fp16')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model = AutoModelForMultipleChoice.from_pretrained(MODEL, ignore_mismatched_sizes=True)\n","model"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["We are using PEFT.\n","trainable params: 2,887,682 || all params: 436,899,842 || trainable%: 0.6609482820549979\n"]}],"source":["if USE_PEFT:\n","    print('We are using PEFT.')\n","    from peft import LoraConfig, get_peft_model, TaskType\n","    peft_config = LoraConfig(\n","        r=8, lora_alpha=4, task_type=TaskType.SEQ_CLS, lora_dropout=0.1, \n","        bias=\"none\", inference_mode=False, \n","        target_modules=[\"query_proj\", \"value_proj\"],\n","        # target_modules = ['query', 'value'],\n","        modules_to_save=['classifier','pooler'],\n","    )\n","    model = get_peft_model(model, peft_config)\n","    model.print_trainable_parameters()\n"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Freezing embeddings.\n","Freezing 18 layers.\n","77875202 436899842 0.17824497633945127\n"]}],"source":["# Deberta\n","if FREEZE_EMBEDDINGS:\n","    print('Freezing embeddings.')\n","#   它是不是其实不止一个 embedding layer?\n","    for param in model.deberta.embeddings.parameters():\n","        param.requires_grad = False\n","        \n","if FREEZE_LAYERS>0:\n","    print(f'Freezing {FREEZE_LAYERS} layers.')\n","    # Deberta\n","    for layer in model.deberta.encoder.layer[:FREEZE_LAYERS]:\n","        for param in layer.parameters():\n","            param.requires_grad = False\n","    # Newly added for v3\n","    for layer in model.deberta.encoder.layer[FREEZE_LAYERS:]:\n","        for param in layer.parameters():\n","            param.requires_grad = True\n","\n","# roberta\n","# if FREEZE_EMBEDDINGS:\n","#     print('Freezing embeddings.')\n","#     for param in model.roberta.embeddings.parameters():\n","#         param.requires_grad = False       \n","\n","# if FREEZE_LAYERS>0:\n","\n","#     for layer in model.roberta.encoder.layer[:FREEZE_LAYERS]:\n","#         for param in layer.parameters():\n","#             param.requires_grad = False\n","#     for layer in model.roberta.encoder.layer[FREEZE_LAYERS:]:\n","#         for param in layer.parameters():\n","#             param.requires_grad = True\n","\n","# bert\n","# if FREEZE_EMBEDDINGS:\n","#     print('Freezing embeddings.')\n","#     for param in model.bert.embeddings.parameters():\n","#         param.requires_grad = False       \n","\n","# if FREEZE_LAYERS>0:\n","\n","#     for layer in model.bert.encoder.layer[:FREEZE_LAYERS]:\n","#         for param in layer.parameters():\n","#             param.requires_grad = False\n","#     for layer in model.bert.encoder.layer[FREEZE_LAYERS:]:\n","#         for param in layer.parameters():\n","#             param.requires_grad = True\n","\n","\n","total_params = sum(p.numel() for p in model.parameters())\n","model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n","trainable_params = sum([np.prod(p.size()) for p in model_parameters])\n","print(trainable_params, total_params, trainable_params/total_params)"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":["from transformers.trainer_utils import PREFIX_CHECKPOINT_DIR\n","from transformers import TrainerState, TrainerControl, TrainerCallback\n","class SavePeftModelCallback(TrainerCallback):\n","    def on_save(\n","        self,\n","        args: TrainingArguments,\n","        state: TrainerState,\n","        control: TrainerControl,\n","        **kwargs,\n","    ):\n","        checkpoint_folder = os.path.join(\n","            args.output_dir, f\"{PREFIX_CHECKPOINT_DIR}-{state.global_step}\"\n","        )       \n","\n","        peft_model_path = os.path.join(checkpoint_folder, \"torch_model\")\n","        # peft_model_path = os.path.join(checkpoint_folder)\n","        kwargs[\"model\"].base_model.save_pretrained(peft_model_path)\n","\n","        # pytorch_model_path = os.path.join(checkpoint_folder, \"pytorch_model.bin\")\n","        # if os.path.exists(pytorch_model_path):\n","        #     os.remove(pytorch_model_path)\n","        return control"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2023-09-09T03:47:40.478843Z","iopub.status.busy":"2023-09-09T03:47:40.478276Z","iopub.status.idle":"2023-09-09T03:47:40.489110Z","shell.execute_reply":"2023-09-09T03:47:40.488156Z","shell.execute_reply.started":"2023-09-09T03:47:40.478811Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["10\n"]}],"source":["batch_size = 8 # Try this if possible for 18 512\n","# batch_size = 4 # for 18 512\n","\n","\n","effective_batch_size = 256\n","GRAD_ACCUM = effective_batch_size // batch_size\n","SAVING_STEP = 10\n","LOGGING_STEPS = SAVING_STEP\n","print(SAVING_STEP)\n","training_args = TrainingArguments(\n","    warmup_ratio=0.1, \n","    learning_rate=2e-5, # change this in accordance to effective_batch_size\n","    # learning_rate = 7e-6,\n","    # per_device_train_batch_size=16,\n","    per_device_train_batch_size=8,\n","    # per_device_eval_batch_size=32,\n","    per_device_eval_batch_size=16,\n","    num_train_epochs=2.5,  # 2.5\n","    report_to='none',\n","    output_dir = f'./{checkpoint_folder}/{VER}',\n","    overwrite_output_dir=True,\n","    fp16=True,\n","    # gradient_accumulation_steps=8,\n","    gradient_accumulation_steps=GRAD_ACCUM,\n","    logging_steps=LOGGING_STEPS,\n","    evaluation_strategy='steps',\n","    eval_steps=SAVING_STEP,\n","    save_strategy=\"steps\",\n","    save_steps=SAVING_STEP,\n","    load_best_model_at_end=False,\n","    metric_for_best_model='map@3',\n","    # lr_scheduler_type='cosine',\n","    lr_scheduler_type='cosine_with_restarts',    \n","    # lr_scheduler_type='reduce_lr_on_plateau',\n","    # weight_decay=0.01,\n","    weight_decay=1e-5, # set this slightly higher to reduce oscillation\n","    save_total_limit=5,\n","    \n",")"]},{"cell_type":"markdown","metadata":{},"source":["## Training Start"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2023-09-09T03:47:44.277808Z","iopub.status.busy":"2023-09-09T03:47:44.277446Z","iopub.status.idle":"2023-09-09T03:54:54.290539Z","shell.execute_reply":"2023-09-09T03:54:54.289078Z","shell.execute_reply.started":"2023-09-09T03:47:44.277778Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/dijkstraz/anaconda3/envs/kaggle/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='588' max='588' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [588/588 1:53:45, Epoch 2/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Map@3</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>10</td>\n","      <td>1.614100</td>\n","      <td>1.609292</td>\n","      <td>0.455833</td>\n","    </tr>\n","    <tr>\n","      <td>20</td>\n","      <td>1.611700</td>\n","      <td>1.608354</td>\n","      <td>0.634167</td>\n","    </tr>\n","    <tr>\n","      <td>30</td>\n","      <td>1.608700</td>\n","      <td>1.602827</td>\n","      <td>0.765000</td>\n","    </tr>\n","    <tr>\n","      <td>40</td>\n","      <td>1.570100</td>\n","      <td>1.461763</td>\n","      <td>0.782500</td>\n","    </tr>\n","    <tr>\n","      <td>50</td>\n","      <td>1.217500</td>\n","      <td>1.152031</td>\n","      <td>0.815833</td>\n","    </tr>\n","    <tr>\n","      <td>60</td>\n","      <td>1.028000</td>\n","      <td>0.944971</td>\n","      <td>0.838333</td>\n","    </tr>\n","    <tr>\n","      <td>70</td>\n","      <td>0.966900</td>\n","      <td>0.806068</td>\n","      <td>0.827500</td>\n","    </tr>\n","    <tr>\n","      <td>80</td>\n","      <td>0.909800</td>\n","      <td>0.767373</td>\n","      <td>0.834167</td>\n","    </tr>\n","    <tr>\n","      <td>90</td>\n","      <td>0.869200</td>\n","      <td>0.694823</td>\n","      <td>0.847500</td>\n","    </tr>\n","    <tr>\n","      <td>100</td>\n","      <td>0.838600</td>\n","      <td>0.699032</td>\n","      <td>0.845000</td>\n","    </tr>\n","    <tr>\n","      <td>110</td>\n","      <td>0.830400</td>\n","      <td>0.662020</td>\n","      <td>0.860833</td>\n","    </tr>\n","    <tr>\n","      <td>120</td>\n","      <td>0.826300</td>\n","      <td>0.717908</td>\n","      <td>0.853333</td>\n","    </tr>\n","    <tr>\n","      <td>130</td>\n","      <td>0.800200</td>\n","      <td>0.629594</td>\n","      <td>0.865833</td>\n","    </tr>\n","    <tr>\n","      <td>140</td>\n","      <td>0.820700</td>\n","      <td>0.640993</td>\n","      <td>0.869167</td>\n","    </tr>\n","    <tr>\n","      <td>150</td>\n","      <td>0.814400</td>\n","      <td>0.624861</td>\n","      <td>0.870833</td>\n","    </tr>\n","    <tr>\n","      <td>160</td>\n","      <td>0.740500</td>\n","      <td>0.652845</td>\n","      <td>0.860833</td>\n","    </tr>\n","    <tr>\n","      <td>170</td>\n","      <td>0.761100</td>\n","      <td>0.598622</td>\n","      <td>0.858333</td>\n","    </tr>\n","    <tr>\n","      <td>180</td>\n","      <td>0.764500</td>\n","      <td>0.581104</td>\n","      <td>0.874167</td>\n","    </tr>\n","    <tr>\n","      <td>190</td>\n","      <td>0.735500</td>\n","      <td>0.601811</td>\n","      <td>0.864167</td>\n","    </tr>\n","    <tr>\n","      <td>200</td>\n","      <td>0.752600</td>\n","      <td>0.569995</td>\n","      <td>0.872500</td>\n","    </tr>\n","    <tr>\n","      <td>210</td>\n","      <td>0.750700</td>\n","      <td>0.593259</td>\n","      <td>0.868333</td>\n","    </tr>\n","    <tr>\n","      <td>220</td>\n","      <td>0.777800</td>\n","      <td>0.599156</td>\n","      <td>0.864167</td>\n","    </tr>\n","    <tr>\n","      <td>230</td>\n","      <td>0.774100</td>\n","      <td>0.582552</td>\n","      <td>0.868333</td>\n","    </tr>\n","    <tr>\n","      <td>240</td>\n","      <td>0.749400</td>\n","      <td>0.600653</td>\n","      <td>0.871667</td>\n","    </tr>\n","    <tr>\n","      <td>250</td>\n","      <td>0.703600</td>\n","      <td>0.562898</td>\n","      <td>0.880000</td>\n","    </tr>\n","    <tr>\n","      <td>260</td>\n","      <td>0.722800</td>\n","      <td>0.562847</td>\n","      <td>0.868333</td>\n","    </tr>\n","    <tr>\n","      <td>270</td>\n","      <td>0.725700</td>\n","      <td>0.564451</td>\n","      <td>0.870833</td>\n","    </tr>\n","    <tr>\n","      <td>280</td>\n","      <td>0.694600</td>\n","      <td>0.555285</td>\n","      <td>0.885000</td>\n","    </tr>\n","    <tr>\n","      <td>290</td>\n","      <td>0.689400</td>\n","      <td>0.553630</td>\n","      <td>0.879167</td>\n","    </tr>\n","    <tr>\n","      <td>300</td>\n","      <td>0.715400</td>\n","      <td>0.568064</td>\n","      <td>0.884167</td>\n","    </tr>\n","    <tr>\n","      <td>310</td>\n","      <td>0.692100</td>\n","      <td>0.545408</td>\n","      <td>0.880000</td>\n","    </tr>\n","    <tr>\n","      <td>320</td>\n","      <td>0.715200</td>\n","      <td>0.565730</td>\n","      <td>0.889167</td>\n","    </tr>\n","    <tr>\n","      <td>330</td>\n","      <td>0.684400</td>\n","      <td>0.552693</td>\n","      <td>0.880833</td>\n","    </tr>\n","    <tr>\n","      <td>340</td>\n","      <td>0.689800</td>\n","      <td>0.562354</td>\n","      <td>0.876667</td>\n","    </tr>\n","    <tr>\n","      <td>350</td>\n","      <td>0.725300</td>\n","      <td>0.571890</td>\n","      <td>0.884167</td>\n","    </tr>\n","    <tr>\n","      <td>360</td>\n","      <td>0.664200</td>\n","      <td>0.537808</td>\n","      <td>0.885833</td>\n","    </tr>\n","    <tr>\n","      <td>370</td>\n","      <td>0.682400</td>\n","      <td>0.548115</td>\n","      <td>0.884167</td>\n","    </tr>\n","    <tr>\n","      <td>380</td>\n","      <td>0.682400</td>\n","      <td>0.554653</td>\n","      <td>0.890000</td>\n","    </tr>\n","    <tr>\n","      <td>390</td>\n","      <td>0.673100</td>\n","      <td>0.544855</td>\n","      <td>0.885000</td>\n","    </tr>\n","    <tr>\n","      <td>400</td>\n","      <td>0.720700</td>\n","      <td>0.553769</td>\n","      <td>0.890833</td>\n","    </tr>\n","    <tr>\n","      <td>410</td>\n","      <td>0.683500</td>\n","      <td>0.544975</td>\n","      <td>0.888333</td>\n","    </tr>\n","    <tr>\n","      <td>420</td>\n","      <td>0.637100</td>\n","      <td>0.541554</td>\n","      <td>0.885000</td>\n","    </tr>\n","    <tr>\n","      <td>430</td>\n","      <td>0.686000</td>\n","      <td>0.545386</td>\n","      <td>0.883333</td>\n","    </tr>\n","    <tr>\n","      <td>440</td>\n","      <td>0.673600</td>\n","      <td>0.550606</td>\n","      <td>0.884167</td>\n","    </tr>\n","    <tr>\n","      <td>450</td>\n","      <td>0.675600</td>\n","      <td>0.547122</td>\n","      <td>0.886667</td>\n","    </tr>\n","    <tr>\n","      <td>460</td>\n","      <td>0.691300</td>\n","      <td>0.545992</td>\n","      <td>0.886667</td>\n","    </tr>\n","    <tr>\n","      <td>470</td>\n","      <td>0.661500</td>\n","      <td>0.540608</td>\n","      <td>0.888333</td>\n","    </tr>\n","    <tr>\n","      <td>480</td>\n","      <td>0.640200</td>\n","      <td>0.535522</td>\n","      <td>0.890833</td>\n","    </tr>\n","    <tr>\n","      <td>490</td>\n","      <td>0.668600</td>\n","      <td>0.537888</td>\n","      <td>0.890833</td>\n","    </tr>\n","    <tr>\n","      <td>500</td>\n","      <td>0.658700</td>\n","      <td>0.541519</td>\n","      <td>0.890833</td>\n","    </tr>\n","    <tr>\n","      <td>510</td>\n","      <td>0.675800</td>\n","      <td>0.542835</td>\n","      <td>0.890833</td>\n","    </tr>\n","    <tr>\n","      <td>520</td>\n","      <td>0.633500</td>\n","      <td>0.541536</td>\n","      <td>0.890833</td>\n","    </tr>\n","    <tr>\n","      <td>530</td>\n","      <td>0.683900</td>\n","      <td>0.541423</td>\n","      <td>0.889167</td>\n","    </tr>\n","    <tr>\n","      <td>540</td>\n","      <td>0.657800</td>\n","      <td>0.541038</td>\n","      <td>0.890833</td>\n","    </tr>\n","    <tr>\n","      <td>550</td>\n","      <td>0.664900</td>\n","      <td>0.540985</td>\n","      <td>0.889167</td>\n","    </tr>\n","    <tr>\n","      <td>560</td>\n","      <td>0.651300</td>\n","      <td>0.540783</td>\n","      <td>0.890833</td>\n","    </tr>\n","    <tr>\n","      <td>570</td>\n","      <td>0.651300</td>\n","      <td>0.540888</td>\n","      <td>0.889167</td>\n","    </tr>\n","    <tr>\n","      <td>580</td>\n","      <td>0.661100</td>\n","      <td>0.540917</td>\n","      <td>0.890833</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":["trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    tokenizer=tokenizer,\n","    data_collator=DataCollatorForMultipleChoice(tokenizer=tokenizer),\n","    train_dataset=tokenized_dataset,\n","    eval_dataset=tokenized_dataset_valid,\n","    compute_metrics = compute_metrics,\n","    callbacks=[SavePeftModelCallback] if USE_PEFT else None,\n","    #callbacks=[EarlyStoppingCallback(early_stopping_patience=5)],\n",")\n","\n","trainer.train()\n","\n","if USE_PEFT:\n","    trainer.model.save_pretrained(f'model_v{VER}') # 我改了这个\n","else:\n","    trainer.save_model(f'model_v{VER}')\n","    \n","# I think I read from some parts of the discussion that some length of input during training could be changed.\n","# Training longer during training will hopefully cover the length of even the longest sentence in testing.\n","# This is some problem with model extrapolation.\n","# Basically, if the model is not using very good positional encoding, it will perform badly in sequences longer than what it has been trained on."]},{"cell_type":"markdown","metadata":{},"source":["# Verify Saved Model\n","During training, we see the MAP@3 validation score above. Let's load the saved model and compute it again here to verify that our model is saved correctly."]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.status.busy":"2023-09-02T02:42:35.053535Z","iopub.status.idle":"2023-09-02T02:42:35.054036Z","shell.execute_reply":"2023-09-02T02:42:35.053806Z","shell.execute_reply.started":"2023-09-02T02:42:35.053782Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["loading peft\n"]},{"name":"stderr","output_type":"stream","text":["Some weights of DebertaV2ForMultipleChoice were not initialized from the model checkpoint at microsoft/deberta-v3-large and are newly initialized: ['pooler.dense.weight', 'pooler.dense.bias', 'classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]},{"name":"stdout","output_type":"stream","text":["trainable params: 2,101,250 || all params: 436,899,842 || trainable%: 0.4809454703350522\n"]},{"name":"stderr","output_type":"stream","text":["You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"]},{"data":{"text/html":[],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["{'eval_loss': 0.5277463793754578,\n"," 'eval_map@3': 0.8941666666666667,\n"," 'eval_runtime': 13.29,\n"," 'eval_samples_per_second': 15.049,\n"," 'eval_steps_per_second': 1.881}"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["# del model, trainer\n","from peft import get_peft_model, set_peft_model_state_dict, LoraConfig\n","if USE_PEFT:\n","    print('loading peft')\n","    checkpoint = f'./{checkpoint_folder}/{VER}/checkpoint-550'\n","    model = AutoModelForMultipleChoice.from_pretrained(MODEL)\n","    tokenizer = AutoTokenizer.from_pretrained(MODEL)\n","    peft_config = LoraConfig.from_pretrained(checkpoint)\n","    model = get_peft_model(model, peft_config)\n","    # checkpoint = torch.load(f'model_v{VER}/adapter_model.bin')\n","    # checkpoint = torch.load(f'./checkpoints_3/checkpoint-160/adapter_model.bin')\n","    # checkpoint = torch.load(f'./checkpoints_3/checkpoint-224/adapter_model.bin')\n","    # checkpoint = torch.load(f'./checkpoints_5/checkpoint-3900/adapter_model.bin')\n","\n","    # checkpoint = torch.load(f'./checkpoints_{VER}/checkpoint-550/torch_model/pytorch_model.bin')\n","    checkpoint = torch.load(f'{checkpoint}/torch_model/pytorch_model.bin')\n","\n","    model.base_model.model.load_state_dict(checkpoint)\n","\n","\n","    # print('loading state dict')\n","    # set_peft_model_state_dict(model, checkpoint)\n","    # if FREEZE_EMBEDDINGS:\n","    #     print('Freezing embeddings.')\n","    #     for param in model.deberta.embeddings.parameters():\n","    #         param.requires_grad = False\n","    # if FREEZE_LAYERS>0:\n","    #     print(f'Freezing {FREEZE_LAYERS} layers.')\n","    #     for layer in model.deberta.encoder.layer[:FREEZE_LAYERS]:\n","    #         for param in layer.parameters():\n","    #             param.requires_grad = False\n","    #     # Newly added for v3\n","    #     for layer in model.deberta.encoder.layer[FREEZE_LAYERS:]:\n","    #         for param in layer.parameters():\n","    #             param.requires_grad = True\n","    model.eval()\n","    model.print_trainable_parameters()\n","else:\n","    model = AutoModelForMultipleChoice.from_pretrained(f'model_v{VER}')\n","trainer = Trainer(model=model, \n","                data_collator=DataCollatorForMultipleChoice(tokenizer=tokenizer), \n","                tokenizer=tokenizer,\n","                train_dataset=tokenized_dataset,\n","                eval_dataset=tokenized_dataset_valid,\n","                compute_metrics = compute_metrics,)\n","trainer.evaluate()"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"813c1fe837e348b5a06491aad3f9b15c","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/200 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"ename":"KeyError","evalue":"'context'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[1;32m/mnt/e/ScienceExam/working/jason_how-to prompt open-book-model.ipynb Cell 26\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/e/ScienceExam/working/jason_how-to%20prompt%20open-book-model.ipynb#X42sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39m# # test_df[\"prompt\"] = test_df[\"context\"].apply(lambda x: x[:1750]) + \" #### \" +  test_df[\"prompt\"]\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/e/ScienceExam/working/jason_how-to%20prompt%20open-book-model.ipynb#X42sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m test_df[\u001b[39m\"\u001b[39m\u001b[39mprompt\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m test_df[\u001b[39m\"\u001b[39m\u001b[39mcontext\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m x: x[:\u001b[39m2500\u001b[39m]) \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m #### \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m  test_df[\u001b[39m\"\u001b[39m\u001b[39mprompt\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/e/ScienceExam/working/jason_how-to%20prompt%20open-book-model.ipynb#X42sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m tokenized_test_dataset \u001b[39m=\u001b[39m Dataset\u001b[39m.\u001b[39;49mfrom_pandas(test_df[[\u001b[39m'\u001b[39;49m\u001b[39mid\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mprompt\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mA\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mB\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mC\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mD\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mE\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39manswer\u001b[39;49m\u001b[39m'\u001b[39;49m]]\u001b[39m.\u001b[39;49mdrop(columns\u001b[39m=\u001b[39;49m[\u001b[39m'\u001b[39;49m\u001b[39mid\u001b[39;49m\u001b[39m'\u001b[39;49m]))\u001b[39m.\u001b[39;49mmap(preprocess, remove_columns\u001b[39m=\u001b[39;49m[\u001b[39m'\u001b[39;49m\u001b[39mprompt\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mA\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mB\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mC\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mD\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mE\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39manswer\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/e/ScienceExam/working/jason_how-to%20prompt%20open-book-model.ipynb#X42sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/e/ScienceExam/working/jason_how-to%20prompt%20open-book-model.ipynb#X42sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m     tokenized_test_dataset \u001b[39m=\u001b[39m tokenized_test_dataset\u001b[39m.\u001b[39mremove_columns([\u001b[39m\"\u001b[39m\u001b[39m__index_level_0__\u001b[39m\u001b[39m\"\u001b[39m])\n","File \u001b[0;32m/home/dijkstraz/anaconda3/envs/kaggle/lib/python3.10/site-packages/datasets/arrow_dataset.py:592\u001b[0m, in \u001b[0;36mtransmit_tasks.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    590\u001b[0m     \u001b[39mself\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mself\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    591\u001b[0m \u001b[39m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 592\u001b[0m out: Union[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mDatasetDict\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    593\u001b[0m datasets: List[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(out\u001b[39m.\u001b[39mvalues()) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(out, \u001b[39mdict\u001b[39m) \u001b[39melse\u001b[39;00m [out]\n\u001b[1;32m    594\u001b[0m \u001b[39mfor\u001b[39;00m dataset \u001b[39min\u001b[39;00m datasets:\n\u001b[1;32m    595\u001b[0m     \u001b[39m# Remove task templates if a column mapping of the template is no longer valid\u001b[39;00m\n","File \u001b[0;32m/home/dijkstraz/anaconda3/envs/kaggle/lib/python3.10/site-packages/datasets/arrow_dataset.py:557\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    550\u001b[0m self_format \u001b[39m=\u001b[39m {\n\u001b[1;32m    551\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtype\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_type,\n\u001b[1;32m    552\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mformat_kwargs\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_kwargs,\n\u001b[1;32m    553\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mcolumns\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_columns,\n\u001b[1;32m    554\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39moutput_all_columns\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output_all_columns,\n\u001b[1;32m    555\u001b[0m }\n\u001b[1;32m    556\u001b[0m \u001b[39m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 557\u001b[0m out: Union[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mDatasetDict\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    558\u001b[0m datasets: List[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(out\u001b[39m.\u001b[39mvalues()) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(out, \u001b[39mdict\u001b[39m) \u001b[39melse\u001b[39;00m [out]\n\u001b[1;32m    559\u001b[0m \u001b[39m# re-apply format to the output\u001b[39;00m\n","File \u001b[0;32m/home/dijkstraz/anaconda3/envs/kaggle/lib/python3.10/site-packages/datasets/arrow_dataset.py:3097\u001b[0m, in \u001b[0;36mDataset.map\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[1;32m   3090\u001b[0m \u001b[39mif\u001b[39;00m transformed_dataset \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   3091\u001b[0m     \u001b[39mwith\u001b[39;00m logging\u001b[39m.\u001b[39mtqdm(\n\u001b[1;32m   3092\u001b[0m         disable\u001b[39m=\u001b[39m\u001b[39mnot\u001b[39;00m logging\u001b[39m.\u001b[39mis_progress_bar_enabled(),\n\u001b[1;32m   3093\u001b[0m         unit\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m examples\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   3094\u001b[0m         total\u001b[39m=\u001b[39mpbar_total,\n\u001b[1;32m   3095\u001b[0m         desc\u001b[39m=\u001b[39mdesc \u001b[39mor\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mMap\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   3096\u001b[0m     ) \u001b[39mas\u001b[39;00m pbar:\n\u001b[0;32m-> 3097\u001b[0m         \u001b[39mfor\u001b[39;00m rank, done, content \u001b[39min\u001b[39;00m Dataset\u001b[39m.\u001b[39m_map_single(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mdataset_kwargs):\n\u001b[1;32m   3098\u001b[0m             \u001b[39mif\u001b[39;00m done:\n\u001b[1;32m   3099\u001b[0m                 shards_done \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n","File \u001b[0;32m/home/dijkstraz/anaconda3/envs/kaggle/lib/python3.10/site-packages/datasets/arrow_dataset.py:3450\u001b[0m, in \u001b[0;36mDataset._map_single\u001b[0;34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset)\u001b[0m\n\u001b[1;32m   3448\u001b[0m _time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m   3449\u001b[0m \u001b[39mfor\u001b[39;00m i, example \u001b[39min\u001b[39;00m shard_iterable:\n\u001b[0;32m-> 3450\u001b[0m     example \u001b[39m=\u001b[39m apply_function_on_filtered_inputs(example, i, offset\u001b[39m=\u001b[39;49moffset)\n\u001b[1;32m   3451\u001b[0m     \u001b[39mif\u001b[39;00m update_data:\n\u001b[1;32m   3452\u001b[0m         \u001b[39mif\u001b[39;00m i \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n","File \u001b[0;32m/home/dijkstraz/anaconda3/envs/kaggle/lib/python3.10/site-packages/datasets/arrow_dataset.py:3353\u001b[0m, in \u001b[0;36mDataset._map_single.<locals>.apply_function_on_filtered_inputs\u001b[0;34m(pa_inputs, indices, check_same_num_examples, offset)\u001b[0m\n\u001b[1;32m   3351\u001b[0m \u001b[39mif\u001b[39;00m with_rank:\n\u001b[1;32m   3352\u001b[0m     additional_args \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (rank,)\n\u001b[0;32m-> 3353\u001b[0m processed_inputs \u001b[39m=\u001b[39m function(\u001b[39m*\u001b[39;49mfn_args, \u001b[39m*\u001b[39;49madditional_args, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfn_kwargs)\n\u001b[1;32m   3354\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(processed_inputs, LazyDict):\n\u001b[1;32m   3355\u001b[0m     processed_inputs \u001b[39m=\u001b[39m {\n\u001b[1;32m   3356\u001b[0m         k: v \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m processed_inputs\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mitems() \u001b[39mif\u001b[39;00m k \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m processed_inputs\u001b[39m.\u001b[39mkeys_to_format\n\u001b[1;32m   3357\u001b[0m     }\n","\u001b[1;32m/mnt/e/ScienceExam/working/jason_how-to prompt open-book-model.ipynb Cell 26\u001b[0m line \u001b[0;36m6\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/e/ScienceExam/working/jason_how-to%20prompt%20open-book-model.ipynb#X42sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpreprocess\u001b[39m(example):\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/e/ScienceExam/working/jason_how-to%20prompt%20open-book-model.ipynb#X42sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m     first_sentence \u001b[39m=\u001b[39m [ \u001b[39m\"\u001b[39m\u001b[39m[CLS] \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m example[\u001b[39m'\u001b[39;49m\u001b[39mcontext\u001b[39;49m\u001b[39m'\u001b[39;49m] ] \u001b[39m*\u001b[39m \u001b[39m5\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/e/ScienceExam/working/jason_how-to%20prompt%20open-book-model.ipynb#X42sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m     second_sentences \u001b[39m=\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39m #### \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m example[\u001b[39m'\u001b[39m\u001b[39mprompt\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m [SEP] \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m example[option] \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m [SEP]\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mfor\u001b[39;00m option \u001b[39min\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mABCDE\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/e/ScienceExam/working/jason_how-to%20prompt%20open-book-model.ipynb#X42sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m     tokenized_example \u001b[39m=\u001b[39m tokenizer(first_sentence, second_sentences, truncation\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39monly_first\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/e/ScienceExam/working/jason_how-to%20prompt%20open-book-model.ipynb#X42sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m                                   max_length\u001b[39m=\u001b[39mMAX_INPUT, add_special_tokens\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n","File \u001b[0;32m/home/dijkstraz/anaconda3/envs/kaggle/lib/python3.10/site-packages/datasets/formatting/formatting.py:270\u001b[0m, in \u001b[0;36mLazyDict.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, key):\n\u001b[0;32m--> 270\u001b[0m     value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdata[key]\n\u001b[1;32m    271\u001b[0m     \u001b[39mif\u001b[39;00m key \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkeys_to_format:\n\u001b[1;32m    272\u001b[0m         value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mformat(key)\n","\u001b[0;31mKeyError\u001b[0m: 'context'"]}],"source":["# MAX_INPUT = None\n","MAX_INPUT = 512\n","\n","\n","from torch.utils.data import DataLoader\n","test_df = pd.read_csv('../input/60k-data-with-context-v2/train_with_context2.csv')\n","# tokenized_dataset_valid = dataset_valid.map(preprocess, remove_columns=['prompt', 'context', 'A', 'B', 'C', 'D', 'E', 'answer'])\n","# tokenized_test_dataset = Dataset.from_pandas(test_df).map(\n","        # preprocess, remove_columns=['prompt', 'context', 'A', 'B', 'C', 'D', 'E'])\n","\n","## kaggle parte\n","test_df.index = list(range(len(test_df)))\n","test_df['id'] = list(range(len(test_df)))\n","# # test_df[\"prompt\"] = test_df[\"context\"].apply(lambda x: x[:1750]) + \" #### \" +  test_df[\"prompt\"]\n","test_df[\"prompt\"] = test_df[\"context\"].apply(lambda x: x[:2500]) + \" #### \" +  test_df[\"prompt\"]\n","\n","\n","tokenized_test_dataset = Dataset.from_pandas(test_df[['id', 'prompt', 'A', 'B', 'C', 'D', 'E', 'answer']].drop(columns=['id'])).map(preprocess, remove_columns=['prompt', 'A', 'B', 'C', 'D', 'E', 'answer'])\n","try:\n","    tokenized_test_dataset = tokenized_test_dataset.remove_columns([\"__index_level_0__\"])\n","except:\n","    print('no need to remove column')\n","    pass\n","data_collator = DataCollatorForMultipleChoice(tokenizer=tokenizer)"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.status.busy":"2023-09-02T02:42:35.055773Z","iopub.status.idle":"2023-09-02T02:42:35.056299Z","shell.execute_reply":"2023-09-02T02:42:35.056073Z","shell.execute_reply.started":"2023-09-02T02:42:35.056015Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1bceaea551f64a2e9ce62363ef54df92","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/200 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"}],"source":["test_dataloader = DataLoader(tokenized_test_dataset, batch_size=1, shuffle=False, collate_fn=data_collator)\n","\n","with torch.no_grad():\n","    test_predictions = []\n","    for batch in test_dataloader:\n","        for k in batch.keys():\n","            batch[k] = batch[k].cuda()\n","        with torch.no_grad():\n","            outputs = model(**batch)\n","        test_predictions.append(outputs.logits.cpu().detach())\n","\n","test_predictions = torch.cat(test_predictions)\n","test_predictions = test_predictions.numpy()\n","\n","\n","predictions_as_ids = np.argsort(-test_predictions, 1)\n","\n","predictions_as_answer_letters = np.array(list('ABCDE'))[predictions_as_ids]\n","# predictions_as_answer_letters[:3]\n","\n","predictions_as_string = test_df['prediction'] = [\n","    ' '.join(row) for row in predictions_as_answer_letters[:, :3]\n","]"]},{"cell_type":"markdown","metadata":{},"source":["法1：\n","1. 通过 question 在 wiki 中 进行 context matching，用 sentence embedding 的 cosine similarity，得到 k 个 context，Ctx\n","2. 通过 answer 在 Ctx 中进行 sentence similarity matching, 将每个答案的 similarity 进行 normalize, 当作先验概率（prior probability）\n","3. 以训练过的方式，让模型给logits，然后用 sonormalize 得到后验概率（posterior probability）\n","4. 先后 相乘，然后 softmax，准备ensemble\n","\n","法2：\n","1. 通过 question 在 wiki 中 进行 context matching，用 sentence embedding 的 cosine similarity，得到 k 个 context，Ctx\n","2. 通过某种方法分解提取答案，然后将分解出来的 entity 拿到 context 中，计算 context 每个句子的权重，挑出最重要的 s 个句子，以他们在wiki 中出现的顺序喂给模型"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["786\n","[CLS] The presence of a clustered thick disk-like component of dark matter in the Galaxy has been suggested by Sanchez-Salcedo (1997, 1999) and Kerins (1997).Kerins, E. J. 1997, Astronomy and Astrophysics, 322, 709-718 (ADS entry )Sánchez-Salcedo, F. J. 1997, Astrophysical Journal, 487, L61-L64 (ADS entry )Sánchez-Salcedo, F. J. 1999, Monthly Notices of the Royal Astronomical Society, 303, 755-772 (ADS entry ) ==See also== * Dark matter * Brown dwarfs * White dwarfs * Microlensing * Hypercompact stellar system * Massive compact halo object (MACHOs) * Weakly interacting massive particles (WIMPs) ==References== Category:Star clusters Category:Open clusters Observations of the Bullet Cluster are the strongest evidence for the existence of dark matter; however, Brownstein and Moffat have shown that their modified gravity theory can also account for the properties of the cluster. == Observational methods == Clusters of galaxies have been found in surveys by a number of observational techniques and have been studied in detail using many methods: * Optical or infrared: The individual galaxies of clusters can be studied through optical or infrared imaging and spectroscopy. The observed distortions can be used to model the distribution of dark matter in the cluster. == Temperature and density == Clusters of galaxies are the most recent and most massive objects to have arisen in the hierarchical structure formation of the Universe and the study of clusters tells one about the way galaxies form and evolve. A 2021 article postulated that approximately 50% of all baryonic matter is outside dark matter haloes, filling the space between galaxies, and that this would explain the missing baryons not accounted for in the 2017 paper. == Current state == Currently, many groups have observed the intergalactic medium and circum-galactic medium to obtain more measurements and observations of baryons to support the leading observations. In cosmology, the missing baryon problem is an observed discrepancy between the amount of baryonic matter detected from shortly after the Big Bang and from more recent epochs. Brownstein and Moffat use a theory of modified gravity to explain X-ray cluster masses without dark matter. The missing baryon problem has been resolved but research groups are working to detect the WHIM using varying methods to confirm results. ==References== Category:Physical cosmology Category:Baryons Baryons make up only ~5% of the universe, while dark matter makes up 26.8%. ==Early universe measurements== The abundance of baryonic matter in the early universe can be obtained indirectly from two independent methods: * The theory of Big Bang nucleosynthesis, which predicts the observed relative abundance of the chemical elements in observations of the recent universe. The missing baryon problem is different from the dark matter problem, which is non-baryonic in nature.See Lambda-CDM model. In a typical cluster perhaps only 5% of the total mass is in the form of galaxies, maybe 10% in the form of hot X-ray emitting gas and the remainder is dark matter. In astronomy, a RAMBO or robust association of massive baryonic objects is a dark cluster made of brown dwarfs or white dwarfs. It is composed of mostly ionized hydrogen and is about 10% of a galaxy cluster's total mass; the rest being dark matter. This is highly nontrivial, since although luminous #### Which of the following statements accurately describes the impact of Modified Newtonian Dynamics (MOND) on the observed \"missing baryonic mass\" discrepancy in galaxy clusters?[SEP] MOND is a theory that reduces the observed missing baryonic mass in galaxy clusters by postulating the existence of a new form of matter called \"fuzzy dark matter.\"[SEP]\n","786\n","[CLS] The presence of a clustered thick disk-like component of dark matter in the Galaxy has been suggested by Sanchez-Salcedo (1997, 1999) and Kerins (1997).Kerins, E. J. 1997, Astronomy and Astrophysics, 322, 709-718 (ADS entry )Sánchez-Salcedo, F. J. 1997, Astrophysical Journal, 487, L61-L64 (ADS entry )Sánchez-Salcedo, F. J. 1999, Monthly Notices of the Royal Astronomical Society, 303, 755-772 (ADS entry ) ==See also== * Dark matter * Brown dwarfs * White dwarfs * Microlensing * Hypercompact stellar system * Massive compact halo object (MACHOs) * Weakly interacting massive particles (WIMPs) ==References== Category:Star clusters Category:Open clusters Observations of the Bullet Cluster are the strongest evidence for the existence of dark matter; however, Brownstein and Moffat have shown that their modified gravity theory can also account for the properties of the cluster. == Observational methods == Clusters of galaxies have been found in surveys by a number of observational techniques and have been studied in detail using many methods: * Optical or infrared: The individual galaxies of clusters can be studied through optical or infrared imaging and spectroscopy. The observed distortions can be used to model the distribution of dark matter in the cluster. == Temperature and density == Clusters of galaxies are the most recent and most massive objects to have arisen in the hierarchical structure formation of the Universe and the study of clusters tells one about the way galaxies form and evolve. A 2021 article postulated that approximately 50% of all baryonic matter is outside dark matter haloes, filling the space between galaxies, and that this would explain the missing baryons not accounted for in the 2017 paper. == Current state == Currently, many groups have observed the intergalactic medium and circum-galactic medium to obtain more measurements and observations of baryons to support the leading observations. In cosmology, the missing baryon problem is an observed discrepancy between the amount of baryonic matter detected from shortly after the Big Bang and from more recent epochs. Brownstein and Moffat use a theory of modified gravity to explain X-ray cluster masses without dark matter. The missing baryon problem has been resolved but research groups are working to detect the WHIM using varying methods to confirm results. ==References== Category:Physical cosmology Category:Baryons Baryons make up only ~5% of the universe, while dark matter makes up 26.8%. ==Early universe measurements== The abundance of baryonic matter in the early universe can be obtained indirectly from two independent methods: * The theory of Big Bang nucleosynthesis, which predicts the observed relative abundance of the chemical elements in observations of the recent universe. The missing baryon problem is different from the dark matter problem, which is non-baryonic in nature.See Lambda-CDM model. In a typical cluster perhaps only 5% of the total mass is in the form of galaxies, maybe 10% in the form of hot X-ray emitting gas and the remainder is dark matter. In astronomy, a RAMBO or robust association of massive baryonic objects is a dark cluster made of brown dwarfs or white dwarfs. It is composed of mostly ionized hydrogen and is about 10% of a galaxy cluster's total mass; the rest being dark matter. This is highly nontrivial #### Which of the following statements accurately describes the impact of Modified Newtonian Dynamics (MOND) on the observed \"missing baryonic mass\" discrepancy in galaxy clusters?[SEP] MOND is a theory that increases the discrepancy between the observed missing baryonic mass in galaxy clusters and the measured velocity dispersions from a factor of around 10 to a factor of about 20.[SEP]\n","786\n","[CLS] The presence of a clustered thick disk-like component of dark matter in the Galaxy has been suggested by Sanchez-Salcedo (1997, 1999) and Kerins (1997).Kerins, E. J. 1997, Astronomy and Astrophysics, 322, 709-718 (ADS entry )Sánchez-Salcedo, F. J. 1997, Astrophysical Journal, 487, L61-L64 (ADS entry )Sánchez-Salcedo, F. J. 1999, Monthly Notices of the Royal Astronomical Society, 303, 755-772 (ADS entry ) ==See also== * Dark matter * Brown dwarfs * White dwarfs * Microlensing * Hypercompact stellar system * Massive compact halo object (MACHOs) * Weakly interacting massive particles (WIMPs) ==References== Category:Star clusters Category:Open clusters Observations of the Bullet Cluster are the strongest evidence for the existence of dark matter; however, Brownstein and Moffat have shown that their modified gravity theory can also account for the properties of the cluster. == Observational methods == Clusters of galaxies have been found in surveys by a number of observational techniques and have been studied in detail using many methods: * Optical or infrared: The individual galaxies of clusters can be studied through optical or infrared imaging and spectroscopy. The observed distortions can be used to model the distribution of dark matter in the cluster. == Temperature and density == Clusters of galaxies are the most recent and most massive objects to have arisen in the hierarchical structure formation of the Universe and the study of clusters tells one about the way galaxies form and evolve. A 2021 article postulated that approximately 50% of all baryonic matter is outside dark matter haloes, filling the space between galaxies, and that this would explain the missing baryons not accounted for in the 2017 paper. == Current state == Currently, many groups have observed the intergalactic medium and circum-galactic medium to obtain more measurements and observations of baryons to support the leading observations. In cosmology, the missing baryon problem is an observed discrepancy between the amount of baryonic matter detected from shortly after the Big Bang and from more recent epochs. Brownstein and Moffat use a theory of modified gravity to explain X-ray cluster masses without dark matter. The missing baryon problem has been resolved but research groups are working to detect the WHIM using varying methods to confirm results. ==References== Category:Physical cosmology Category:Baryons Baryons make up only ~5% of the universe, while dark matter makes up 26.8%. ==Early universe measurements== The abundance of baryonic matter in the early universe can be obtained indirectly from two independent methods: * The theory of Big Bang nucleosynthesis, which predicts the observed relative abundance of the chemical elements in observations of the recent universe. The missing baryon problem is different from the dark matter problem, which is non-baryonic in nature.See Lambda-CDM model. In a typical cluster perhaps only 5% of the total mass is in the form of galaxies, maybe 10% in the form of hot X-ray emitting gas and the remainder is dark matter. In astronomy, a RAMBO or robust association of massive baryonic objects is a dark cluster made of brown dwarfs or white dwarfs. It is composed of mostly ionized hydrogen and is about 10% of a galaxy cluster's total mass; the rest being dark matter. This is highly nontrivial, #### Which of the following statements accurately describes the impact of Modified Newtonian Dynamics (MOND) on the observed \"missing baryonic mass\" discrepancy in galaxy clusters?[SEP] MOND is a theory that explains the missing baryonic mass in galaxy clusters that was previously considered dark matter by demonstrating that the mass is in the form of neutrinos and axions.[SEP]\n","786\n","[CLS] The presence of a clustered thick disk-like component of dark matter in the Galaxy has been suggested by Sanchez-Salcedo (1997, 1999) and Kerins (1997).Kerins, E. J. 1997, Astronomy and Astrophysics, 322, 709-718 (ADS entry )Sánchez-Salcedo, F. J. 1997, Astrophysical Journal, 487, L61-L64 (ADS entry )Sánchez-Salcedo, F. J. 1999, Monthly Notices of the Royal Astronomical Society, 303, 755-772 (ADS entry ) ==See also== * Dark matter * Brown dwarfs * White dwarfs * Microlensing * Hypercompact stellar system * Massive compact halo object (MACHOs) * Weakly interacting massive particles (WIMPs) ==References== Category:Star clusters Category:Open clusters Observations of the Bullet Cluster are the strongest evidence for the existence of dark matter; however, Brownstein and Moffat have shown that their modified gravity theory can also account for the properties of the cluster. == Observational methods == Clusters of galaxies have been found in surveys by a number of observational techniques and have been studied in detail using many methods: * Optical or infrared: The individual galaxies of clusters can be studied through optical or infrared imaging and spectroscopy. The observed distortions can be used to model the distribution of dark matter in the cluster. == Temperature and density == Clusters of galaxies are the most recent and most massive objects to have arisen in the hierarchical structure formation of the Universe and the study of clusters tells one about the way galaxies form and evolve. A 2021 article postulated that approximately 50% of all baryonic matter is outside dark matter haloes, filling the space between galaxies, and that this would explain the missing baryons not accounted for in the 2017 paper. == Current state == Currently, many groups have observed the intergalactic medium and circum-galactic medium to obtain more measurements and observations of baryons to support the leading observations. In cosmology, the missing baryon problem is an observed discrepancy between the amount of baryonic matter detected from shortly after the Big Bang and from more recent epochs. Brownstein and Moffat use a theory of modified gravity to explain X-ray cluster masses without dark matter. The missing baryon problem has been resolved but research groups are working to detect the WHIM using varying methods to confirm results. ==References== Category:Physical cosmology Category:Baryons Baryons make up only ~5% of the universe, while dark matter makes up 26.8%. ==Early universe measurements== The abundance of baryonic matter in the early universe can be obtained indirectly from two independent methods: * The theory of Big Bang nucleosynthesis, which predicts the observed relative abundance of the chemical elements in observations of the recent universe. The missing baryon problem is different from the dark matter problem, which is non-baryonic in nature.See Lambda-CDM model. In a typical cluster perhaps only 5% of the total mass is in the form of galaxies, maybe 10% in the form of hot X-ray emitting gas and the remainder is dark matter. In astronomy, a RAMBO or robust association of massive baryonic objects is a dark cluster made of brown dwarfs or white dwarfs. It is composed of mostly ionized hydrogen and is about 10% of a galaxy cluster's total mass; the rest being dark matter. This is highly nontrivial #### Which of the following statements accurately describes the impact of Modified Newtonian Dynamics (MOND) on the observed \"missing baryonic mass\" discrepancy in galaxy clusters?[SEP] MOND is a theory that reduces the discrepancy between the observed missing baryonic mass in galaxy clusters and the measured velocity dispersions from a factor of around 10 to a factor of about 2.[SEP]\n","786\n","[CLS] The presence of a clustered thick disk-like component of dark matter in the Galaxy has been suggested by Sanchez-Salcedo (1997, 1999) and Kerins (1997).Kerins, E. J. 1997, Astronomy and Astrophysics, 322, 709-718 (ADS entry )Sánchez-Salcedo, F. J. 1997, Astrophysical Journal, 487, L61-L64 (ADS entry )Sánchez-Salcedo, F. J. 1999, Monthly Notices of the Royal Astronomical Society, 303, 755-772 (ADS entry ) ==See also== * Dark matter * Brown dwarfs * White dwarfs * Microlensing * Hypercompact stellar system * Massive compact halo object (MACHOs) * Weakly interacting massive particles (WIMPs) ==References== Category:Star clusters Category:Open clusters Observations of the Bullet Cluster are the strongest evidence for the existence of dark matter; however, Brownstein and Moffat have shown that their modified gravity theory can also account for the properties of the cluster. == Observational methods == Clusters of galaxies have been found in surveys by a number of observational techniques and have been studied in detail using many methods: * Optical or infrared: The individual galaxies of clusters can be studied through optical or infrared imaging and spectroscopy. The observed distortions can be used to model the distribution of dark matter in the cluster. == Temperature and density == Clusters of galaxies are the most recent and most massive objects to have arisen in the hierarchical structure formation of the Universe and the study of clusters tells one about the way galaxies form and evolve. A 2021 article postulated that approximately 50% of all baryonic matter is outside dark matter haloes, filling the space between galaxies, and that this would explain the missing baryons not accounted for in the 2017 paper. == Current state == Currently, many groups have observed the intergalactic medium and circum-galactic medium to obtain more measurements and observations of baryons to support the leading observations. In cosmology, the missing baryon problem is an observed discrepancy between the amount of baryonic matter detected from shortly after the Big Bang and from more recent epochs. Brownstein and Moffat use a theory of modified gravity to explain X-ray cluster masses without dark matter. The missing baryon problem has been resolved but research groups are working to detect the WHIM using varying methods to confirm results. ==References== Category:Physical cosmology Category:Baryons Baryons make up only ~5% of the universe, while dark matter makes up 26.8%. ==Early universe measurements== The abundance of baryonic matter in the early universe can be obtained indirectly from two independent methods: * The theory of Big Bang nucleosynthesis, which predicts the observed relative abundance of the chemical elements in observations of the recent universe. The missing baryon problem is different from the dark matter problem, which is non-baryonic in nature.See Lambda-CDM model. In a typical cluster perhaps only 5% of the total mass is in the form of galaxies, maybe 10% in the form of hot X-ray emitting gas and the remainder is dark matter. In astronomy, a RAMBO or robust association of massive baryonic objects is a dark cluster made of brown dwarfs or white dwarfs. It is composed of mostly ionized hydrogen and is about 10% of a galaxy cluster's total mass; the rest being dark matter. This is highly nontrivial, since although luminous #### Which of the following statements accurately describes the impact of Modified Newtonian Dynamics (MOND) on the observed \"missing baryonic mass\" discrepancy in galaxy clusters?[SEP] MOND is a theory that eliminates the observed missing baryonic mass in galaxy clusters by imposing a new mathematical formulation of gravity that does not require the existence of dark matter.[SEP]\n"]}],"source":["for x in tokenized_test_dataset[0]['input_ids']: \n","    print(len(x))\n","    print(tokenizer.decode(x)) # 许需要加一个 句子修剪\n","    # print(x)\n","# I think tokenized_dataset[0]['input_ids'] is a good example for checking the model's prediction\n","# 因为它的 information retrieval 刚好找到了正确的context，因此需要看看模型是否能从这个context中获取正确答案。\n","# 还应该 check 一下其他的 retrieval 里面，有多少 context 是相关的。\n","# 能不能做一个 multi-hop retrieval:\n","#\n","# "]},{"cell_type":"markdown","metadata":{},"source":["# Compute Validation Score"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.status.busy":"2023-09-02T02:42:35.062122Z","iopub.status.idle":"2023-09-02T02:42:35.06267Z","shell.execute_reply":"2023-09-02T02:42:35.062401Z","shell.execute_reply.started":"2023-09-02T02:42:35.062377Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["CV MAP@3 = 0.8983333333333333\n"]}],"source":["# https://www.kaggle.com/code/philippsinger/h2ogpt-perplexity-ranking\n","import numpy as np\n","def precision_at_k(r, k):\n","    \"\"\"Precision at k\"\"\"\n","    assert k <= len(r)\n","    assert k != 0\n","    return sum(int(x) for x in r[:k]) / k\n","\n","def MAP_at_3(predictions, true_items):\n","    \"\"\"Score is mean average precision at 3\"\"\"\n","    U = len(predictions)\n","    map_at_3 = 0.0\n","    for u in range(U):\n","        user_preds = predictions[u].split()\n","        user_true = true_items[u]\n","        user_results = [1 if item == user_true else 0 for item in user_preds]\n","        for k in range(min(len(user_preds), 3)):\n","            map_at_3 += precision_at_k(user_results, k+1) * user_results[k]\n","    return map_at_3 / U\n","\n","m = MAP_at_3(test_df.prediction.values, test_df.answer.values)\n","print( 'CV MAP@3 =',m )"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
