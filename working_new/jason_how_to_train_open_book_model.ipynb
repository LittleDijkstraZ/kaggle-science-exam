{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "acC1C2sWpOXV"
      },
      "source": [
        "# How To Train Model for Open Book Q&A Technique\n",
        "In this notebook we demonstrate how to train a model to be used with top scoring Open Book Q&A method. The Open Book method was first presented by JJ (@jjinho) [here][1], then Quangteo (@quangbk) improved RAM usage [here][2], and Anil (@nlztrk) combined with Q&A [here][3]. Radek (@radek1) demonstrated the strength of Q&A [here][5]. Next Mgoksu (@mgoksu) demonstrated how to achieve top public LB=0.807 using this method [here][4] by finetuning DeBerta large on this method.\n",
        "\n",
        "In order to train a model for use with Open Book Q&A, we need a CSV that contains; `prompt` (i.e. question), `A, B, C, D, E` (i.e. answer choices), and we need a column of `context` extracted from wikipedia pages for each question. To generate the `context` column, we run Mgoksu's notebook [here][4]. In code cell #5, we load our CSV without `context` column with code `trn = pd.read_csv(OUR_DATASET.CSV)`. Then in code cell #21 our dataset is saved to disk as `test_context.csv` with the column `context` added.\n",
        "\n",
        "I have searched and concatenated all publicly shared datasets into one 60k CSV and then ran Mgoksu's notebook with `NUM_TITLES_INCLUDE = 5` and `NUM_SENTENCES_INCLUDE = 20`. This added an additional `context` column. I uploaded the resultant CSV file to a Kaggle dataset [here][6]. If you enjoy the notebook you are reading, please upvote the dataset too. Thanks!\n",
        "\n",
        "![](https://miro.medium.com/v2/resize:fit:800/format:webp/1*bTGY3fKIgNefQxNsOYpnBw.png)\n",
        "\n",
        "(image source [here][7])\n",
        "\n",
        "[1]: https://www.kaggle.com/code/jjinho/open-book-llm-science-exam\n",
        "[2]: https://www.kaggle.com/code/quangbk/open-book-llm-science-exam-reduced-ram-usage\n",
        "[3]: https://www.kaggle.com/code/nlztrk/openbook-debertav3-large-baseline-single-model\n",
        "[4]: https://www.kaggle.com/code/mgoksu/0-807-sharing-my-trained-with-context-model\n",
        "[5]: https://www.kaggle.com/code/radek1/new-dataset-deberta-v3-large-training\n",
        "[6]: https://www.kaggle.com/datasets/cdeotte/60k-data-with-context-v2\n",
        "[7]: https://blog.gopenai.com/enrich-llms-with-retrieval-augmented-generation-rag-17b82a96b6f0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wm9oVgjOpOXX"
      },
      "source": [
        "# Load CSV\n",
        "We will load 60k CSV of `prompts`, `A,B,C,D,E`, and `context` from my Kaggle dataset [here][1]. This dataset is all publicly shared datasets concatenated then processed with Mgoksu's notebook [here][2] to create a `context` column. (To learn more about the datasets within read my discussion post). This Kaggle dataset also contains competition `train.csv` with added `context` column (to be used as a validation dataset).\n",
        "\n",
        "In this train notebook, we have internet turned on and can choose whatever model we wish to download and train. After we finetune this model, we will create a second notebook with the Open Book Q&A technique and load the finetuned model from the output of this notebook. The second notebook will have internet turned off so that it can be submitted to Kaggle's competition.\n",
        "\n",
        "[1]: https://www.kaggle.com/datasets/cdeotte/60k-data-with-context-v2\n",
        "[2]: https://www.kaggle.com/code/mgoksu/0-807-sharing-my-trained-with-context-model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-09-10T15:51:09.289992Z",
          "iopub.status.busy": "2023-09-10T15:51:09.289234Z",
          "iopub.status.idle": "2023-09-10T15:51:09.301973Z",
          "shell.execute_reply": "2023-09-10T15:51:09.300654Z",
          "shell.execute_reply.started": "2023-09-10T15:51:09.289938Z"
        },
        "id": "2-BKwNZYpOXX",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
        "\n",
        "from typing import Optional, Union\n",
        "import pandas as pd, numpy as np, torch\n",
        "from datasets import Dataset # 这个的使用方法 hugging face 上面有教程\n",
        "from dataclasses import dataclass\n",
        "from transformers import AutoTokenizer\n",
        "from transformers import EarlyStoppingCallback\n",
        "from transformers.tokenization_utils_base import PreTrainedTokenizerBase, PaddingStrategy\n",
        "from transformers import AutoModelForMultipleChoice, TrainingArguments, Trainer\n",
        "\n",
        "VER='repeat_V3'\n",
        "# TRAIN WITH SUBSET OF 60K\n",
        "# NUM_TRAIN_SAMPLES = 1_024\n",
        "NUM_TRAIN_SAMPLES = None\n",
        "\n",
        "# PARAMETER EFFICIENT FINE TUNING\n",
        "# PEFT REQUIRES 1XP100 GPU NOT 2XT4\n",
        "USE_PEFT = True\n",
        "# USE_PEFT = True # 这个的全称是 pretrained efficient finetuning, hugging face 上面有教程\n",
        "\n",
        "# NUMBER OF LAYERS TO FREEZE\n",
        "# DEBERTA LARGE HAS TOTAL OF 24 LAYERS\n",
        "FREEZE_LAYERS = 18\n",
        "# FREEZE_LAYERS = 24\n",
        "# FREEZE_LAYERS = 20\n",
        "\n",
        "\n",
        "# BOOLEAN TO FREEZE EMBEDDINGS\n",
        "FREEZE_EMBEDDINGS = True\n",
        "# LENGTH OF CONTEXT PLUS QUESTION ANSWER\n",
        "# 我需要搞懂这个长度到底指的是什么，尤其是 context 和 question 的长度是怎么分配的。256 不可能 cover 全部。\n",
        "# 因为如果模型没能在足够长的 input 中训练，那么positional encoding 很差的模型就不好 extrapolate\n",
        "# MAX_INPUT = 256\n",
        "MAX_INPUT = 512 # 调整这个的大小的时候，每次都需要重新跑一下dataset\n",
        "\n",
        "# HUGGING FACE MODEL\n",
        "MODEL = 'microsoft/deberta-v3-large'\n",
        "\n",
        "checkpoint_folder = MODEL.split('/')[-1] + '_checkpoints'\n",
        "dataset_folder = MODEL.split('/')[-1] + '_datasets'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-09-10T15:50:47.929122Z",
          "iopub.status.busy": "2023-09-10T15:50:47.928767Z",
          "iopub.status.idle": "2023-09-10T15:50:48.010881Z",
          "shell.execute_reply": "2023-09-10T15:50:48.009882Z",
          "shell.execute_reply.started": "2023-09-10T15:50:47.929092Z"
        },
        "id": "oE5C4UcDpOXY",
        "outputId": "96e5d5db-2376-49b8-ccfd-3b61ba607069",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation data size: (200, 8)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>prompt</th>\n",
              "      <th>context</th>\n",
              "      <th>A</th>\n",
              "      <th>B</th>\n",
              "      <th>C</th>\n",
              "      <th>D</th>\n",
              "      <th>E</th>\n",
              "      <th>answer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Which of the following statements accurately d...</td>\n",
              "      <td>The presence of a clustered thick disk-like co...</td>\n",
              "      <td>MOND is a theory that reduces the observed mis...</td>\n",
              "      <td>MOND is a theory that increases the discrepanc...</td>\n",
              "      <td>MOND is a theory that explains the missing bar...</td>\n",
              "      <td>MOND is a theory that reduces the discrepancy ...</td>\n",
              "      <td>MOND is a theory that eliminates the observed ...</td>\n",
              "      <td>D</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Which of the following is an accurate definiti...</td>\n",
              "      <td>Many of these systems evolve in a self-similar...</td>\n",
              "      <td>Dynamic scaling refers to the evolution of sel...</td>\n",
              "      <td>Dynamic scaling refers to the non-evolution of...</td>\n",
              "      <td>Dynamic scaling refers to the evolution of sel...</td>\n",
              "      <td>Dynamic scaling refers to the non-evolution of...</td>\n",
              "      <td>Dynamic scaling refers to the evolution of sel...</td>\n",
              "      <td>A</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Which of the following statements accurately d...</td>\n",
              "      <td>It is possible that this usage is related with...</td>\n",
              "      <td>The triskeles symbol was reconstructed as a fe...</td>\n",
              "      <td>The triskeles symbol is a representation of th...</td>\n",
              "      <td>The triskeles symbol is a representation of a ...</td>\n",
              "      <td>The triskeles symbol represents three interloc...</td>\n",
              "      <td>The triskeles symbol is a representation of th...</td>\n",
              "      <td>A</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>What is the significance of regularization in ...</td>\n",
              "      <td>Renormalization is distinct from regularizatio...</td>\n",
              "      <td>Regularizing the mass-energy of an electron wi...</td>\n",
              "      <td>Regularizing the mass-energy of an electron wi...</td>\n",
              "      <td>Regularizing the mass-energy of an electron wi...</td>\n",
              "      <td>Regularizing the mass-energy of an electron wi...</td>\n",
              "      <td>Regularizing the mass-energy of an electron wi...</td>\n",
              "      <td>C</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Which of the following statements accurately d...</td>\n",
              "      <td>Several qualitative observations can be made o...</td>\n",
              "      <td>The angular spacing of features in the diffrac...</td>\n",
              "      <td>The angular spacing of features in the diffrac...</td>\n",
              "      <td>The angular spacing of features in the diffrac...</td>\n",
              "      <td>The angular spacing of features in the diffrac...</td>\n",
              "      <td>The angular spacing of features in the diffrac...</td>\n",
              "      <td>D</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>195</th>\n",
              "      <td>What is the relation between the three moment ...</td>\n",
              "      <td>The second equation is more general as it does...</td>\n",
              "      <td>The three moment theorem expresses the relatio...</td>\n",
              "      <td>The three moment theorem is used to calculate ...</td>\n",
              "      <td>The three moment theorem describes the relatio...</td>\n",
              "      <td>The three moment theorem is used to calculate ...</td>\n",
              "      <td>The three moment theorem is used to derive the...</td>\n",
              "      <td>C</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>196</th>\n",
              "      <td>What is the throttling process, and why is it ...</td>\n",
              "      <td>A throttle is the mechanism by which fluid flo...</td>\n",
              "      <td>The throttling process is a steady flow of a f...</td>\n",
              "      <td>The throttling process is a steady adiabatic f...</td>\n",
              "      <td>The throttling process is a steady adiabatic f...</td>\n",
              "      <td>The throttling process is a steady flow of a f...</td>\n",
              "      <td>The throttling process is a steady adiabatic f...</td>\n",
              "      <td>B</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>197</th>\n",
              "      <td>What happens to excess base metal as a solutio...</td>\n",
              "      <td>Furthermore, this melting may begin at a tempe...</td>\n",
              "      <td>The excess base metal will often solidify, bec...</td>\n",
              "      <td>The excess base metal will often crystallize-o...</td>\n",
              "      <td>The excess base metal will often dissolve, bec...</td>\n",
              "      <td>The excess base metal will often liquefy, beco...</td>\n",
              "      <td>The excess base metal will often evaporate, be...</td>\n",
              "      <td>B</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>198</th>\n",
              "      <td>What is the relationship between mass, force, ...</td>\n",
              "      <td>Newton first set out the definition of mass Th...</td>\n",
              "      <td>Mass is a property that determines the weight ...</td>\n",
              "      <td>Mass is an inertial property that determines a...</td>\n",
              "      <td>Mass is an inertial property that determines a...</td>\n",
              "      <td>Mass is an inertial property that determines a...</td>\n",
              "      <td>Mass is a property that determines the size of...</td>\n",
              "      <td>D</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>199</th>\n",
              "      <td>What did Arthur Eddington discover about two o...</td>\n",
              "      <td>Eddington's criticism seems to have been based...</td>\n",
              "      <td>Arthur Eddington showed that two of Einstein's...</td>\n",
              "      <td>Arthur Eddington showed that two of Einstein's...</td>\n",
              "      <td>Arthur Eddington showed that two of Einstein's...</td>\n",
              "      <td>Arthur Eddington showed that two of Einstein's...</td>\n",
              "      <td>Arthur Eddington showed that two of Einstein's...</td>\n",
              "      <td>C</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>200 rows × 8 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                prompt  \\\n",
              "0    Which of the following statements accurately d...   \n",
              "1    Which of the following is an accurate definiti...   \n",
              "2    Which of the following statements accurately d...   \n",
              "3    What is the significance of regularization in ...   \n",
              "4    Which of the following statements accurately d...   \n",
              "..                                                 ...   \n",
              "195  What is the relation between the three moment ...   \n",
              "196  What is the throttling process, and why is it ...   \n",
              "197  What happens to excess base metal as a solutio...   \n",
              "198  What is the relationship between mass, force, ...   \n",
              "199  What did Arthur Eddington discover about two o...   \n",
              "\n",
              "                                               context  \\\n",
              "0    The presence of a clustered thick disk-like co...   \n",
              "1    Many of these systems evolve in a self-similar...   \n",
              "2    It is possible that this usage is related with...   \n",
              "3    Renormalization is distinct from regularizatio...   \n",
              "4    Several qualitative observations can be made o...   \n",
              "..                                                 ...   \n",
              "195  The second equation is more general as it does...   \n",
              "196  A throttle is the mechanism by which fluid flo...   \n",
              "197  Furthermore, this melting may begin at a tempe...   \n",
              "198  Newton first set out the definition of mass Th...   \n",
              "199  Eddington's criticism seems to have been based...   \n",
              "\n",
              "                                                     A  \\\n",
              "0    MOND is a theory that reduces the observed mis...   \n",
              "1    Dynamic scaling refers to the evolution of sel...   \n",
              "2    The triskeles symbol was reconstructed as a fe...   \n",
              "3    Regularizing the mass-energy of an electron wi...   \n",
              "4    The angular spacing of features in the diffrac...   \n",
              "..                                                 ...   \n",
              "195  The three moment theorem expresses the relatio...   \n",
              "196  The throttling process is a steady flow of a f...   \n",
              "197  The excess base metal will often solidify, bec...   \n",
              "198  Mass is a property that determines the weight ...   \n",
              "199  Arthur Eddington showed that two of Einstein's...   \n",
              "\n",
              "                                                     B  \\\n",
              "0    MOND is a theory that increases the discrepanc...   \n",
              "1    Dynamic scaling refers to the non-evolution of...   \n",
              "2    The triskeles symbol is a representation of th...   \n",
              "3    Regularizing the mass-energy of an electron wi...   \n",
              "4    The angular spacing of features in the diffrac...   \n",
              "..                                                 ...   \n",
              "195  The three moment theorem is used to calculate ...   \n",
              "196  The throttling process is a steady adiabatic f...   \n",
              "197  The excess base metal will often crystallize-o...   \n",
              "198  Mass is an inertial property that determines a...   \n",
              "199  Arthur Eddington showed that two of Einstein's...   \n",
              "\n",
              "                                                     C  \\\n",
              "0    MOND is a theory that explains the missing bar...   \n",
              "1    Dynamic scaling refers to the evolution of sel...   \n",
              "2    The triskeles symbol is a representation of a ...   \n",
              "3    Regularizing the mass-energy of an electron wi...   \n",
              "4    The angular spacing of features in the diffrac...   \n",
              "..                                                 ...   \n",
              "195  The three moment theorem describes the relatio...   \n",
              "196  The throttling process is a steady adiabatic f...   \n",
              "197  The excess base metal will often dissolve, bec...   \n",
              "198  Mass is an inertial property that determines a...   \n",
              "199  Arthur Eddington showed that two of Einstein's...   \n",
              "\n",
              "                                                     D  \\\n",
              "0    MOND is a theory that reduces the discrepancy ...   \n",
              "1    Dynamic scaling refers to the non-evolution of...   \n",
              "2    The triskeles symbol represents three interloc...   \n",
              "3    Regularizing the mass-energy of an electron wi...   \n",
              "4    The angular spacing of features in the diffrac...   \n",
              "..                                                 ...   \n",
              "195  The three moment theorem is used to calculate ...   \n",
              "196  The throttling process is a steady flow of a f...   \n",
              "197  The excess base metal will often liquefy, beco...   \n",
              "198  Mass is an inertial property that determines a...   \n",
              "199  Arthur Eddington showed that two of Einstein's...   \n",
              "\n",
              "                                                     E answer  \n",
              "0    MOND is a theory that eliminates the observed ...      D  \n",
              "1    Dynamic scaling refers to the evolution of sel...      A  \n",
              "2    The triskeles symbol is a representation of th...      A  \n",
              "3    Regularizing the mass-energy of an electron wi...      C  \n",
              "4    The angular spacing of features in the diffrac...      D  \n",
              "..                                                 ...    ...  \n",
              "195  The three moment theorem is used to derive the...      C  \n",
              "196  The throttling process is a steady adiabatic f...      B  \n",
              "197  The excess base metal will often evaporate, be...      B  \n",
              "198  Mass is a property that determines the size of...      D  \n",
              "199  Arthur Eddington showed that two of Einstein's...      C  \n",
              "\n",
              "[200 rows x 8 columns]"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_valid = pd.read_csv('../input/60k-data-with-context-v2/train_with_context2.csv')\n",
        "print('Validation data size:', df_valid.shape )\n",
        "df_valid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-09-10T15:52:03.937771Z",
          "iopub.status.busy": "2023-09-10T15:52:03.936994Z",
          "iopub.status.idle": "2023-09-10T15:52:11.860278Z",
          "shell.execute_reply": "2023-09-10T15:52:11.859288Z",
          "shell.execute_reply.started": "2023-09-10T15:52:03.937732Z"
        },
        "id": "hRQBsmRlpOXZ",
        "outputId": "e0e6a537-0e94-4acf-95a8-3f568324617a",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "size of dataset 97687\n",
            "Train data size: (97687, 9)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>prompt</th>\n",
              "      <th>context</th>\n",
              "      <th>A</th>\n",
              "      <th>B</th>\n",
              "      <th>C</th>\n",
              "      <th>D</th>\n",
              "      <th>answer</th>\n",
              "      <th>is_question</th>\n",
              "      <th>dataset</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>We can know from the passage that the author w...</td>\n",
              "      <td>Last week I talked with some of my students ab...</td>\n",
              "      <td>doctor</td>\n",
              "      <td>model</td>\n",
              "      <td>teacher</td>\n",
              "      <td>reporter</td>\n",
              "      <td>C</td>\n",
              "      <td>False</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Many graduates today turn to cosmetic surgery ...</td>\n",
              "      <td>Last week I talked with some of my students ab...</td>\n",
              "      <td>marry a better man/woman</td>\n",
              "      <td>become a model</td>\n",
              "      <td>get an advantage over others in job-hunting</td>\n",
              "      <td>attract more admirers</td>\n",
              "      <td>C</td>\n",
              "      <td>False</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>According to the passage, the author believes ...</td>\n",
              "      <td>Last week I talked with some of my students ab...</td>\n",
              "      <td>everyone should purchase perfection, whatever ...</td>\n",
              "      <td>it's right for graduates to ask for others to ...</td>\n",
              "      <td>it is one's appearance instead of skills that ...</td>\n",
              "      <td>media are to blame for misleading young people...</td>\n",
              "      <td>D</td>\n",
              "      <td>False</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Which' s the best title for the passage?.</td>\n",
              "      <td>Last week I talked with some of my students ab...</td>\n",
              "      <td>Young Graduates Have Higher Expectations</td>\n",
              "      <td>Young Graduates Look to Surgery for Better Jobs</td>\n",
              "      <td>Young Graduates' Opinion About Cosmetic Surgery</td>\n",
              "      <td>Young Graduates Face a Different Situation in ...</td>\n",
              "      <td>B</td>\n",
              "      <td>True</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>What could be the best title for this passage?</td>\n",
              "      <td>YUZHOU, HENAN -An accident in a central China ...</td>\n",
              "      <td>Death Toll Rises in an Accident in China</td>\n",
              "      <td>A Coal Mine Accident in Central China</td>\n",
              "      <td>An Accident in Central China</td>\n",
              "      <td>Coal Mine Accidents in China</td>\n",
              "      <td>B</td>\n",
              "      <td>True</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>97682</th>\n",
              "      <td>From the passage, we know that the writer   _  .</td>\n",
              "      <td>One day in the eighth grade, I was taking a Ma...</td>\n",
              "      <td>could read Serbian</td>\n",
              "      <td>didn't cheat at last</td>\n",
              "      <td>got a good grade at last</td>\n",
              "      <td>didn't work hard that night</td>\n",
              "      <td>B</td>\n",
              "      <td>False</td>\n",
              "      <td>test</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>97683</th>\n",
              "      <td>It takes   _   for the moon to go around the e...</td>\n",
              "      <td>When you look at the sky at night, the moon lo...</td>\n",
              "      <td>more than a week</td>\n",
              "      <td>nearly a month</td>\n",
              "      <td>half a year</td>\n",
              "      <td>more than a year</td>\n",
              "      <td>B</td>\n",
              "      <td>False</td>\n",
              "      <td>test</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>97684</th>\n",
              "      <td>The moon is bright because   _  .</td>\n",
              "      <td>When you look at the sky at night, the moon lo...</td>\n",
              "      <td>there is some fire on it</td>\n",
              "      <td>it is near the sun, too</td>\n",
              "      <td>there is a big mirror on it</td>\n",
              "      <td>it can get light from the sun</td>\n",
              "      <td>D</td>\n",
              "      <td>False</td>\n",
              "      <td>test</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>97685</th>\n",
              "      <td>The passage tells us   _  .</td>\n",
              "      <td>When you look at the sky at night, the moon lo...</td>\n",
              "      <td>the sun is not the biggest star</td>\n",
              "      <td>the sun is bigger than any other star</td>\n",
              "      <td>only the sun can shine</td>\n",
              "      <td>the sun is one of the farthest stars to us</td>\n",
              "      <td>A</td>\n",
              "      <td>False</td>\n",
              "      <td>test</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>97686</th>\n",
              "      <td>Why do we call the sun \"a life giving star\"?</td>\n",
              "      <td>When you look at the sky at night, the moon lo...</td>\n",
              "      <td>Because it gives us warmth.</td>\n",
              "      <td>Because there are some living things on it.</td>\n",
              "      <td>Because plants and animals can't live without it.</td>\n",
              "      <td>Because people have to live in the dark withou...</td>\n",
              "      <td>C</td>\n",
              "      <td>True</td>\n",
              "      <td>test</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>97687 rows × 9 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  prompt  \\\n",
              "0      We can know from the passage that the author w...   \n",
              "1      Many graduates today turn to cosmetic surgery ...   \n",
              "2      According to the passage, the author believes ...   \n",
              "3              Which' s the best title for the passage?.   \n",
              "4         What could be the best title for this passage?   \n",
              "...                                                  ...   \n",
              "97682   From the passage, we know that the writer   _  .   \n",
              "97683  It takes   _   for the moon to go around the e...   \n",
              "97684                  The moon is bright because   _  .   \n",
              "97685                        The passage tells us   _  .   \n",
              "97686       Why do we call the sun \"a life giving star\"?   \n",
              "\n",
              "                                                 context  \\\n",
              "0      Last week I talked with some of my students ab...   \n",
              "1      Last week I talked with some of my students ab...   \n",
              "2      Last week I talked with some of my students ab...   \n",
              "3      Last week I talked with some of my students ab...   \n",
              "4      YUZHOU, HENAN -An accident in a central China ...   \n",
              "...                                                  ...   \n",
              "97682  One day in the eighth grade, I was taking a Ma...   \n",
              "97683  When you look at the sky at night, the moon lo...   \n",
              "97684  When you look at the sky at night, the moon lo...   \n",
              "97685  When you look at the sky at night, the moon lo...   \n",
              "97686  When you look at the sky at night, the moon lo...   \n",
              "\n",
              "                                                       A  \\\n",
              "0                                                 doctor   \n",
              "1                               marry a better man/woman   \n",
              "2      everyone should purchase perfection, whatever ...   \n",
              "3               Young Graduates Have Higher Expectations   \n",
              "4               Death Toll Rises in an Accident in China   \n",
              "...                                                  ...   \n",
              "97682                                 could read Serbian   \n",
              "97683                                   more than a week   \n",
              "97684                           there is some fire on it   \n",
              "97685                    the sun is not the biggest star   \n",
              "97686                        Because it gives us warmth.   \n",
              "\n",
              "                                                       B  \\\n",
              "0                                                  model   \n",
              "1                                         become a model   \n",
              "2      it's right for graduates to ask for others to ...   \n",
              "3        Young Graduates Look to Surgery for Better Jobs   \n",
              "4                  A Coal Mine Accident in Central China   \n",
              "...                                                  ...   \n",
              "97682                               didn't cheat at last   \n",
              "97683                                     nearly a month   \n",
              "97684                            it is near the sun, too   \n",
              "97685              the sun is bigger than any other star   \n",
              "97686        Because there are some living things on it.   \n",
              "\n",
              "                                                       C  \\\n",
              "0                                                teacher   \n",
              "1            get an advantage over others in job-hunting   \n",
              "2      it is one's appearance instead of skills that ...   \n",
              "3        Young Graduates' Opinion About Cosmetic Surgery   \n",
              "4                           An Accident in Central China   \n",
              "...                                                  ...   \n",
              "97682                           got a good grade at last   \n",
              "97683                                        half a year   \n",
              "97684                        there is a big mirror on it   \n",
              "97685                             only the sun can shine   \n",
              "97686  Because plants and animals can't live without it.   \n",
              "\n",
              "                                                       D answer  is_question  \\\n",
              "0                                               reporter      C        False   \n",
              "1                                  attract more admirers      C        False   \n",
              "2      media are to blame for misleading young people...      D        False   \n",
              "3      Young Graduates Face a Different Situation in ...      B         True   \n",
              "4                           Coal Mine Accidents in China      B         True   \n",
              "...                                                  ...    ...          ...   \n",
              "97682                        didn't work hard that night      B        False   \n",
              "97683                                   more than a year      B        False   \n",
              "97684                      it can get light from the sun      D        False   \n",
              "97685         the sun is one of the farthest stars to us      A        False   \n",
              "97686  Because people have to live in the dark withou...      C         True   \n",
              "\n",
              "      dataset  \n",
              "0       train  \n",
              "1       train  \n",
              "2       train  \n",
              "3       train  \n",
              "4       train  \n",
              "...       ...  \n",
              "97682    test  \n",
              "97683    test  \n",
              "97684    test  \n",
              "97685    test  \n",
              "97686    test  \n",
              "\n",
              "[97687 rows x 9 columns]"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# df_train = pd.read_csv('..//input/60k-data-with-context-v2/all_12_with_context2.csv')\n",
        "df_train = pd.read_csv('../input/99k-context/RACE_with_context_original.csv')\n",
        "\n",
        "print('size of dataset', len(df_train))\n",
        "# df_train = df_train.drop(columns=\"source\")\n",
        "if 'source' in df_train.columns:\n",
        "    df_train = df_train.drop(columns=\"source\")\n",
        "df_train = df_train.fillna('')\n",
        "if NUM_TRAIN_SAMPLES:\n",
        "    df_train = df_train.sample(NUM_TRAIN_SAMPLES) # taken NUM_TRAIN_SAMPLES of samples here\n",
        "print('Train data size:', df_train.shape )\n",
        "df_train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GctqBhbbpOXZ"
      },
      "source": [
        "# Data Loader\n",
        "Code is from Radek's notebook [here][1] with modifications to the tokenization process.\n",
        "\n",
        "[1]: https://www.kaggle.com/code/radek1/new-dataset-deberta-v3-large-training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-09-10T15:52:22.561293Z",
          "iopub.status.busy": "2023-09-10T15:52:22.560935Z",
          "iopub.status.idle": "2023-09-10T15:52:22.575102Z",
          "shell.execute_reply": "2023-09-10T15:52:22.574170Z",
          "shell.execute_reply.started": "2023-09-10T15:52:22.561265Z"
        },
        "id": "WIt-GqLApOXZ",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "option_to_index = {option: idx for idx, option in enumerate('ABCDE')}\n",
        "index_to_option = {v: k for k,v in option_to_index.items()}\n",
        "\n",
        "# 等于说训练的时候模型是可以看到context的，因此与预测保持一致\n",
        "def preprocess(example, tokenizer):\n",
        "    first_sentence = [ \"[CLS] \" + example['context'] ] * 5\n",
        "    second_sentences = [\" #### \" + example['prompt'] + \" [SEP] \" + example[option] + \" [SEP]\" for option in 'ABCDE']\n",
        "    tokenized_example = tokenizer(first_sentence, second_sentences, truncation='only_first',\n",
        "                                  max_length=MAX_INPUT, add_special_tokens=False)\n",
        "    tokenized_example['label'] = option_to_index[example['answer']]\n",
        "\n",
        "    return tokenized_example\n",
        "\n",
        "def race_preprocess(example, tokenizer, max_length=None):\n",
        "    context = example[\"context\"].replace(\"\\n\", \" \")\n",
        "    first_sentence = [\"[CLS] \" + context] * 4\n",
        "    second_sentences = [\n",
        "        \" #### \" + example[\"prompt\"] + \" [SEP] \" + example[option] + \" [SEP]\"\n",
        "        for option in \"ABCD\"\n",
        "    ]\n",
        "    tokenized_example = tokenizer(\n",
        "        first_sentence,\n",
        "        second_sentences,\n",
        "        truncation=\"only_first\",\n",
        "        max_length=max_length,\n",
        "        add_special_tokens=False,\n",
        "    )\n",
        "    tokenized_example[\"label\"] = option_to_index[example[\"answer\"]]\n",
        "\n",
        "    return tokenized_example\n",
        "\n",
        "@dataclass\n",
        "class DataCollatorForMultipleChoice:\n",
        "    tokenizer: PreTrainedTokenizerBase\n",
        "    padding: Union[bool, str, PaddingStrategy] = True\n",
        "    max_length: Optional[int] = None\n",
        "    pad_to_multiple_of: Optional[int] = None\n",
        "    \n",
        "    def __call__(self, features):\n",
        "        label_name = 'label' if 'label' in features[0].keys() else 'labels'\n",
        "        labels = [feature.pop(label_name) for feature in features]\n",
        "        batch_size = len(features)\n",
        "        num_choices = len(features[0]['input_ids'])\n",
        "        flattened_features = [\n",
        "            [{k: v[i] for k, v in feature.items()} for i in range(num_choices)] for feature in features\n",
        "        ]\n",
        "        flattened_features = sum(flattened_features, [])\n",
        "        \n",
        "        # Huggingface tokenizer padding\n",
        "        batch = self.tokenizer.pad(\n",
        "            flattened_features,\n",
        "            padding=self.padding,\n",
        "            max_length=self.max_length,\n",
        "            pad_to_multiple_of=self.pad_to_multiple_of, # this is related to mixed precision training\n",
        "            return_tensors='pt',\n",
        "        )\n",
        "\n",
        "        batch = {k: v.view(batch_size, num_choices, -1) for k, v in batch.items()}\n",
        "        batch['labels'] = torch.tensor(labels, dtype=torch.int64)\n",
        "        return batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-09-10T15:52:29.894379Z",
          "iopub.status.busy": "2023-09-10T15:52:29.894012Z",
          "iopub.status.idle": "2023-09-10T15:52:31.881652Z",
          "shell.execute_reply": "2023-09-10T15:52:31.880578Z",
          "shell.execute_reply.started": "2023-09-10T15:52:29.894346Z"
        },
        "id": "HAwYv9UgpOXZ",
        "outputId": "43b1daa9-d0be-475f-c162-a6070a9d277a",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "/home/dijkstraz/anaconda3/envs/kaggle/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:470: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
            "  warnings.warn(\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "DebertaV2TokenizerFast(name_or_path='microsoft/deberta-v3-large', vocab_size=128000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '[CLS]', 'eos_token': '[SEP]', 'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True)"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 每个 tokenizer 都是 pretrained model 在 pretraining 的时候使用的\n",
        "# 在 tokenizer 的 special tokens 中, 注意到 [CLS] 同时是 bos 和 cls token; cls_token 是为了让模型知道要做 classification 了\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
        "tokenizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-09-10T15:55:08.821857Z",
          "iopub.status.busy": "2023-09-10T15:55:08.821159Z",
          "iopub.status.idle": "2023-09-10T15:55:09.689547Z",
          "shell.execute_reply": "2023-09-10T15:55:09.688542Z",
          "shell.execute_reply.started": "2023-09-10T15:55:08.821820Z"
        },
        "id": "6vtlJkf_pOXZ",
        "outputId": "e3f30554-cb76-4ee7-acc6-66a6e03edcf6",
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['prompt', 'context', 'A', 'B', 'C', 'D', 'answer', 'is_question', 'dataset'],\n",
              "    num_rows: 97687\n",
              "})"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset_valid = Dataset.from_pandas(df_valid)\n",
        "dataset = Dataset.from_pandas(df_train)\n",
        "if '__index_level_0__' in dataset._info.features: # 加了这行防爆\n",
        "    print('removing __index_level_0__')\n",
        "    dataset = dataset.remove_columns([\"__index_level_0__\"])\n",
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "5ec15ebf654445758191ae45232728b8"
          ]
        },
        "execution": {
          "iopub.execute_input": "2023-09-10T15:55:13.711687Z",
          "iopub.status.busy": "2023-09-10T15:55:13.711294Z",
          "iopub.status.idle": "2023-09-10T15:55:36.510447Z",
          "shell.execute_reply": "2023-09-10T15:55:36.508573Z",
          "shell.execute_reply.started": "2023-09-10T15:55:13.711657Z"
        },
        "id": "bM1YxNYqpOXa",
        "outputId": "57e233ff-9cc1-4c3d-85e4-112a29151ad4",
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c34e4dfe2b2548cd99964b3563563eba",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "getting tokenized_dataset_512 from disk\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['input_ids', 'token_type_ids', 'attention_mask', 'label'],\n",
              "    num_rows: 97687\n",
              "})"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from functools import partial\n",
        "preprocess = partial(preprocess, tokenizer=tokenizer)\n",
        "race_preprocess = partial(race_preprocess, tokenizer=tokenizer)\n",
        "\n",
        "tokenized_dataset_valid = dataset_valid.map(preprocess, remove_columns=['prompt', 'context', 'A', 'B', 'C', 'D', 'E', 'answer'])\n",
        "\n",
        "num_choices = len([x for x in dataset.column_names if x in 'ABCDEFG'])\n",
        "dataset_path = f'./{dataset_folder}/tokenized_dataset_{MAX_INPUT}_{num_choices}'\n",
        "if os.path.exists(dataset_path):\n",
        "    print(f'getting tokenized_dataset_{MAX_INPUT} from disk')\n",
        "    tokenized_dataset = Dataset.load_from_disk(dataset_path)\n",
        "else:\n",
        "    if num_choices == 5:\n",
        "        tokenized_dataset = dataset.map(preprocess, remove_columns=dataset.column_names)\n",
        "    elif num_choices == 4:\n",
        "        tokenized_dataset = dataset.map(race_preprocess, remove_columns=dataset.column_names)\n",
        "        \n",
        "    tokenized_dataset.save_to_disk(dataset_path)\n",
        "\n",
        "# changed by cxzheng, fuck! stucked!\n",
        "\n",
        "tokenized_dataset # 他跑到 21100 附近的时候会卡住，有点奇怪"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V6VMqZv8pOXa"
      },
      "source": [
        "# MAP@3 Metric\n",
        "The competition metric is MAP@3 therefore we will make a custom code to add to Hugging Face's trainer. Discussion [here][1]\n",
        "\n",
        "[1]: https://www.kaggle.com/competitions/kaggle-llm-science-exam/discussion/435602"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-09-09T03:47:36.361768Z",
          "iopub.status.busy": "2023-09-09T03:47:36.361320Z",
          "iopub.status.idle": "2023-09-09T03:47:36.378492Z",
          "shell.execute_reply": "2023-09-09T03:47:36.377185Z",
          "shell.execute_reply.started": "2023-09-09T03:47:36.361728Z"
        },
        "id": "Is1hFh9KpOXa",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def map_at_3(predictions, labels):\n",
        "    map_sum = 0\n",
        "    pred = np.argsort(-1*np.array(predictions),axis=1)[:,:3]\n",
        "    for x,y in zip(pred,labels):\n",
        "        z = [1/i if y==j else 0 for i,j in zip([1,2,3],x)]\n",
        "        map_sum += np.sum(z)\n",
        "    return map_sum / len(predictions)\n",
        "\n",
        "def compute_metrics(p):\n",
        "    predictions = p.predictions.tolist()\n",
        "    labels = p.label_ids.tolist()\n",
        "    return {\"map@3\": map_at_3(predictions, labels)}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Drq0chcepOXa"
      },
      "source": [
        "# Train and Save\n",
        "We will now train and save our model using Hugging Face's easy to use trainer. By adjusting the parameters in this notebook, we can achieve `CV MAP@3 = 0.915+` and corresponding single model `LB MAP@3 = 0.830+` wow!\n",
        "\n",
        "In we run this notebook outside of Kaggle then we can train longer and with more RAM. If we run this notebook on Kaggle, then we need to use tricks to train models efficiently. Here are some ideas:\n",
        "* use fp16 (this speeds up T4 not P100)\n",
        "* use gradient_accumlation_steps (this simulates larger batch sizes)\n",
        "* use gradient_checkpointing (this uses disk to save RAM)\n",
        "* use 2xT4 instead of 1xP100 (this doubles GPUs)\n",
        "* freeze model embeddings (this reduces weights to train)\n",
        "* freeze some model layers (this reduces weights to train)\n",
        "* use PEFT (this reduces weights to train)\n",
        "* increase LR and decrease epochs (this reduces work)\n",
        "* use smaller models (this reduces weights to train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gu-ItbRYpOXa"
      },
      "source": [
        "We will use a Hugging Face AutoModelForMultipleChoice. For the list of possible models, see Hugging Face's repository [here][1]. We can optionally use PEFT to accelerate training and use less memory. However i have noticed that validation accuracy is less. (Note that PEFT requires us to use 1xP100 not 2xT4 GPU. I'm not sure why). We can also optionally freeze layers. This also accelerates training and uses less memory. However validation accuracy may become less.\n",
        "\n",
        "[1]: https://huggingface.co/models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9x6IM3xDpOXa"
      },
      "outputs": [],
      "source": [
        "# # NOTE PEFT REQUIRES US TO USE 1XP100 NOT 2XT4. I'M NOT SURE WHY.\n",
        "# if USE_PEFT:\n",
        "#     !pip install --no-index --no-deps ../input/llm-whls/peft-0.4.0-py3-none-any.whl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "7TBjQX4QpOXa",
        "outputId": "80577902-ddd0-4d1d-f15a-bafa1fde1deb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of DebertaV2ForMultipleChoice were not initialized from the model checkpoint at microsoft/deberta-v3-large and are newly initialized: ['classifier.bias', 'pooler.dense.weight', 'classifier.weight', 'pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "We are using PEFT.\n",
            "trainable params: 2,887,682 || all params: 436,899,842 || trainable%: 0.6609482820549979\n"
          ]
        }
      ],
      "source": [
        "model = AutoModelForMultipleChoice.from_pretrained(MODEL)\n",
        "\n",
        "if USE_PEFT:\n",
        "    print('We are using PEFT.')\n",
        "    from peft import LoraConfig, get_peft_model, TaskType\n",
        "    peft_config = LoraConfig(\n",
        "        r=8, lora_alpha=4, task_type=TaskType.SEQ_CLS, lora_dropout=0.1,\n",
        "        bias=\"none\", inference_mode=False,\n",
        "        target_modules=[\"query_proj\", \"value_proj\"],\n",
        "        modules_to_save=['classifier','pooler'],\n",
        "    )\n",
        "    model = get_peft_model(model, peft_config)\n",
        "    model.print_trainable_parameters()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "NfLCv-dCpOXb",
        "outputId": "dcd0b1f7-1035-4534-f96c-482de3bfbf56"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Freezing embeddings.\n",
            "Freezing 18 layers.\n",
            "77875202 436899842 0.17824497633945127\n"
          ]
        }
      ],
      "source": [
        "\n",
        "if FREEZE_EMBEDDINGS:\n",
        "    print('Freezing embeddings.')\n",
        "    for param in model.deberta.embeddings.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "if FREEZE_LAYERS>0:\n",
        "    print(f'Freezing {FREEZE_LAYERS} layers.')\n",
        "    for layer in model.deberta.encoder.layer[:FREEZE_LAYERS]:\n",
        "        for param in layer.parameters():\n",
        "            param.requires_grad = False\n",
        "    # Newly added for v3\n",
        "    for layer in model.deberta.encoder.layer[FREEZE_LAYERS:]:\n",
        "        for param in layer.parameters():\n",
        "            param.requires_grad = True\n",
        "\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
        "    trainable_params = sum([np.prod(p.size()) for p in model_parameters])\n",
        "    print(trainable_params, total_params, trainable_params/total_params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "ZQvoNAiUpOXb"
      },
      "outputs": [],
      "source": [
        "from transformers.trainer_utils import PREFIX_CHECKPOINT_DIR\n",
        "from transformers import TrainerState, TrainerControl, TrainerCallback\n",
        "class SavePeftModelCallback(TrainerCallback):\n",
        "    def on_save(\n",
        "        self,\n",
        "        args: TrainingArguments,\n",
        "        state: TrainerState,\n",
        "        control: TrainerControl,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        checkpoint_folder = os.path.join(\n",
        "            args.output_dir, f\"{PREFIX_CHECKPOINT_DIR}-{state.global_step}\"\n",
        "        )\n",
        "\n",
        "        peft_model_path = os.path.join(checkpoint_folder, \"torch_model\")\n",
        "        # peft_model_path = os.path.join(checkpoint_folder)\n",
        "        kwargs[\"model\"].base_model.save_pretrained(peft_model_path)\n",
        "\n",
        "        # pytorch_model_path = os.path.join(checkpoint_folder, \"pytorch_model.bin\")\n",
        "        # if os.path.exists(pytorch_model_path):\n",
        "        #     os.remove(pytorch_model_path)\n",
        "        return control"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-09-09T03:47:40.478843Z",
          "iopub.status.busy": "2023-09-09T03:47:40.478276Z",
          "iopub.status.idle": "2023-09-09T03:47:40.489110Z",
          "shell.execute_reply": "2023-09-09T03:47:40.488156Z",
          "shell.execute_reply.started": "2023-09-09T03:47:40.478811Z"
        },
        "id": "cDnA4352pOXb",
        "outputId": "d31cfc8b-7ecd-48be-bc95-b295a9e9b210",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4\n"
          ]
        }
      ],
      "source": [
        "# batch_size = 4\n",
        "# target_batch_size = 512\n",
        "# GRAD_ACCUM = target_batch_size // batch_size\n",
        "# # SAVING_STEP = 10\n",
        "# SAVING_STEP = 4\n",
        "\n",
        "# LOGGING_STEPS = SAVING_STEP\n",
        "# print(SAVING_STEP)\n",
        "# training_args = TrainingArguments(\n",
        "#     warmup_ratio=0.03,\n",
        "#     learning_rate=1e-5, # maybe 1e5\n",
        "#     # per_device_train_batch_size=16,\n",
        "#     per_device_train_batch_size=batch_size,\n",
        "#     # per_device_eval_batch_size=32,\n",
        "#     per_device_eval_batch_size=batch_size*2,\n",
        "#     num_train_epochs=3,  # 2.5\n",
        "#     report_to='none',\n",
        "#     output_dir = f'./checkpoints_{VER}',\n",
        "#     overwrite_output_dir=True,\n",
        "#     fp16=True,\n",
        "#     # gradient_accumulation_steps=8,\n",
        "#     gradient_accumulation_steps=GRAD_ACCUM,\n",
        "#     logging_steps=LOGGING_STEPS,\n",
        "#     evaluation_strategy='steps',\n",
        "#     eval_steps=SAVING_STEP,\n",
        "#     save_strategy=\"steps\",\n",
        "#     save_steps=SAVING_STEP,\n",
        "#     load_best_model_at_end=False,\n",
        "#     metric_for_best_model='map@3',\n",
        "#     lr_scheduler_type='cosine',\n",
        "#     # weight_decay=0.01,\n",
        "#     weight_decay=1e-3,\n",
        "#     save_total_limit=5,\n",
        "\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2\n"
          ]
        }
      ],
      "source": [
        "# batch_size = 8 # Try this if possible for 18 512\n",
        "batch_size = 4 # for 16 512\n",
        "\n",
        "# effective_batch_size = 1024\n",
        "effective_batch_size = 512\n",
        "# effective_batch_size = 256\n",
        "\n",
        "# effective_batch_size = 128\n",
        "GRAD_ACCUM = effective_batch_size // batch_size\n",
        "SAVING_STEP = 2\n",
        "LOGGING_STEPS = SAVING_STEP\n",
        "print(SAVING_STEP)\n",
        "training_args = TrainingArguments(\n",
        "    # warmup_ratio=0.1, \n",
        "    # warmup_ratio=0.0, \n",
        "    warmup_ratio = 0.03,\n",
        "    # warmup_ratio = 0.0,\n",
        "    # learning_rate = 1e-4,\n",
        "    # learning_rate = 2.28e-5,\n",
        "    learning_rate = 2.28e-5 * 1.6,\n",
        "\n",
        "    # max_grad_norm = 2.0,\n",
        "    max_grad_norm = 1.0,\n",
        "\n",
        "    # max_grad_norm = 0.3,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size*2,\n",
        "    num_train_epochs=3,  # 2.5\n",
        "    report_to='none',\n",
        "    output_dir = f'./{checkpoint_folder}/{VER}',\n",
        "    overwrite_output_dir=True,\n",
        "    fp16=True,\n",
        "    # gradient_accumulation_steps=8,\n",
        "    gradient_accumulation_steps=GRAD_ACCUM,\n",
        "    logging_steps=LOGGING_STEPS,\n",
        "    evaluation_strategy='steps',\n",
        "    eval_steps=SAVING_STEP,\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=SAVING_STEP,\n",
        "    load_best_model_at_end=False,\n",
        "    # metric_for_best_model='map@3',\n",
        "    metric_for_best_model='eval_loss',\n",
        "    seed=666,\n",
        "    # lr_scheduler_type='linear',\n",
        "    lr_scheduler_type='cosine',\n",
        "    # lr_scheduler_type='cosine_with_restarts',    \n",
        "    # lr_scheduler_type='reduce_lr_on_plateau',\n",
        "    # weight_decay=0.01,\n",
        "    # weight_decay=1e-6, # set this slightly higher to reduce oscillation\n",
        "    weight_decay=1e-3, # set this slightly higher to reduce oscillation\n",
        "    # weight_decay=3e-4, # set this slightly higher to reduce oscillation\n",
        "    save_total_limit=5,\n",
        "    \n",
        ")\n",
        "# training_args = training_args.set_optimizer(name=\"adamw_torch\", beta1=0.9, beta2=0.98, weight_decay=training_args.weight_decay)\n",
        "# training_args = training_args.set_lr_scheduler(name=\"reduce_lr_on_plateau\", )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "del model, trainer\n",
        "import gc\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-09-09T03:47:44.277808Z",
          "iopub.status.busy": "2023-09-09T03:47:44.277446Z",
          "iopub.status.idle": "2023-09-09T03:54:54.290539Z",
          "shell.execute_reply": "2023-09-09T03:54:54.289078Z",
          "shell.execute_reply.started": "2023-09-09T03:47:44.277778Z"
        },
        "id": "XTDW7baOpOXb",
        "outputId": "24cab7c6-d6a2-484f-d2b8-f23cad18264b",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dijkstraz/anaconda3/envs/kaggle/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='449' max='570' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [449/570 5:31:06 < 1:29:37, 0.02 it/s, Epoch 2.35/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Map@3</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>1.388700</td>\n",
              "      <td>1.609858</td>\n",
              "      <td>0.315000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>1.387800</td>\n",
              "      <td>1.609736</td>\n",
              "      <td>0.311667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>1.386900</td>\n",
              "      <td>1.609585</td>\n",
              "      <td>0.371667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>1.390500</td>\n",
              "      <td>1.609473</td>\n",
              "      <td>0.433333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>1.389900</td>\n",
              "      <td>1.609287</td>\n",
              "      <td>0.503333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>1.388300</td>\n",
              "      <td>1.608857</td>\n",
              "      <td>0.550000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>1.386600</td>\n",
              "      <td>1.607915</td>\n",
              "      <td>0.629167</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32</td>\n",
              "      <td>1.385200</td>\n",
              "      <td>1.605498</td>\n",
              "      <td>0.682500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36</td>\n",
              "      <td>1.375600</td>\n",
              "      <td>1.597373</td>\n",
              "      <td>0.741667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>1.347700</td>\n",
              "      <td>1.568926</td>\n",
              "      <td>0.776667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>44</td>\n",
              "      <td>1.240300</td>\n",
              "      <td>1.431741</td>\n",
              "      <td>0.798333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>48</td>\n",
              "      <td>1.084900</td>\n",
              "      <td>1.373645</td>\n",
              "      <td>0.603333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>52</td>\n",
              "      <td>1.004800</td>\n",
              "      <td>1.198090</td>\n",
              "      <td>0.679167</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>56</td>\n",
              "      <td>0.943900</td>\n",
              "      <td>1.208233</td>\n",
              "      <td>0.690833</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>0.897700</td>\n",
              "      <td>1.069452</td>\n",
              "      <td>0.806667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>64</td>\n",
              "      <td>0.856000</td>\n",
              "      <td>1.030103</td>\n",
              "      <td>0.806667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>68</td>\n",
              "      <td>0.844300</td>\n",
              "      <td>0.931033</td>\n",
              "      <td>0.810833</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>72</td>\n",
              "      <td>0.833400</td>\n",
              "      <td>0.966074</td>\n",
              "      <td>0.811667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>76</td>\n",
              "      <td>0.823800</td>\n",
              "      <td>0.937023</td>\n",
              "      <td>0.815000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>0.799100</td>\n",
              "      <td>0.897933</td>\n",
              "      <td>0.825833</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>84</td>\n",
              "      <td>0.746900</td>\n",
              "      <td>0.952233</td>\n",
              "      <td>0.810833</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>88</td>\n",
              "      <td>0.750000</td>\n",
              "      <td>0.976632</td>\n",
              "      <td>0.810833</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>92</td>\n",
              "      <td>0.705900</td>\n",
              "      <td>0.864886</td>\n",
              "      <td>0.820833</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>96</td>\n",
              "      <td>0.697900</td>\n",
              "      <td>0.881107</td>\n",
              "      <td>0.823333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.661700</td>\n",
              "      <td>0.930654</td>\n",
              "      <td>0.791667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>104</td>\n",
              "      <td>0.671600</td>\n",
              "      <td>0.877476</td>\n",
              "      <td>0.804167</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>108</td>\n",
              "      <td>0.701000</td>\n",
              "      <td>0.828438</td>\n",
              "      <td>0.816667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>112</td>\n",
              "      <td>0.682400</td>\n",
              "      <td>0.881723</td>\n",
              "      <td>0.808333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>116</td>\n",
              "      <td>0.658100</td>\n",
              "      <td>0.839341</td>\n",
              "      <td>0.817500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>0.639100</td>\n",
              "      <td>0.786944</td>\n",
              "      <td>0.835833</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>124</td>\n",
              "      <td>0.662700</td>\n",
              "      <td>0.795879</td>\n",
              "      <td>0.833333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>128</td>\n",
              "      <td>0.661900</td>\n",
              "      <td>0.839680</td>\n",
              "      <td>0.822500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>132</td>\n",
              "      <td>0.624800</td>\n",
              "      <td>0.858259</td>\n",
              "      <td>0.808333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>136</td>\n",
              "      <td>0.628800</td>\n",
              "      <td>0.826990</td>\n",
              "      <td>0.820000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>0.614400</td>\n",
              "      <td>0.810005</td>\n",
              "      <td>0.825833</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>144</td>\n",
              "      <td>0.627500</td>\n",
              "      <td>0.816927</td>\n",
              "      <td>0.824167</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>148</td>\n",
              "      <td>0.609900</td>\n",
              "      <td>0.810154</td>\n",
              "      <td>0.825000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>152</td>\n",
              "      <td>0.625000</td>\n",
              "      <td>0.809298</td>\n",
              "      <td>0.828333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>156</td>\n",
              "      <td>0.612100</td>\n",
              "      <td>0.833616</td>\n",
              "      <td>0.822500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>160</td>\n",
              "      <td>0.642000</td>\n",
              "      <td>0.845114</td>\n",
              "      <td>0.828333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>164</td>\n",
              "      <td>0.635300</td>\n",
              "      <td>0.816087</td>\n",
              "      <td>0.834167</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>168</td>\n",
              "      <td>0.613600</td>\n",
              "      <td>0.818889</td>\n",
              "      <td>0.832500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>172</td>\n",
              "      <td>0.599600</td>\n",
              "      <td>0.831715</td>\n",
              "      <td>0.825000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>176</td>\n",
              "      <td>0.608400</td>\n",
              "      <td>0.831877</td>\n",
              "      <td>0.823333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>180</td>\n",
              "      <td>0.590000</td>\n",
              "      <td>0.818542</td>\n",
              "      <td>0.817500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>184</td>\n",
              "      <td>0.604400</td>\n",
              "      <td>0.826950</td>\n",
              "      <td>0.817500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>188</td>\n",
              "      <td>0.614600</td>\n",
              "      <td>0.841840</td>\n",
              "      <td>0.818333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>192</td>\n",
              "      <td>0.565000</td>\n",
              "      <td>0.850342</td>\n",
              "      <td>0.812500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>196</td>\n",
              "      <td>0.537600</td>\n",
              "      <td>0.813530</td>\n",
              "      <td>0.822500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.597200</td>\n",
              "      <td>0.770786</td>\n",
              "      <td>0.835000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>204</td>\n",
              "      <td>0.573500</td>\n",
              "      <td>0.773347</td>\n",
              "      <td>0.833333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>208</td>\n",
              "      <td>0.552900</td>\n",
              "      <td>0.788700</td>\n",
              "      <td>0.838333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>212</td>\n",
              "      <td>0.567200</td>\n",
              "      <td>0.808680</td>\n",
              "      <td>0.835000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>216</td>\n",
              "      <td>0.558100</td>\n",
              "      <td>0.806714</td>\n",
              "      <td>0.830833</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>220</td>\n",
              "      <td>0.552700</td>\n",
              "      <td>0.801982</td>\n",
              "      <td>0.831667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>224</td>\n",
              "      <td>0.571200</td>\n",
              "      <td>0.800002</td>\n",
              "      <td>0.830833</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>228</td>\n",
              "      <td>0.576900</td>\n",
              "      <td>0.789312</td>\n",
              "      <td>0.831667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>232</td>\n",
              "      <td>0.538400</td>\n",
              "      <td>0.785091</td>\n",
              "      <td>0.835833</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>236</td>\n",
              "      <td>0.543600</td>\n",
              "      <td>0.794409</td>\n",
              "      <td>0.833333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>240</td>\n",
              "      <td>0.545800</td>\n",
              "      <td>0.798689</td>\n",
              "      <td>0.833333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>244</td>\n",
              "      <td>0.550100</td>\n",
              "      <td>0.791923</td>\n",
              "      <td>0.835000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>248</td>\n",
              "      <td>0.554700</td>\n",
              "      <td>0.794117</td>\n",
              "      <td>0.836667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>252</td>\n",
              "      <td>0.558700</td>\n",
              "      <td>0.793433</td>\n",
              "      <td>0.830000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>256</td>\n",
              "      <td>0.548600</td>\n",
              "      <td>0.780257</td>\n",
              "      <td>0.829167</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>260</td>\n",
              "      <td>0.542600</td>\n",
              "      <td>0.777592</td>\n",
              "      <td>0.829167</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>264</td>\n",
              "      <td>0.512500</td>\n",
              "      <td>0.780239</td>\n",
              "      <td>0.834167</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>268</td>\n",
              "      <td>0.546100</td>\n",
              "      <td>0.782247</td>\n",
              "      <td>0.832500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>272</td>\n",
              "      <td>0.537800</td>\n",
              "      <td>0.775509</td>\n",
              "      <td>0.833333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>276</td>\n",
              "      <td>0.558600</td>\n",
              "      <td>0.778020</td>\n",
              "      <td>0.830000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>280</td>\n",
              "      <td>0.517400</td>\n",
              "      <td>0.779202</td>\n",
              "      <td>0.830833</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>284</td>\n",
              "      <td>0.569100</td>\n",
              "      <td>0.785438</td>\n",
              "      <td>0.835833</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>288</td>\n",
              "      <td>0.543400</td>\n",
              "      <td>0.790874</td>\n",
              "      <td>0.835000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>292</td>\n",
              "      <td>0.542700</td>\n",
              "      <td>0.797698</td>\n",
              "      <td>0.836667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>296</td>\n",
              "      <td>0.552000</td>\n",
              "      <td>0.803805</td>\n",
              "      <td>0.836667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.545400</td>\n",
              "      <td>0.806088</td>\n",
              "      <td>0.837500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>304</td>\n",
              "      <td>0.499100</td>\n",
              "      <td>0.801085</td>\n",
              "      <td>0.839167</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>308</td>\n",
              "      <td>0.517700</td>\n",
              "      <td>0.797659</td>\n",
              "      <td>0.836667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>312</td>\n",
              "      <td>0.578600</td>\n",
              "      <td>0.790415</td>\n",
              "      <td>0.837500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>316</td>\n",
              "      <td>0.527800</td>\n",
              "      <td>0.778859</td>\n",
              "      <td>0.842500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>320</td>\n",
              "      <td>0.505100</td>\n",
              "      <td>0.773282</td>\n",
              "      <td>0.845000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>324</td>\n",
              "      <td>0.520700</td>\n",
              "      <td>0.770250</td>\n",
              "      <td>0.846667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>328</td>\n",
              "      <td>0.548800</td>\n",
              "      <td>0.775925</td>\n",
              "      <td>0.844167</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>332</td>\n",
              "      <td>0.532500</td>\n",
              "      <td>0.784539</td>\n",
              "      <td>0.843333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>336</td>\n",
              "      <td>0.559500</td>\n",
              "      <td>0.793341</td>\n",
              "      <td>0.845833</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>340</td>\n",
              "      <td>0.523800</td>\n",
              "      <td>0.794662</td>\n",
              "      <td>0.845833</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>344</td>\n",
              "      <td>0.526200</td>\n",
              "      <td>0.792082</td>\n",
              "      <td>0.843333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>348</td>\n",
              "      <td>0.544800</td>\n",
              "      <td>0.786088</td>\n",
              "      <td>0.845000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>352</td>\n",
              "      <td>0.507700</td>\n",
              "      <td>0.782892</td>\n",
              "      <td>0.845833</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>356</td>\n",
              "      <td>0.542000</td>\n",
              "      <td>0.782539</td>\n",
              "      <td>0.845833</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>360</td>\n",
              "      <td>0.526400</td>\n",
              "      <td>0.781699</td>\n",
              "      <td>0.845833</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>364</td>\n",
              "      <td>0.528100</td>\n",
              "      <td>0.783514</td>\n",
              "      <td>0.842500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>368</td>\n",
              "      <td>0.535100</td>\n",
              "      <td>0.781670</td>\n",
              "      <td>0.845833</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>372</td>\n",
              "      <td>0.523700</td>\n",
              "      <td>0.781237</td>\n",
              "      <td>0.845000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>376</td>\n",
              "      <td>0.522800</td>\n",
              "      <td>0.779012</td>\n",
              "      <td>0.844167</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>380</td>\n",
              "      <td>0.541400</td>\n",
              "      <td>0.778063</td>\n",
              "      <td>0.846667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>384</td>\n",
              "      <td>0.513000</td>\n",
              "      <td>0.778574</td>\n",
              "      <td>0.844167</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>388</td>\n",
              "      <td>0.499700</td>\n",
              "      <td>0.783282</td>\n",
              "      <td>0.840000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>392</td>\n",
              "      <td>0.503100</td>\n",
              "      <td>0.785909</td>\n",
              "      <td>0.840000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>396</td>\n",
              "      <td>0.479600</td>\n",
              "      <td>0.789319</td>\n",
              "      <td>0.837500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>0.534400</td>\n",
              "      <td>0.784871</td>\n",
              "      <td>0.840000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>404</td>\n",
              "      <td>0.531400</td>\n",
              "      <td>0.779451</td>\n",
              "      <td>0.841667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>408</td>\n",
              "      <td>0.501600</td>\n",
              "      <td>0.776575</td>\n",
              "      <td>0.848333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>412</td>\n",
              "      <td>0.493700</td>\n",
              "      <td>0.776619</td>\n",
              "      <td>0.850833</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>416</td>\n",
              "      <td>0.478400</td>\n",
              "      <td>0.775831</td>\n",
              "      <td>0.846667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>420</td>\n",
              "      <td>0.475800</td>\n",
              "      <td>0.776627</td>\n",
              "      <td>0.844167</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>424</td>\n",
              "      <td>0.492000</td>\n",
              "      <td>0.776313</td>\n",
              "      <td>0.845000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>428</td>\n",
              "      <td>0.507200</td>\n",
              "      <td>0.778439</td>\n",
              "      <td>0.848333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>432</td>\n",
              "      <td>0.492900</td>\n",
              "      <td>0.779350</td>\n",
              "      <td>0.848333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>436</td>\n",
              "      <td>0.506500</td>\n",
              "      <td>0.780175</td>\n",
              "      <td>0.848333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>440</td>\n",
              "      <td>0.509300</td>\n",
              "      <td>0.780050</td>\n",
              "      <td>0.848333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>444</td>\n",
              "      <td>0.512400</td>\n",
              "      <td>0.781370</td>\n",
              "      <td>0.848333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>448</td>\n",
              "      <td>0.528000</td>\n",
              "      <td>0.782100</td>\n",
              "      <td>0.847500</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=DataCollatorForMultipleChoice(tokenizer=tokenizer),\n",
        "    train_dataset=tokenized_dataset,\n",
        "    eval_dataset=tokenized_dataset_valid,\n",
        "    compute_metrics = compute_metrics,\n",
        "    callbacks=[SavePeftModelCallback] if USE_PEFT else None,\n",
        "    #callbacks=[EarlyStoppingCallback(early_stopping_patience=5)],\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "if USE_PEFT:\n",
        "    trainer.model.save_pretrained(f'model_v{VER}') # 我改了这个\n",
        "else:\n",
        "    trainer.save_model(f'model_v{VER}')\n",
        "\n",
        "# I think I read from some parts of the discussion that some length of input during training could be changed.\n",
        "# Training longer during training will hopefully cover the length of even the longest sentence in testing.\n",
        "# This is some problem with model extrapolation.\n",
        "# Basically, if the model is not using very good positional encoding, it will perform badly in sequences longer than what it has been trained on."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8VhfWy4jpOXb"
      },
      "source": [
        "# Verify Saved Model\n",
        "During training, we see the MAP@3 validation score above. Let's load the saved model and compute it again here to verify that our model is saved correctly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-02T02:42:35.053535Z",
          "iopub.status.idle": "2023-09-02T02:42:35.054036Z",
          "shell.execute_reply": "2023-09-02T02:42:35.053806Z",
          "shell.execute_reply.started": "2023-09-02T02:42:35.053782Z"
        },
        "id": "K6Ko_1V9pOXb",
        "outputId": "fff68db6-d5b1-41e8-f8d2-f32d9285940e",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loading peft\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of DebertaV2ForMultipleChoice were not initialized from the model checkpoint at microsoft/deberta-v3-large and are newly initialized: ['classifier.weight', 'classifier.bias', 'pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trainable params: 2,887,682 || all params: 436,899,842 || trainable%: 0.6609482820549979\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "{'eval_loss': 1.6089160442352295,\n",
              " 'eval_map@3': 0.6274999999999998,\n",
              " 'eval_runtime': 13.387,\n",
              " 'eval_samples_per_second': 14.94,\n",
              " 'eval_steps_per_second': 1.867}"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "del model, trainer\n",
        "from peft import get_peft_model, set_peft_model_state_dict\n",
        "if USE_PEFT:\n",
        "    print('loading peft')\n",
        "    model = AutoModelForMultipleChoice.from_pretrained(MODEL)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
        "    model = get_peft_model(model, peft_config)\n",
        "    # checkpoint = torch.load(f'model_v{VER}/adapter_model.bin')\n",
        "    # checkpoint = torch.load(f'./checkpoints_3/checkpoint-160/adapter_model.bin')\n",
        "    # checkpoint = torch.load(f'./checkpoints_3/checkpoint-224/adapter_model.bin')\n",
        "    # checkpoint = torch.load(f'./checkpoints_5/checkpoint-3900/adapter_model.bin')\n",
        "\n",
        "    checkpoint = torch.load(f'./checkpoints_{VER}/checkpoint-30/torch_model/pytorch_model.bin')\n",
        "    model.base_model.model.load_state_dict(checkpoint)\n",
        "\n",
        "\n",
        "    # print('loading state dict')\n",
        "    set_peft_model_state_dict(model, checkpoint)\n",
        "    # if FREEZE_EMBEDDINGS:\n",
        "    #     print('Freezing embeddings.')\n",
        "    #     for param in model.deberta.embeddings.parameters():\n",
        "    #         param.requires_grad = False\n",
        "    # if FREEZE_LAYERS>0:\n",
        "    #     print(f'Freezing {FREEZE_LAYERS} layers.')\n",
        "    #     for layer in model.deberta.encoder.layer[:FREEZE_LAYERS]:\n",
        "    #         for param in layer.parameters():\n",
        "    #             param.requires_grad = False\n",
        "    #     # Newly added for v3\n",
        "    #     for layer in model.deberta.encoder.layer[FREEZE_LAYERS:]:\n",
        "    #         for param in layer.parameters():\n",
        "    #             param.requires_grad = True\n",
        "    model.eval()\n",
        "    model.print_trainable_parameters()\n",
        "else:\n",
        "    model = AutoModelForMultipleChoice.from_pretrained(f'model_v{VER}')\n",
        "trainer = Trainer(model=model,\n",
        "                data_collator=DataCollatorForMultipleChoice(tokenizer=tokenizer),\n",
        "                tokenizer=tokenizer,\n",
        "                train_dataset=tokenized_dataset,\n",
        "                eval_dataset=tokenized_dataset_valid,\n",
        "                compute_metrics = compute_metrics,)\n",
        "trainer.evaluate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "ee0430024c5b4ceeb15a2e1048e074d9"
          ]
        },
        "execution": {
          "iopub.status.busy": "2023-09-02T02:42:35.055773Z",
          "iopub.status.idle": "2023-09-02T02:42:35.056299Z",
          "shell.execute_reply": "2023-09-02T02:42:35.056073Z",
          "shell.execute_reply.started": "2023-09-02T02:42:35.056015Z"
        },
        "id": "t9XwQPoLpOXb",
        "outputId": "2cd4121b-4fee-4d68-da11-9dbf006e4b1f",
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ee0430024c5b4ceeb15a2e1048e074d9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "MAX_INPUT = 512\n",
        "# MAX_INPUT = 256\n",
        "\n",
        "from functools import partial\n",
        "test_df = pd.read_csv('../input/60k-data-with-context-v2/train_with_context2.csv')\n",
        "\n",
        "# tokenized_dataset_valid = dataset_valid.map(preprocess, remove_columns=['prompt', 'context', 'A', 'B', 'C', 'D', 'E', 'answer'])\n",
        "tokenized_test_dataset = Dataset.from_pandas(test_df).map(\n",
        "        preprocess, remove_columns=['prompt', 'context', 'A', 'B', 'C', 'D', 'E'])\n",
        "\n",
        "# with torch.no_grad():\n",
        "test_predictions = trainer.predict(tokenized_test_dataset).predictions\n",
        "predictions_as_ids = np.argsort(-test_predictions, 1)\n",
        "predictions_as_answer_letters = np.array(list('ABCDE'))[predictions_as_ids]\n",
        "predictions_as_string = test_df['prediction'] = [\n",
        "    ' '.join(row) for row in predictions_as_answer_letters[:, :3]\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O1dkTbA7pOXb"
      },
      "source": [
        "法1：\n",
        "1. 通过 question 在 wiki 中 进行 context matching，用 sentence embedding 的 cosine similarity，得到 k 个 context，Ctx\n",
        "2. 通过 answer 在 Ctx 中进行 sentence similarity matching, 将每个答案的 similarity 进行 normalize, 当作先验概率（prior probability）\n",
        "3. 以训练过的方式，让模型给logits，然后用 sonormalize 得到后验概率（posterior probability）\n",
        "4. 先后 相乘，然后 softmax，准备ensemble\n",
        "\n",
        "法2：\n",
        "1. 通过 question 在 wiki 中 进行 context matching，用 sentence embedding 的 cosine similarity，得到 k 个 context，Ctx\n",
        "2. 通过某种方法分解提取答案，然后将分解出来的 entity 拿到 context 中，计算 context 每个句子的权重，挑出最重要的 s 个句子，以他们在wiki 中出现的顺序喂给模型"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oK-tb269pOXb",
        "outputId": "26017790-b467-45b1-8120-4230f8824004"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "512\n",
            "[CLS] Eunice Fay McKenzie (February 19, 1918 – April 16, 2019) was an American actress and singer. She also entertained the troops with her former screen partner, Gene Autry. ===Later career=== After World War II, McKenzie retired from films to raise her two children. She was briefly billed as Fay Shannon. ==Biography== ===Early life and silent film=== McKenzie was born on February 19, 1918, in Hollywood, California, to show business parents, film actor Eva (née Heazlitt) and Irish American actor/director Robert McKenzie.Mike Fitzgerald, \"An Interview with... She starred in silent films as a child, and then sound films as an adult, but perhaps she is best known for her leading roles opposite Gene Autry in the early 1940s in five horse opera features. Fay's sister Ida Mae McKenzie, cousin Ella McKenzie, and brother-in-law Billy Gilbert, were also actors. McKenzie sang duets with Autry in each of these films. Ida Mae also played the character of Sarah Lincoln in The Dramatic Life of Abraham Lincoln, in the part of the film where she had become a teenager. ===Schooling=== In the mid-1920s, McKenzie took a ten-year break from acting in order to focus on her education. Her father had a stock company called the McKenzie Merry Makers, and was both an actor and director in stage productions and films. McKenzie later recalled, ===Sound films=== McKenzie appeared in numerous uncredited roles throughout the 1930s, with occasional credited roles in films such as The Boss Cowboy (1934) as Sally Nolan, and the anti-cannabis propaganda film Assassin of Youth (1937) as Linda Clayton. She later remembered: ===Theater and touring=== During World War II, McKenzie left Republic Pictures to work in theater and pursue other projects. McKenzie went on to appear in four additional Autry films as his leading lady: Sierra Sue (1941) as Sue Larrabee, Cowboy Serenade (1942) as Stephanie Lock, Heart of the Rio Grande (1942) as Alice Bennett, and Home in Wyomin' (1942) as Clementine Benson. Fay McKenzie\", Western Clippings. In 1938, she began to appear mainly in Western films, such as Ghost Town Riders (1938) #### In relation to Eunice Fay McKenzie's career, which statement accurately reflects her most notable work?[SEP] McKenzie showcased her singing talents in numerous musical productions, garnering critical acclaim.[SEP]\n",
            "512\n",
            "[CLS] Eunice Fay McKenzie (February 19, 1918 – April 16, 2019) was an American actress and singer. She also entertained the troops with her former screen partner, Gene Autry. ===Later career=== After World War II, McKenzie retired from films to raise her two children. She was briefly billed as Fay Shannon. ==Biography== ===Early life and silent film=== McKenzie was born on February 19, 1918, in Hollywood, California, to show business parents, film actor Eva (née Heazlitt) and Irish American actor/director Robert McKenzie.Mike Fitzgerald, \"An Interview with... She starred in silent films as a child, and then sound films as an adult, but perhaps she is best known for her leading roles opposite Gene Autry in the early 1940s in five horse opera features. Fay's sister Ida Mae McKenzie, cousin Ella McKenzie, and brother-in-law Billy Gilbert, were also actors. McKenzie sang duets with Autry in each of these films. Ida Mae also played the character of Sarah Lincoln in The Dramatic Life of Abraham Lincoln, in the part of the film where she had become a teenager. ===Schooling=== In the mid-1920s, McKenzie took a ten-year break from acting in order to focus on her education. Her father had a stock company called the McKenzie Merry Makers, and was both an actor and director in stage productions and films. McKenzie later recalled, ===Sound films=== McKenzie appeared in numerous uncredited roles throughout the 1930s, with occasional credited roles in films such as The Boss Cowboy (1934) as Sally Nolan, and the anti-cannabis propaganda film Assassin of Youth (1937) as Linda Clayton. She later remembered: ===Theater and touring=== During World War II, McKenzie left Republic Pictures to work in theater and pursue other projects. McKenzie went on to appear in four additional Autry films as his leading lady: Sierra Sue (1941) as Sue Larrabee, Cowboy Serenade (1942) as Stephanie Lock, Heart of the Rio Grande (1942) as Alice Bennett, and Home in Wyomin' (1942) as Clementine Benson. Fay McKenzie\", Western Clippings. In 1938, she began to appear mainly in Western films, such as #### In relation to Eunice Fay McKenzie's career, which statement accurately reflects her most notable work?[SEP] McKenzie is primarily remembered for her starring roles opposite Gene Autry in popular Western films of the 1940s.[SEP]\n",
            "512\n",
            "[CLS] Eunice Fay McKenzie (February 19, 1918 – April 16, 2019) was an American actress and singer. She also entertained the troops with her former screen partner, Gene Autry. ===Later career=== After World War II, McKenzie retired from films to raise her two children. She was briefly billed as Fay Shannon. ==Biography== ===Early life and silent film=== McKenzie was born on February 19, 1918, in Hollywood, California, to show business parents, film actor Eva (née Heazlitt) and Irish American actor/director Robert McKenzie.Mike Fitzgerald, \"An Interview with... She starred in silent films as a child, and then sound films as an adult, but perhaps she is best known for her leading roles opposite Gene Autry in the early 1940s in five horse opera features. Fay's sister Ida Mae McKenzie, cousin Ella McKenzie, and brother-in-law Billy Gilbert, were also actors. McKenzie sang duets with Autry in each of these films. Ida Mae also played the character of Sarah Lincoln in The Dramatic Life of Abraham Lincoln, in the part of the film where she had become a teenager. ===Schooling=== In the mid-1920s, McKenzie took a ten-year break from acting in order to focus on her education. Her father had a stock company called the McKenzie Merry Makers, and was both an actor and director in stage productions and films. McKenzie later recalled, ===Sound films=== McKenzie appeared in numerous uncredited roles throughout the 1930s, with occasional credited roles in films such as The Boss Cowboy (1934) as Sally Nolan, and the anti-cannabis propaganda film Assassin of Youth (1937) as Linda Clayton. She later remembered: ===Theater and touring=== During World War II, McKenzie left Republic Pictures to work in theater and pursue other projects. McKenzie went on to appear in four additional Autry films as his leading lady: Sierra Sue (1941) as Sue Larrabee, Cowboy Serenade (1942) as Stephanie Lock, Heart of the Rio Grande (1942) as Alice Bennett, and Home in Wyomin' (1942) as Clementine Benson. Fay McKenzie\", Western Clippings. In 1938, she began to appear mainly in Western films, such as Ghost Town #### In relation to Eunice Fay McKenzie's career, which statement accurately reflects her most notable work?[SEP] McKenzie gained recognition for her role as a child actress in a series of iconic silent films.[SEP]\n",
            "512\n",
            "[CLS] Eunice Fay McKenzie (February 19, 1918 – April 16, 2019) was an American actress and singer. She also entertained the troops with her former screen partner, Gene Autry. ===Later career=== After World War II, McKenzie retired from films to raise her two children. She was briefly billed as Fay Shannon. ==Biography== ===Early life and silent film=== McKenzie was born on February 19, 1918, in Hollywood, California, to show business parents, film actor Eva (née Heazlitt) and Irish American actor/director Robert McKenzie.Mike Fitzgerald, \"An Interview with... She starred in silent films as a child, and then sound films as an adult, but perhaps she is best known for her leading roles opposite Gene Autry in the early 1940s in five horse opera features. Fay's sister Ida Mae McKenzie, cousin Ella McKenzie, and brother-in-law Billy Gilbert, were also actors. McKenzie sang duets with Autry in each of these films. Ida Mae also played the character of Sarah Lincoln in The Dramatic Life of Abraham Lincoln, in the part of the film where she had become a teenager. ===Schooling=== In the mid-1920s, McKenzie took a ten-year break from acting in order to focus on her education. Her father had a stock company called the McKenzie Merry Makers, and was both an actor and director in stage productions and films. McKenzie later recalled, ===Sound films=== McKenzie appeared in numerous uncredited roles throughout the 1930s, with occasional credited roles in films such as The Boss Cowboy (1934) as Sally Nolan, and the anti-cannabis propaganda film Assassin of Youth (1937) as Linda Clayton. She later remembered: ===Theater and touring=== During World War II, McKenzie left Republic Pictures to work in theater and pursue other projects. McKenzie went on to appear in four additional Autry films as his leading lady: Sierra Sue (1941) as Sue Larrabee, Cowboy Serenade (1942) as Stephanie Lock, Heart of the Rio Grande (1942) as Alice Bennett, and Home in Wyomin' (1942) as Clementine Benson. Fay McKenzie\", Western Clippings. In 1938, she began to appear mainly in Western films, such as Ghost Town Riders ( #### In relation to Eunice Fay McKenzie's career, which statement accurately reflects her most notable work?[SEP] McKenzie's collaborations with director Blake Edwards were instrumental in her rise to fame.[SEP]\n",
            "512\n",
            "[CLS] Eunice Fay McKenzie (February 19, 1918 – April 16, 2019) was an American actress and singer. She also entertained the troops with her former screen partner, Gene Autry. ===Later career=== After World War II, McKenzie retired from films to raise her two children. She was briefly billed as Fay Shannon. ==Biography== ===Early life and silent film=== McKenzie was born on February 19, 1918, in Hollywood, California, to show business parents, film actor Eva (née Heazlitt) and Irish American actor/director Robert McKenzie.Mike Fitzgerald, \"An Interview with... She starred in silent films as a child, and then sound films as an adult, but perhaps she is best known for her leading roles opposite Gene Autry in the early 1940s in five horse opera features. Fay's sister Ida Mae McKenzie, cousin Ella McKenzie, and brother-in-law Billy Gilbert, were also actors. McKenzie sang duets with Autry in each of these films. Ida Mae also played the character of Sarah Lincoln in The Dramatic Life of Abraham Lincoln, in the part of the film where she had become a teenager. ===Schooling=== In the mid-1920s, McKenzie took a ten-year break from acting in order to focus on her education. Her father had a stock company called the McKenzie Merry Makers, and was both an actor and director in stage productions and films. McKenzie later recalled, ===Sound films=== McKenzie appeared in numerous uncredited roles throughout the 1930s, with occasional credited roles in films such as The Boss Cowboy (1934) as Sally Nolan, and the anti-cannabis propaganda film Assassin of Youth (1937) as Linda Clayton. She later remembered: ===Theater and touring=== During World War II, McKenzie left Republic Pictures to work in theater and pursue other projects. McKenzie went on to appear in four additional Autry films as his leading lady: Sierra Sue (1941) as Sue Larrabee, Cowboy Serenade (1942) as Stephanie Lock, Heart of the Rio Grande (1942) as Alice Bennett, and Home in Wyomin' (1942) as Clementine Benson. Fay McKenzie\", Western Clippings. In 1938, she began to appear mainly in Western films, such as #### In relation to Eunice Fay McKenzie's career, which statement accurately reflects her most notable work?[SEP] McKenzie's successful career in sound films continued into adulthood, becoming known for her versatile acting abilities.[SEP]\n"
          ]
        }
      ],
      "source": [
        "for x in tokenized_dataset[0]['input_ids']:\n",
        "    print(len(x))\n",
        "    print(tokenizer.decode(x)) # 许需要加一个 句子修剪\n",
        "    # print(x)\n",
        "# I think tokenized_dataset[0]['input_ids'] is a good example for checking the model's prediction\n",
        "# 因为它的 information retrieval 刚好找到了正确的context，因此需要看看模型是否能从这个context中获取正确答案。\n",
        "# 还应该 check 一下其他的 retrieval 里面，有多少 context 是相关的。\n",
        "# 能不能做一个 multi-hop retrieval:\n",
        "#\n",
        "#"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DwlrU9JqpOXb"
      },
      "source": [
        "# Compute Validation Score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-02T02:42:35.05812Z",
          "iopub.status.idle": "2023-09-02T02:42:35.059239Z",
          "shell.execute_reply": "2023-09-02T02:42:35.058895Z",
          "shell.execute_reply.started": "2023-09-02T02:42:35.058829Z"
        },
        "id": "tWk5T_rTpOXc",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# https://www.kaggle.com/code/philippsinger/h2ogpt-perplexity-ranking\n",
        "import numpy as np\n",
        "def precision_at_k(r, k):\n",
        "    \"\"\"Precision at k\"\"\"\n",
        "    assert k <= len(r)\n",
        "    assert k != 0\n",
        "    return sum(int(x) for x in r[:k]) / k\n",
        "\n",
        "def MAP_at_3(predictions, true_items):\n",
        "    \"\"\"Score is mean average precision at 3\"\"\"\n",
        "    U = len(predictions)\n",
        "    map_at_3 = 0.0\n",
        "    for u in range(U):\n",
        "        user_preds = predictions[u].split()\n",
        "        user_true = true_items[u]\n",
        "        user_results = [1 if item == user_true else 0 for item in user_preds]\n",
        "        for k in range(min(len(user_preds), 3)):\n",
        "            map_at_3 += precision_at_k(user_results, k+1) * user_results[k]\n",
        "    return map_at_3 / U"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-02T02:42:35.062122Z",
          "iopub.status.idle": "2023-09-02T02:42:35.06267Z",
          "shell.execute_reply": "2023-09-02T02:42:35.062401Z",
          "shell.execute_reply.started": "2023-09-02T02:42:35.062377Z"
        },
        "id": "Z639mdHopOXc",
        "outputId": "9925c861-1281-456c-9063-fd5d9a465b1d",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CV MAP@3 = 0.6274999999999998\n"
          ]
        }
      ],
      "source": [
        "m = MAP_at_3(test_df.prediction.values, test_df.answer.values)\n",
        "print( 'CV MAP@3 =',m )"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
