{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "acC1C2sWpOXV"
      },
      "source": [
        "# How To Train Model for Open Book Q&A Technique\n",
        "In this notebook we demonstrate how to train a model to be used with top scoring Open Book Q&A method. The Open Book method was first presented by JJ (@jjinho) [here][1], then Quangteo (@quangbk) improved RAM usage [here][2], and Anil (@nlztrk) combined with Q&A [here][3]. Radek (@radek1) demonstrated the strength of Q&A [here][5]. Next Mgoksu (@mgoksu) demonstrated how to achieve top public LB=0.807 using this method [here][4] by finetuning DeBerta large on this method.\n",
        "\n",
        "In order to train a model for use with Open Book Q&A, we need a CSV that contains; `prompt` (i.e. question), `A, B, C, D, E` (i.e. answer choices), and we need a column of `context` extracted from wikipedia pages for each question. To generate the `context` column, we run Mgoksu's notebook [here][4]. In code cell #5, we load our CSV without `context` column with code `trn = pd.read_csv(OUR_DATASET.CSV)`. Then in code cell #21 our dataset is saved to disk as `test_context.csv` with the column `context` added.\n",
        "\n",
        "I have searched and concatenated all publicly shared datasets into one 60k CSV and then ran Mgoksu's notebook with `NUM_TITLES_INCLUDE = 5` and `NUM_SENTENCES_INCLUDE = 20`. This added an additional `context` column. I uploaded the resultant CSV file to a Kaggle dataset [here][6]. If you enjoy the notebook you are reading, please upvote the dataset too. Thanks!\n",
        "\n",
        "![](https://miro.medium.com/v2/resize:fit:800/format:webp/1*bTGY3fKIgNefQxNsOYpnBw.png)\n",
        "\n",
        "(image source [here][7])\n",
        "\n",
        "[1]: https://www.kaggle.com/code/jjinho/open-book-llm-science-exam\n",
        "[2]: https://www.kaggle.com/code/quangbk/open-book-llm-science-exam-reduced-ram-usage\n",
        "[3]: https://www.kaggle.com/code/nlztrk/openbook-debertav3-large-baseline-single-model\n",
        "[4]: https://www.kaggle.com/code/mgoksu/0-807-sharing-my-trained-with-context-model\n",
        "[5]: https://www.kaggle.com/code/radek1/new-dataset-deberta-v3-large-training\n",
        "[6]: https://www.kaggle.com/datasets/cdeotte/60k-data-with-context-v2\n",
        "[7]: https://blog.gopenai.com/enrich-llms-with-retrieval-augmented-generation-rag-17b82a96b6f0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wm9oVgjOpOXX"
      },
      "source": [
        "# Load CSV\n",
        "We will load 60k CSV of `prompts`, `A,B,C,D,E`, and `context` from my Kaggle dataset [here][1]. This dataset is all publicly shared datasets concatenated then processed with Mgoksu's notebook [here][2] to create a `context` column. (To learn more about the datasets within read my discussion post). This Kaggle dataset also contains competition `train.csv` with added `context` column (to be used as a validation dataset).\n",
        "\n",
        "In this train notebook, we have internet turned on and can choose whatever model we wish to download and train. After we finetune this model, we will create a second notebook with the Open Book Q&A technique and load the finetuned model from the output of this notebook. The second notebook will have internet turned off so that it can be submitted to Kaggle's competition.\n",
        "\n",
        "[1]: https://www.kaggle.com/datasets/cdeotte/60k-data-with-context-v2\n",
        "[2]: https://www.kaggle.com/code/mgoksu/0-807-sharing-my-trained-with-context-model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-09-10T15:51:09.289992Z",
          "iopub.status.busy": "2023-09-10T15:51:09.289234Z",
          "iopub.status.idle": "2023-09-10T15:51:09.301973Z",
          "shell.execute_reply": "2023-09-10T15:51:09.300654Z",
          "shell.execute_reply.started": "2023-09-10T15:51:09.289938Z"
        },
        "id": "2-BKwNZYpOXX",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "import os\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
        "\n",
        "from typing import Optional, Union\n",
        "import pandas as pd, numpy as np, torch\n",
        "from datasets import Dataset # 这个的使用方法 hugging face 上面有教程\n",
        "from dataclasses import dataclass\n",
        "from transformers import AutoTokenizer\n",
        "from transformers import EarlyStoppingCallback\n",
        "from transformers.tokenization_utils_base import PreTrainedTokenizerBase, PaddingStrategy\n",
        "from transformers import AutoModelForMultipleChoice, TrainingArguments, Trainer\n",
        "\n",
        "# VER='repeat_V3'\n",
        "# TRAIN WITH SUBSET OF 60K\n",
        "# NUM_TRAIN_SAMPLES = 1_024\n",
        "NUM_TRAIN_SAMPLES = None\n",
        "\n",
        "# PARAMETER EFFICIENT FINE TUNING\n",
        "# PEFT REQUIRES 1XP100 GPU NOT 2XT4\n",
        "USE_PEFT = True\n",
        "# USE_PEFT = True # 这个的全称是 pretrained efficient finetuning, hugging face 上面有教程\n",
        "\n",
        "# NUMBER OF LAYERS TO FREEZE\n",
        "# DEBERTA LARGE HAS TOTAL OF 24 LAYERS\n",
        "FREEZE_LAYERS = 18\n",
        "# FREEZE_LAYERS = 24\n",
        "# FREEZE_LAYERS = 20\n",
        "\n",
        "\n",
        "# BOOLEAN TO FREEZE EMBEDDINGS\n",
        "FREEZE_EMBEDDINGS = True\n",
        "# LENGTH OF CONTEXT PLUS QUESTION ANSWER\n",
        "# 我需要搞懂这个长度到底指的是什么，尤其是 context 和 question 的长度是怎么分配的。256 不可能 cover 全部。\n",
        "# 因为如果模型没能在足够长的 input 中训练，那么positional encoding 很差的模型就不好 extrapolate\n",
        "# MAX_INPUT = 256\n",
        "MAX_INPUT = 786 # 调整这个的大小的时候，每次都需要重新跑一下dataset\n",
        "\n",
        "# HUGGING FACE MODEL\n",
        "MODEL = 'microsoft/deberta-v3-large'\n",
        "VER=f'{FREEZE_LAYERS}_{MAX_INPUT}_original_data'\n",
        "\n",
        "\n",
        "checkpoint_folder = MODEL.split('/')[-1] + '_checkpoints'\n",
        "dataset_folder = MODEL.split('/')[-1] + '_datasets'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-09-10T15:50:47.929122Z",
          "iopub.status.busy": "2023-09-10T15:50:47.928767Z",
          "iopub.status.idle": "2023-09-10T15:50:48.010881Z",
          "shell.execute_reply": "2023-09-10T15:50:48.009882Z",
          "shell.execute_reply.started": "2023-09-10T15:50:47.929092Z"
        },
        "id": "oE5C4UcDpOXY",
        "outputId": "96e5d5db-2376-49b8-ccfd-3b61ba607069",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation data size: (200, 8)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>prompt</th>\n",
              "      <th>context</th>\n",
              "      <th>A</th>\n",
              "      <th>B</th>\n",
              "      <th>C</th>\n",
              "      <th>D</th>\n",
              "      <th>E</th>\n",
              "      <th>answer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Which of the following statements accurately d...</td>\n",
              "      <td>The presence of a clustered thick disk-like co...</td>\n",
              "      <td>MOND is a theory that reduces the observed mis...</td>\n",
              "      <td>MOND is a theory that increases the discrepanc...</td>\n",
              "      <td>MOND is a theory that explains the missing bar...</td>\n",
              "      <td>MOND is a theory that reduces the discrepancy ...</td>\n",
              "      <td>MOND is a theory that eliminates the observed ...</td>\n",
              "      <td>D</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Which of the following is an accurate definiti...</td>\n",
              "      <td>Many of these systems evolve in a self-similar...</td>\n",
              "      <td>Dynamic scaling refers to the evolution of sel...</td>\n",
              "      <td>Dynamic scaling refers to the non-evolution of...</td>\n",
              "      <td>Dynamic scaling refers to the evolution of sel...</td>\n",
              "      <td>Dynamic scaling refers to the non-evolution of...</td>\n",
              "      <td>Dynamic scaling refers to the evolution of sel...</td>\n",
              "      <td>A</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Which of the following statements accurately d...</td>\n",
              "      <td>It is possible that this usage is related with...</td>\n",
              "      <td>The triskeles symbol was reconstructed as a fe...</td>\n",
              "      <td>The triskeles symbol is a representation of th...</td>\n",
              "      <td>The triskeles symbol is a representation of a ...</td>\n",
              "      <td>The triskeles symbol represents three interloc...</td>\n",
              "      <td>The triskeles symbol is a representation of th...</td>\n",
              "      <td>A</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>What is the significance of regularization in ...</td>\n",
              "      <td>Renormalization is distinct from regularizatio...</td>\n",
              "      <td>Regularizing the mass-energy of an electron wi...</td>\n",
              "      <td>Regularizing the mass-energy of an electron wi...</td>\n",
              "      <td>Regularizing the mass-energy of an electron wi...</td>\n",
              "      <td>Regularizing the mass-energy of an electron wi...</td>\n",
              "      <td>Regularizing the mass-energy of an electron wi...</td>\n",
              "      <td>C</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Which of the following statements accurately d...</td>\n",
              "      <td>Several qualitative observations can be made o...</td>\n",
              "      <td>The angular spacing of features in the diffrac...</td>\n",
              "      <td>The angular spacing of features in the diffrac...</td>\n",
              "      <td>The angular spacing of features in the diffrac...</td>\n",
              "      <td>The angular spacing of features in the diffrac...</td>\n",
              "      <td>The angular spacing of features in the diffrac...</td>\n",
              "      <td>D</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>195</th>\n",
              "      <td>What is the relation between the three moment ...</td>\n",
              "      <td>The second equation is more general as it does...</td>\n",
              "      <td>The three moment theorem expresses the relatio...</td>\n",
              "      <td>The three moment theorem is used to calculate ...</td>\n",
              "      <td>The three moment theorem describes the relatio...</td>\n",
              "      <td>The three moment theorem is used to calculate ...</td>\n",
              "      <td>The three moment theorem is used to derive the...</td>\n",
              "      <td>C</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>196</th>\n",
              "      <td>What is the throttling process, and why is it ...</td>\n",
              "      <td>A throttle is the mechanism by which fluid flo...</td>\n",
              "      <td>The throttling process is a steady flow of a f...</td>\n",
              "      <td>The throttling process is a steady adiabatic f...</td>\n",
              "      <td>The throttling process is a steady adiabatic f...</td>\n",
              "      <td>The throttling process is a steady flow of a f...</td>\n",
              "      <td>The throttling process is a steady adiabatic f...</td>\n",
              "      <td>B</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>197</th>\n",
              "      <td>What happens to excess base metal as a solutio...</td>\n",
              "      <td>Furthermore, this melting may begin at a tempe...</td>\n",
              "      <td>The excess base metal will often solidify, bec...</td>\n",
              "      <td>The excess base metal will often crystallize-o...</td>\n",
              "      <td>The excess base metal will often dissolve, bec...</td>\n",
              "      <td>The excess base metal will often liquefy, beco...</td>\n",
              "      <td>The excess base metal will often evaporate, be...</td>\n",
              "      <td>B</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>198</th>\n",
              "      <td>What is the relationship between mass, force, ...</td>\n",
              "      <td>Newton first set out the definition of mass Th...</td>\n",
              "      <td>Mass is a property that determines the weight ...</td>\n",
              "      <td>Mass is an inertial property that determines a...</td>\n",
              "      <td>Mass is an inertial property that determines a...</td>\n",
              "      <td>Mass is an inertial property that determines a...</td>\n",
              "      <td>Mass is a property that determines the size of...</td>\n",
              "      <td>D</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>199</th>\n",
              "      <td>What did Arthur Eddington discover about two o...</td>\n",
              "      <td>Eddington's criticism seems to have been based...</td>\n",
              "      <td>Arthur Eddington showed that two of Einstein's...</td>\n",
              "      <td>Arthur Eddington showed that two of Einstein's...</td>\n",
              "      <td>Arthur Eddington showed that two of Einstein's...</td>\n",
              "      <td>Arthur Eddington showed that two of Einstein's...</td>\n",
              "      <td>Arthur Eddington showed that two of Einstein's...</td>\n",
              "      <td>C</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>200 rows × 8 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                prompt  \\\n",
              "0    Which of the following statements accurately d...   \n",
              "1    Which of the following is an accurate definiti...   \n",
              "2    Which of the following statements accurately d...   \n",
              "3    What is the significance of regularization in ...   \n",
              "4    Which of the following statements accurately d...   \n",
              "..                                                 ...   \n",
              "195  What is the relation between the three moment ...   \n",
              "196  What is the throttling process, and why is it ...   \n",
              "197  What happens to excess base metal as a solutio...   \n",
              "198  What is the relationship between mass, force, ...   \n",
              "199  What did Arthur Eddington discover about two o...   \n",
              "\n",
              "                                               context  \\\n",
              "0    The presence of a clustered thick disk-like co...   \n",
              "1    Many of these systems evolve in a self-similar...   \n",
              "2    It is possible that this usage is related with...   \n",
              "3    Renormalization is distinct from regularizatio...   \n",
              "4    Several qualitative observations can be made o...   \n",
              "..                                                 ...   \n",
              "195  The second equation is more general as it does...   \n",
              "196  A throttle is the mechanism by which fluid flo...   \n",
              "197  Furthermore, this melting may begin at a tempe...   \n",
              "198  Newton first set out the definition of mass Th...   \n",
              "199  Eddington's criticism seems to have been based...   \n",
              "\n",
              "                                                     A  \\\n",
              "0    MOND is a theory that reduces the observed mis...   \n",
              "1    Dynamic scaling refers to the evolution of sel...   \n",
              "2    The triskeles symbol was reconstructed as a fe...   \n",
              "3    Regularizing the mass-energy of an electron wi...   \n",
              "4    The angular spacing of features in the diffrac...   \n",
              "..                                                 ...   \n",
              "195  The three moment theorem expresses the relatio...   \n",
              "196  The throttling process is a steady flow of a f...   \n",
              "197  The excess base metal will often solidify, bec...   \n",
              "198  Mass is a property that determines the weight ...   \n",
              "199  Arthur Eddington showed that two of Einstein's...   \n",
              "\n",
              "                                                     B  \\\n",
              "0    MOND is a theory that increases the discrepanc...   \n",
              "1    Dynamic scaling refers to the non-evolution of...   \n",
              "2    The triskeles symbol is a representation of th...   \n",
              "3    Regularizing the mass-energy of an electron wi...   \n",
              "4    The angular spacing of features in the diffrac...   \n",
              "..                                                 ...   \n",
              "195  The three moment theorem is used to calculate ...   \n",
              "196  The throttling process is a steady adiabatic f...   \n",
              "197  The excess base metal will often crystallize-o...   \n",
              "198  Mass is an inertial property that determines a...   \n",
              "199  Arthur Eddington showed that two of Einstein's...   \n",
              "\n",
              "                                                     C  \\\n",
              "0    MOND is a theory that explains the missing bar...   \n",
              "1    Dynamic scaling refers to the evolution of sel...   \n",
              "2    The triskeles symbol is a representation of a ...   \n",
              "3    Regularizing the mass-energy of an electron wi...   \n",
              "4    The angular spacing of features in the diffrac...   \n",
              "..                                                 ...   \n",
              "195  The three moment theorem describes the relatio...   \n",
              "196  The throttling process is a steady adiabatic f...   \n",
              "197  The excess base metal will often dissolve, bec...   \n",
              "198  Mass is an inertial property that determines a...   \n",
              "199  Arthur Eddington showed that two of Einstein's...   \n",
              "\n",
              "                                                     D  \\\n",
              "0    MOND is a theory that reduces the discrepancy ...   \n",
              "1    Dynamic scaling refers to the non-evolution of...   \n",
              "2    The triskeles symbol represents three interloc...   \n",
              "3    Regularizing the mass-energy of an electron wi...   \n",
              "4    The angular spacing of features in the diffrac...   \n",
              "..                                                 ...   \n",
              "195  The three moment theorem is used to calculate ...   \n",
              "196  The throttling process is a steady flow of a f...   \n",
              "197  The excess base metal will often liquefy, beco...   \n",
              "198  Mass is an inertial property that determines a...   \n",
              "199  Arthur Eddington showed that two of Einstein's...   \n",
              "\n",
              "                                                     E answer  \n",
              "0    MOND is a theory that eliminates the observed ...      D  \n",
              "1    Dynamic scaling refers to the evolution of sel...      A  \n",
              "2    The triskeles symbol is a representation of th...      A  \n",
              "3    Regularizing the mass-energy of an electron wi...      C  \n",
              "4    The angular spacing of features in the diffrac...      D  \n",
              "..                                                 ...    ...  \n",
              "195  The three moment theorem is used to derive the...      C  \n",
              "196  The throttling process is a steady adiabatic f...      B  \n",
              "197  The excess base metal will often evaporate, be...      B  \n",
              "198  Mass is a property that determines the size of...      D  \n",
              "199  Arthur Eddington showed that two of Einstein's...      C  \n",
              "\n",
              "[200 rows x 8 columns]"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_valid = pd.read_csv('../input/60k-data-with-context-v2/train_with_context2.csv')\n",
        "print('Validation data size:', df_valid.shape )\n",
        "df_valid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-09-10T15:52:03.937771Z",
          "iopub.status.busy": "2023-09-10T15:52:03.936994Z",
          "iopub.status.idle": "2023-09-10T15:52:11.860278Z",
          "shell.execute_reply": "2023-09-10T15:52:11.859288Z",
          "shell.execute_reply.started": "2023-09-10T15:52:03.937732Z"
        },
        "id": "hRQBsmRlpOXZ",
        "outputId": "e0e6a537-0e94-4acf-95a8-3f568324617a",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "size of dataset 60347\n",
            "Train data size: (60347, 8)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>prompt</th>\n",
              "      <th>context</th>\n",
              "      <th>A</th>\n",
              "      <th>B</th>\n",
              "      <th>C</th>\n",
              "      <th>D</th>\n",
              "      <th>E</th>\n",
              "      <th>answer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>In relation to Eunice Fay McKenzie's career, w...</td>\n",
              "      <td>Eunice Fay McKenzie (February 19, 1918 – April...</td>\n",
              "      <td>McKenzie showcased her singing talents in nume...</td>\n",
              "      <td>McKenzie is primarily remembered for her starr...</td>\n",
              "      <td>McKenzie gained recognition for her role as a ...</td>\n",
              "      <td>McKenzie's collaborations with director Blake ...</td>\n",
              "      <td>McKenzie's successful career in sound films co...</td>\n",
              "      <td>B</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>How does Modified Newtonian Dynamics (MOND) im...</td>\n",
              "      <td>The presence of a clustered thick disk-like co...</td>\n",
              "      <td>MOND is a theory that increases the discrepanc...</td>\n",
              "      <td>MOND explains the missing baryonic mass in gal...</td>\n",
              "      <td>MOND is a theory that reduces the observed mis...</td>\n",
              "      <td>MOND is a theory that eliminates the observed ...</td>\n",
              "      <td>MOND's impact on the observed missing baryonic...</td>\n",
              "      <td>E</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Which of the following statements accurately d...</td>\n",
              "      <td>Woody Hartman is a retired American soccer goa...</td>\n",
              "      <td>Ray Montgomerie is a former footballer who pla...</td>\n",
              "      <td>Ray Montgomerie is a former footballer who pla...</td>\n",
              "      <td>Ray Montgomerie is a former footballer who pla...</td>\n",
              "      <td>Ray Montgomerie is a former footballer who pla...</td>\n",
              "      <td>Ray Montgomerie is a former footballer who pla...</td>\n",
              "      <td>B</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>What is the significance of the Museum of the ...</td>\n",
              "      <td>The Museum of the Occupation of Latvia () is a...</td>\n",
              "      <td>The Museum of the Occupation of Latvia is a me...</td>\n",
              "      <td>The Museum of the Occupation of Latvia showcas...</td>\n",
              "      <td>The Museum of the Occupation of Latvia was est...</td>\n",
              "      <td>The Museum of the Occupation of Latvia primari...</td>\n",
              "      <td>The Museum of the Occupation of Latvia is a mu...</td>\n",
              "      <td>C</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>What was the previous name of the Christian Sc...</td>\n",
              "      <td>It was named the Evangelical School for the De...</td>\n",
              "      <td>The Christian School for the Deaf (CSD)</td>\n",
              "      <td>The Christian School for the Blind (CSB)</td>\n",
              "      <td>The Evangelical School and Chapel for the Deaf...</td>\n",
              "      <td>The Evangelical School for the Deaf (ESD)</td>\n",
              "      <td>The Evangelical School for the Blind (ESB)</td>\n",
              "      <td>D</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>60342</th>\n",
              "      <td>The outer ear, or ear canal, carries sound to ...</td>\n",
              "      <td>The ear canal (external acoustic meatus, exter...</td>\n",
              "      <td>aorta</td>\n",
              "      <td>ear lobe</td>\n",
              "      <td>eardrum</td>\n",
              "      <td>lungs</td>\n",
              "      <td></td>\n",
              "      <td>C</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>60343</th>\n",
              "      <td>What sport involves people quickly finding des...</td>\n",
              "      <td>Orienteering sports in which route choice is a...</td>\n",
              "      <td>mapping</td>\n",
              "      <td></td>\n",
              "      <td>orienteering</td>\n",
              "      <td>patterning</td>\n",
              "      <td>sticking</td>\n",
              "      <td>C</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>60344</th>\n",
              "      <td>Almost all earthquakes occur at which place?</td>\n",
              "      <td>This subduction zone led to the formation of t...</td>\n",
              "      <td>mountains</td>\n",
              "      <td>land boundaries</td>\n",
              "      <td>plate boundaries</td>\n",
              "      <td>continental shelf</td>\n",
              "      <td></td>\n",
              "      <td>C</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>60345</th>\n",
              "      <td>Melting glaciers, rising temperatures and drou...</td>\n",
              "      <td>Impacts include changes in regional rainfall p...</td>\n",
              "      <td>nature's natural cycle</td>\n",
              "      <td>air pollution</td>\n",
              "      <td>global warming</td>\n",
              "      <td>sudden warming</td>\n",
              "      <td></td>\n",
              "      <td>C</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>60346</th>\n",
              "      <td>What parts of a human possess the highest conc...</td>\n",
              "      <td>In thermoregulation, body heat is generated mo...</td>\n",
              "      <td></td>\n",
              "      <td>hand and ears</td>\n",
              "      <td>face and hair</td>\n",
              "      <td>face and ears</td>\n",
              "      <td>hands and feet</td>\n",
              "      <td>D</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>60347 rows × 8 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  prompt  \\\n",
              "0      In relation to Eunice Fay McKenzie's career, w...   \n",
              "1      How does Modified Newtonian Dynamics (MOND) im...   \n",
              "2      Which of the following statements accurately d...   \n",
              "3      What is the significance of the Museum of the ...   \n",
              "4      What was the previous name of the Christian Sc...   \n",
              "...                                                  ...   \n",
              "60342  The outer ear, or ear canal, carries sound to ...   \n",
              "60343  What sport involves people quickly finding des...   \n",
              "60344       Almost all earthquakes occur at which place?   \n",
              "60345  Melting glaciers, rising temperatures and drou...   \n",
              "60346  What parts of a human possess the highest conc...   \n",
              "\n",
              "                                                 context  \\\n",
              "0      Eunice Fay McKenzie (February 19, 1918 – April...   \n",
              "1      The presence of a clustered thick disk-like co...   \n",
              "2      Woody Hartman is a retired American soccer goa...   \n",
              "3      The Museum of the Occupation of Latvia () is a...   \n",
              "4      It was named the Evangelical School for the De...   \n",
              "...                                                  ...   \n",
              "60342  The ear canal (external acoustic meatus, exter...   \n",
              "60343  Orienteering sports in which route choice is a...   \n",
              "60344  This subduction zone led to the formation of t...   \n",
              "60345  Impacts include changes in regional rainfall p...   \n",
              "60346  In thermoregulation, body heat is generated mo...   \n",
              "\n",
              "                                                       A  \\\n",
              "0      McKenzie showcased her singing talents in nume...   \n",
              "1      MOND is a theory that increases the discrepanc...   \n",
              "2      Ray Montgomerie is a former footballer who pla...   \n",
              "3      The Museum of the Occupation of Latvia is a me...   \n",
              "4                The Christian School for the Deaf (CSD)   \n",
              "...                                                  ...   \n",
              "60342                                              aorta   \n",
              "60343                                            mapping   \n",
              "60344                                          mountains   \n",
              "60345                             nature's natural cycle   \n",
              "60346                                                      \n",
              "\n",
              "                                                       B  \\\n",
              "0      McKenzie is primarily remembered for her starr...   \n",
              "1      MOND explains the missing baryonic mass in gal...   \n",
              "2      Ray Montgomerie is a former footballer who pla...   \n",
              "3      The Museum of the Occupation of Latvia showcas...   \n",
              "4               The Christian School for the Blind (CSB)   \n",
              "...                                                  ...   \n",
              "60342                                           ear lobe   \n",
              "60343                                                      \n",
              "60344                                    land boundaries   \n",
              "60345                                      air pollution   \n",
              "60346                                      hand and ears   \n",
              "\n",
              "                                                       C  \\\n",
              "0      McKenzie gained recognition for her role as a ...   \n",
              "1      MOND is a theory that reduces the observed mis...   \n",
              "2      Ray Montgomerie is a former footballer who pla...   \n",
              "3      The Museum of the Occupation of Latvia was est...   \n",
              "4      The Evangelical School and Chapel for the Deaf...   \n",
              "...                                                  ...   \n",
              "60342                                            eardrum   \n",
              "60343                                       orienteering   \n",
              "60344                                   plate boundaries   \n",
              "60345                                     global warming   \n",
              "60346                                      face and hair   \n",
              "\n",
              "                                                       D  \\\n",
              "0      McKenzie's collaborations with director Blake ...   \n",
              "1      MOND is a theory that eliminates the observed ...   \n",
              "2      Ray Montgomerie is a former footballer who pla...   \n",
              "3      The Museum of the Occupation of Latvia primari...   \n",
              "4              The Evangelical School for the Deaf (ESD)   \n",
              "...                                                  ...   \n",
              "60342                                              lungs   \n",
              "60343                                         patterning   \n",
              "60344                                  continental shelf   \n",
              "60345                                     sudden warming   \n",
              "60346                                      face and ears   \n",
              "\n",
              "                                                       E answer  \n",
              "0      McKenzie's successful career in sound films co...      B  \n",
              "1      MOND's impact on the observed missing baryonic...      E  \n",
              "2      Ray Montgomerie is a former footballer who pla...      B  \n",
              "3      The Museum of the Occupation of Latvia is a mu...      C  \n",
              "4             The Evangelical School for the Blind (ESB)      D  \n",
              "...                                                  ...    ...  \n",
              "60342                                                         C  \n",
              "60343                                           sticking      C  \n",
              "60344                                                         C  \n",
              "60345                                                         C  \n",
              "60346                                     hands and feet      D  \n",
              "\n",
              "[60347 rows x 8 columns]"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_train = pd.read_csv('../input/60k-data-with-context-v2/all_12_with_context2.csv')\n",
        "# df_train = pd.read_csv('../input/99k-context/RACE_with_context_original.csv')\n",
        "\n",
        "print('size of dataset', len(df_train))\n",
        "# df_train = df_train.drop(columns=\"source\")\n",
        "if 'source' in df_train.columns:\n",
        "    df_train = df_train.drop(columns=\"source\")\n",
        "df_train = df_train.fillna('')\n",
        "if NUM_TRAIN_SAMPLES:\n",
        "    df_train = df_train.sample(NUM_TRAIN_SAMPLES) # taken NUM_TRAIN_SAMPLES of samples here\n",
        "print('Train data size:', df_train.shape )\n",
        "df_train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GctqBhbbpOXZ"
      },
      "source": [
        "# Data Loader\n",
        "Code is from Radek's notebook [here][1] with modifications to the tokenization process.\n",
        "\n",
        "[1]: https://www.kaggle.com/code/radek1/new-dataset-deberta-v3-large-training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-09-10T15:52:22.561293Z",
          "iopub.status.busy": "2023-09-10T15:52:22.560935Z",
          "iopub.status.idle": "2023-09-10T15:52:22.575102Z",
          "shell.execute_reply": "2023-09-10T15:52:22.574170Z",
          "shell.execute_reply.started": "2023-09-10T15:52:22.561265Z"
        },
        "id": "WIt-GqLApOXZ",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "option_to_index = {option: idx for idx, option in enumerate('ABCDE')}\n",
        "index_to_option = {v: k for k,v in option_to_index.items()}\n",
        "\n",
        "# 等于说训练的时候模型是可以看到context的，因此与预测保持一致\n",
        "def preprocess(example, tokenizer):\n",
        "    first_sentence = [ \"[CLS] \" + example['context'] ] * 5\n",
        "    second_sentences = [\" #### \" + example['prompt'] + \" [SEP] \" + example[option] + \" [SEP]\" for option in 'ABCDE']\n",
        "    tokenized_example = tokenizer(first_sentence, second_sentences, truncation='only_first',\n",
        "                                  max_length=MAX_INPUT, add_special_tokens=False)\n",
        "    tokenized_example['label'] = option_to_index[example['answer']]\n",
        "\n",
        "    return tokenized_example\n",
        "\n",
        "def race_preprocess(example, tokenizer, max_length=None):\n",
        "    context = example[\"context\"].replace(\"\\n\", \" \")\n",
        "    first_sentence = [\"[CLS] \" + context] * 4\n",
        "    second_sentences = [\n",
        "        \" #### \" + example[\"prompt\"] + \" [SEP] \" + example[option] + \" [SEP]\"\n",
        "        for option in \"ABCD\"\n",
        "    ]\n",
        "    tokenized_example = tokenizer(\n",
        "        first_sentence,\n",
        "        second_sentences,\n",
        "        truncation=\"only_first\",\n",
        "        max_length=max_length,\n",
        "        add_special_tokens=False,\n",
        "    )\n",
        "    tokenized_example[\"label\"] = option_to_index[example[\"answer\"]]\n",
        "\n",
        "    return tokenized_example\n",
        "\n",
        "@dataclass\n",
        "class DataCollatorForMultipleChoice:\n",
        "    tokenizer: PreTrainedTokenizerBase\n",
        "    padding: Union[bool, str, PaddingStrategy] = True\n",
        "    max_length: Optional[int] = None\n",
        "    pad_to_multiple_of: Optional[int] = None\n",
        "    \n",
        "    def __call__(self, features):\n",
        "        label_name = 'label' if 'label' in features[0].keys() else 'labels'\n",
        "        labels = [feature.pop(label_name) for feature in features]\n",
        "        batch_size = len(features)\n",
        "        num_choices = len(features[0]['input_ids'])\n",
        "        flattened_features = [\n",
        "            [{k: v[i] for k, v in feature.items()} for i in range(num_choices)] for feature in features\n",
        "        ]\n",
        "        flattened_features = sum(flattened_features, [])\n",
        "        \n",
        "        # Huggingface tokenizer padding\n",
        "        batch = self.tokenizer.pad(\n",
        "            flattened_features,\n",
        "            padding=self.padding,\n",
        "            max_length=self.max_length,\n",
        "            pad_to_multiple_of=self.pad_to_multiple_of, # this is related to mixed precision training\n",
        "            return_tensors='pt',\n",
        "        )\n",
        "\n",
        "        batch = {k: v.view(batch_size, num_choices, -1) for k, v in batch.items()}\n",
        "        batch['labels'] = torch.tensor(labels, dtype=torch.int64)\n",
        "        return batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-09-10T15:52:29.894379Z",
          "iopub.status.busy": "2023-09-10T15:52:29.894012Z",
          "iopub.status.idle": "2023-09-10T15:52:31.881652Z",
          "shell.execute_reply": "2023-09-10T15:52:31.880578Z",
          "shell.execute_reply.started": "2023-09-10T15:52:29.894346Z"
        },
        "id": "HAwYv9UgpOXZ",
        "outputId": "43b1daa9-d0be-475f-c162-a6070a9d277a",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "/home/dijkstraz/anaconda3/envs/kaggle/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:470: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
            "  warnings.warn(\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "DebertaV2TokenizerFast(name_or_path='microsoft/deberta-v3-large', vocab_size=128000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '[CLS]', 'eos_token': '[SEP]', 'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True)"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 每个 tokenizer 都是 pretrained model 在 pretraining 的时候使用的\n",
        "# 在 tokenizer 的 special tokens 中, 注意到 [CLS] 同时是 bos 和 cls token; cls_token 是为了让模型知道要做 classification 了\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
        "tokenizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-09-10T15:55:08.821857Z",
          "iopub.status.busy": "2023-09-10T15:55:08.821159Z",
          "iopub.status.idle": "2023-09-10T15:55:09.689547Z",
          "shell.execute_reply": "2023-09-10T15:55:09.688542Z",
          "shell.execute_reply.started": "2023-09-10T15:55:08.821820Z"
        },
        "id": "6vtlJkf_pOXZ",
        "outputId": "e3f30554-cb76-4ee7-acc6-66a6e03edcf6",
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['prompt', 'context', 'A', 'B', 'C', 'D', 'E', 'answer'],\n",
              "    num_rows: 60347\n",
              "})"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset_valid = Dataset.from_pandas(df_valid)\n",
        "dataset = Dataset.from_pandas(df_train)\n",
        "if '__index_level_0__' in dataset._info.features: # 加了这行防爆\n",
        "    print('removing __index_level_0__')\n",
        "    dataset = dataset.remove_columns([\"__index_level_0__\"])\n",
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "5ec15ebf654445758191ae45232728b8"
          ]
        },
        "execution": {
          "iopub.execute_input": "2023-09-10T15:55:13.711687Z",
          "iopub.status.busy": "2023-09-10T15:55:13.711294Z",
          "iopub.status.idle": "2023-09-10T15:55:36.510447Z",
          "shell.execute_reply": "2023-09-10T15:55:36.508573Z",
          "shell.execute_reply.started": "2023-09-10T15:55:13.711657Z"
        },
        "id": "bM1YxNYqpOXa",
        "outputId": "57e233ff-9cc1-4c3d-85e4-112a29151ad4",
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ff8707a94b1e4f04bc1c2b45f31d3056",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "getting tokenized_dataset_786 from disk\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['input_ids', 'token_type_ids', 'attention_mask', 'label'],\n",
              "    num_rows: 60347\n",
              "})"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from functools import partial\n",
        "preprocess = partial(preprocess, tokenizer=tokenizer)\n",
        "race_preprocess = partial(race_preprocess, tokenizer=tokenizer)\n",
        "\n",
        "tokenized_dataset_valid = dataset_valid.map(preprocess, remove_columns=['prompt', 'context', 'A', 'B', 'C', 'D', 'E', 'answer'])\n",
        "\n",
        "num_choices = len([x for x in dataset.column_names if x in 'ABCDEFG'])\n",
        "dataset_path = f'./{dataset_folder}/tokenized_dataset_{MAX_INPUT}_{num_choices}'\n",
        "if os.path.exists(dataset_path):\n",
        "    print(f'getting tokenized_dataset_{MAX_INPUT} from disk')\n",
        "    tokenized_dataset = Dataset.load_from_disk(dataset_path)\n",
        "else:\n",
        "    if num_choices == 5:\n",
        "        tokenized_dataset = dataset.map(preprocess, remove_columns=dataset.column_names)\n",
        "    elif num_choices == 4:\n",
        "        tokenized_dataset = dataset.map(race_preprocess, remove_columns=dataset.column_names)\n",
        "        \n",
        "    tokenized_dataset.save_to_disk(dataset_path)\n",
        "\n",
        "# changed by cxzheng, fuck! stucked!\n",
        "\n",
        "tokenized_dataset # 他跑到 21100 附近的时候会卡住，有点奇怪"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V6VMqZv8pOXa"
      },
      "source": [
        "# MAP@3 Metric\n",
        "The competition metric is MAP@3 therefore we will make a custom code to add to Hugging Face's trainer. Discussion [here][1]\n",
        "\n",
        "[1]: https://www.kaggle.com/competitions/kaggle-llm-science-exam/discussion/435602"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-09-09T03:47:36.361768Z",
          "iopub.status.busy": "2023-09-09T03:47:36.361320Z",
          "iopub.status.idle": "2023-09-09T03:47:36.378492Z",
          "shell.execute_reply": "2023-09-09T03:47:36.377185Z",
          "shell.execute_reply.started": "2023-09-09T03:47:36.361728Z"
        },
        "id": "Is1hFh9KpOXa",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def map_at_3(predictions, labels):\n",
        "    map_sum = 0\n",
        "    pred = np.argsort(-1*np.array(predictions),axis=1)[:,:3]\n",
        "    for x,y in zip(pred,labels):\n",
        "        z = [1/i if y==j else 0 for i,j in zip([1,2,3],x)]\n",
        "        map_sum += np.sum(z)\n",
        "    return map_sum / len(predictions)\n",
        "\n",
        "def compute_metrics(p):\n",
        "    predictions = p.predictions.tolist()\n",
        "    labels = p.label_ids.tolist()\n",
        "    return {\"map@3\": map_at_3(predictions, labels)}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Drq0chcepOXa"
      },
      "source": [
        "# Train and Save\n",
        "We will now train and save our model using Hugging Face's easy to use trainer. By adjusting the parameters in this notebook, we can achieve `CV MAP@3 = 0.915+` and corresponding single model `LB MAP@3 = 0.830+` wow!\n",
        "\n",
        "In we run this notebook outside of Kaggle then we can train longer and with more RAM. If we run this notebook on Kaggle, then we need to use tricks to train models efficiently. Here are some ideas:\n",
        "* use fp16 (this speeds up T4 not P100)\n",
        "* use gradient_accumlation_steps (this simulates larger batch sizes)\n",
        "* use gradient_checkpointing (this uses disk to save RAM)\n",
        "* use 2xT4 instead of 1xP100 (this doubles GPUs)\n",
        "* freeze model embeddings (this reduces weights to train)\n",
        "* freeze some model layers (this reduces weights to train)\n",
        "* use PEFT (this reduces weights to train)\n",
        "* increase LR and decrease epochs (this reduces work)\n",
        "* use smaller models (this reduces weights to train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gu-ItbRYpOXa"
      },
      "source": [
        "We will use a Hugging Face AutoModelForMultipleChoice. For the list of possible models, see Hugging Face's repository [here][1]. We can optionally use PEFT to accelerate training and use less memory. However i have noticed that validation accuracy is less. (Note that PEFT requires us to use 1xP100 not 2xT4 GPU. I'm not sure why). We can also optionally freeze layers. This also accelerates training and uses less memory. However validation accuracy may become less.\n",
        "\n",
        "[1]: https://huggingface.co/models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9x6IM3xDpOXa"
      },
      "outputs": [],
      "source": [
        "# # NOTE PEFT REQUIRES US TO USE 1XP100 NOT 2XT4. I'M NOT SURE WHY.\n",
        "# if USE_PEFT:\n",
        "#     !pip install --no-index --no-deps ../input/llm-whls/peft-0.4.0-py3-none-any.whl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "7TBjQX4QpOXa",
        "outputId": "80577902-ddd0-4d1d-f15a-bafa1fde1deb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of DebertaV2ForMultipleChoice were not initialized from the model checkpoint at microsoft/deberta-v3-large and are newly initialized: ['pooler.dense.bias', 'classifier.weight', 'pooler.dense.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "We are using PEFT.\n",
            "trainable params: 2,887,682 || all params: 436,899,842 || trainable%: 0.6609482820549979\n"
          ]
        }
      ],
      "source": [
        "model = AutoModelForMultipleChoice.from_pretrained(MODEL)\n",
        "\n",
        "if USE_PEFT:\n",
        "    print('We are using PEFT.')\n",
        "    from peft import LoraConfig, get_peft_model, TaskType\n",
        "    peft_config = LoraConfig(\n",
        "        r=8, lora_alpha=4, task_type=TaskType.SEQ_CLS, lora_dropout=0.1,\n",
        "        bias=\"none\", inference_mode=False,\n",
        "        target_modules=[\"query_proj\", \"value_proj\"],\n",
        "        modules_to_save=['classifier','pooler'],\n",
        "    )\n",
        "    model = get_peft_model(model, peft_config)\n",
        "    model.print_trainable_parameters()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "NfLCv-dCpOXb",
        "outputId": "dcd0b1f7-1035-4534-f96c-482de3bfbf56"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Freezing embeddings.\n",
            "Freezing 18 layers.\n",
            "77875202 436899842 0.17824497633945127\n"
          ]
        }
      ],
      "source": [
        "\n",
        "if FREEZE_EMBEDDINGS:\n",
        "    print('Freezing embeddings.')\n",
        "    for param in model.deberta.embeddings.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "if FREEZE_LAYERS>0:\n",
        "    print(f'Freezing {FREEZE_LAYERS} layers.')\n",
        "    for layer in model.deberta.encoder.layer[:FREEZE_LAYERS]:\n",
        "        for param in layer.parameters():\n",
        "            param.requires_grad = False\n",
        "    # Newly added for v3\n",
        "    for layer in model.deberta.encoder.layer[FREEZE_LAYERS:]:\n",
        "        for param in layer.parameters():\n",
        "            param.requires_grad = True\n",
        "\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
        "    trainable_params = sum([np.prod(p.size()) for p in model_parameters])\n",
        "    print(trainable_params, total_params, trainable_params/total_params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "ZQvoNAiUpOXb"
      },
      "outputs": [],
      "source": [
        "from transformers.trainer_utils import PREFIX_CHECKPOINT_DIR\n",
        "from transformers import TrainerState, TrainerControl, TrainerCallback\n",
        "class SavePeftModelCallback(TrainerCallback):\n",
        "    def on_save(\n",
        "        self,\n",
        "        args: TrainingArguments,\n",
        "        state: TrainerState,\n",
        "        control: TrainerControl,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        checkpoint_folder = os.path.join(\n",
        "            args.output_dir, f\"{PREFIX_CHECKPOINT_DIR}-{state.global_step}\"\n",
        "        )\n",
        "\n",
        "        peft_model_path = os.path.join(checkpoint_folder, \"torch_model\")\n",
        "        # peft_model_path = os.path.join(checkpoint_folder)\n",
        "        kwargs[\"model\"].base_model.save_pretrained(peft_model_path)\n",
        "\n",
        "        # pytorch_model_path = os.path.join(checkpoint_folder, \"pytorch_model.bin\")\n",
        "        # if os.path.exists(pytorch_model_path):\n",
        "        #     os.remove(pytorch_model_path)\n",
        "        return control"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-09-09T03:47:40.478843Z",
          "iopub.status.busy": "2023-09-09T03:47:40.478276Z",
          "iopub.status.idle": "2023-09-09T03:47:40.489110Z",
          "shell.execute_reply": "2023-09-09T03:47:40.488156Z",
          "shell.execute_reply.started": "2023-09-09T03:47:40.478811Z"
        },
        "id": "cDnA4352pOXb",
        "outputId": "d31cfc8b-7ecd-48be-bc95-b295a9e9b210",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4\n"
          ]
        }
      ],
      "source": [
        "# batch_size = 4\n",
        "# target_batch_size = 512\n",
        "# GRAD_ACCUM = target_batch_size // batch_size\n",
        "# # SAVING_STEP = 10\n",
        "# SAVING_STEP = 4\n",
        "\n",
        "# LOGGING_STEPS = SAVING_STEP\n",
        "# print(SAVING_STEP)\n",
        "# training_args = TrainingArguments(\n",
        "#     warmup_ratio=0.03,\n",
        "#     learning_rate=1e-5, # maybe 1e5\n",
        "#     # per_device_train_batch_size=16,\n",
        "#     per_device_train_batch_size=batch_size,\n",
        "#     # per_device_eval_batch_size=32,\n",
        "#     per_device_eval_batch_size=batch_size*2,\n",
        "#     num_train_epochs=3,  # 2.5\n",
        "#     report_to='none',\n",
        "#     output_dir = f'./checkpoints_{VER}',\n",
        "#     overwrite_output_dir=True,\n",
        "#     fp16=True,\n",
        "#     # gradient_accumulation_steps=8,\n",
        "#     gradient_accumulation_steps=GRAD_ACCUM,\n",
        "#     logging_steps=LOGGING_STEPS,\n",
        "#     evaluation_strategy='steps',\n",
        "#     eval_steps=SAVING_STEP,\n",
        "#     save_strategy=\"steps\",\n",
        "#     save_steps=SAVING_STEP,\n",
        "#     load_best_model_at_end=False,\n",
        "#     metric_for_best_model='map@3',\n",
        "#     lr_scheduler_type='cosine',\n",
        "#     # weight_decay=0.01,\n",
        "#     weight_decay=1e-3,\n",
        "#     save_total_limit=5,\n",
        "\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4\n"
          ]
        }
      ],
      "source": [
        "# batch_size = 8 # Try this if possible for 18 512\n",
        "batch_size = 4 # for 16 512\n",
        "\n",
        "# effective_batch_size = 1024\n",
        "effective_batch_size = 512\n",
        "# effective_batch_size = 256\n",
        "\n",
        "# effective_batch_size = 128\n",
        "GRAD_ACCUM = effective_batch_size // batch_size\n",
        "SAVING_STEP = 4\n",
        "LOGGING_STEPS = SAVING_STEP\n",
        "print(SAVING_STEP)\n",
        "training_args = TrainingArguments(\n",
        "    # warmup_ratio=0.1, \n",
        "    # warmup_ratio=0.0, \n",
        "    warmup_ratio = 0.03,\n",
        "    # warmup_ratio = 0.0,\n",
        "    # learning_rate = 1e-4,\n",
        "    learning_rate = 2.28e-5,\n",
        "    # learning_rate = 2.28e-5 * 1.6,\n",
        "\n",
        "    # max_grad_norm = 2.0,\n",
        "    max_grad_norm = 1.0,\n",
        "\n",
        "    # max_grad_norm = 0.3,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    num_train_epochs=3,  # 2.5\n",
        "    report_to='none',\n",
        "    output_dir = f'./{checkpoint_folder}/{VER}',\n",
        "    overwrite_output_dir=True,\n",
        "    fp16=True,\n",
        "    # gradient_accumulation_steps=8,\n",
        "    gradient_accumulation_steps=GRAD_ACCUM,\n",
        "    logging_steps=LOGGING_STEPS,\n",
        "    evaluation_strategy='steps',\n",
        "    eval_steps=SAVING_STEP,\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=SAVING_STEP,\n",
        "    load_best_model_at_end=False,\n",
        "    # metric_for_best_model='map@3',\n",
        "    metric_for_best_model='eval_loss',\n",
        "    seed=666,\n",
        "    # lr_scheduler_type='linear',\n",
        "    lr_scheduler_type='cosine',\n",
        "    # lr_scheduler_type='cosine_with_restarts',    \n",
        "    # lr_scheduler_type='reduce_lr_on_plateau',\n",
        "    # weight_decay=0.01,\n",
        "    # weight_decay=1e-6, # set this slightly higher to reduce oscillation\n",
        "    weight_decay=1e-3, # set this slightly higher to reduce oscillation\n",
        "    # weight_decay=3e-4, # set this slightly higher to reduce oscillation\n",
        "    save_total_limit=5,\n",
        "    \n",
        ")\n",
        "# training_args = training_args.set_optimizer(name=\"adamw_torch\", beta1=0.9, beta2=0.98, weight_decay=training_args.weight_decay)\n",
        "# training_args = training_args.set_lr_scheduler(name=\"reduce_lr_on_plateau\", )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {},
      "outputs": [],
      "source": [
        "del model, trainer\n",
        "import gc\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-09-09T03:47:44.277808Z",
          "iopub.status.busy": "2023-09-09T03:47:44.277446Z",
          "iopub.status.idle": "2023-09-09T03:54:54.290539Z",
          "shell.execute_reply": "2023-09-09T03:54:54.289078Z",
          "shell.execute_reply.started": "2023-09-09T03:47:44.277778Z"
        },
        "id": "XTDW7baOpOXb",
        "outputId": "24cab7c6-d6a2-484f-d2b8-f23cad18264b",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dijkstraz/anaconda3/envs/kaggle/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='342' max='351' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [342/351 12:48:48 < 20:21, 0.01 it/s, Epoch 2.89/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Map@3</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>1.614300</td>\n",
              "      <td>1.608413</td>\n",
              "      <td>0.398333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>1.615500</td>\n",
              "      <td>1.610454</td>\n",
              "      <td>0.310000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>1.611200</td>\n",
              "      <td>1.609639</td>\n",
              "      <td>0.310000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>1.614500</td>\n",
              "      <td>1.609487</td>\n",
              "      <td>0.325833</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>1.612200</td>\n",
              "      <td>1.609429</td>\n",
              "      <td>0.340833</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>1.613400</td>\n",
              "      <td>1.609424</td>\n",
              "      <td>0.387500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>1.611800</td>\n",
              "      <td>1.609355</td>\n",
              "      <td>0.468333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32</td>\n",
              "      <td>1.612700</td>\n",
              "      <td>1.609175</td>\n",
              "      <td>0.544167</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36</td>\n",
              "      <td>1.613900</td>\n",
              "      <td>1.608804</td>\n",
              "      <td>0.613333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>1.610200</td>\n",
              "      <td>1.606958</td>\n",
              "      <td>0.699167</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>44</td>\n",
              "      <td>1.605200</td>\n",
              "      <td>1.594761</td>\n",
              "      <td>0.733333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>48</td>\n",
              "      <td>1.568400</td>\n",
              "      <td>1.510854</td>\n",
              "      <td>0.741667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>52</td>\n",
              "      <td>1.394500</td>\n",
              "      <td>1.352067</td>\n",
              "      <td>0.740000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>56</td>\n",
              "      <td>1.248100</td>\n",
              "      <td>1.238872</td>\n",
              "      <td>0.705833</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>1.129300</td>\n",
              "      <td>1.109902</td>\n",
              "      <td>0.762500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>64</td>\n",
              "      <td>1.083100</td>\n",
              "      <td>1.033297</td>\n",
              "      <td>0.801667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>68</td>\n",
              "      <td>1.025200</td>\n",
              "      <td>0.961891</td>\n",
              "      <td>0.797500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>72</td>\n",
              "      <td>0.969400</td>\n",
              "      <td>0.864585</td>\n",
              "      <td>0.805000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>76</td>\n",
              "      <td>0.963600</td>\n",
              "      <td>0.799810</td>\n",
              "      <td>0.826667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>0.927800</td>\n",
              "      <td>0.813943</td>\n",
              "      <td>0.793333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>84</td>\n",
              "      <td>0.901900</td>\n",
              "      <td>0.737798</td>\n",
              "      <td>0.844167</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>88</td>\n",
              "      <td>0.918800</td>\n",
              "      <td>0.721176</td>\n",
              "      <td>0.838333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>92</td>\n",
              "      <td>0.863900</td>\n",
              "      <td>0.697639</td>\n",
              "      <td>0.847500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>96</td>\n",
              "      <td>0.863500</td>\n",
              "      <td>0.678374</td>\n",
              "      <td>0.849167</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.844200</td>\n",
              "      <td>0.662344</td>\n",
              "      <td>0.854167</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>104</td>\n",
              "      <td>0.849200</td>\n",
              "      <td>0.650922</td>\n",
              "      <td>0.846667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>108</td>\n",
              "      <td>0.849300</td>\n",
              "      <td>0.666129</td>\n",
              "      <td>0.845833</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>112</td>\n",
              "      <td>0.818000</td>\n",
              "      <td>0.670130</td>\n",
              "      <td>0.856667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>116</td>\n",
              "      <td>0.833600</td>\n",
              "      <td>0.685325</td>\n",
              "      <td>0.841667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>0.824400</td>\n",
              "      <td>0.623818</td>\n",
              "      <td>0.864167</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>124</td>\n",
              "      <td>0.802800</td>\n",
              "      <td>0.613468</td>\n",
              "      <td>0.861667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>128</td>\n",
              "      <td>0.805500</td>\n",
              "      <td>0.606031</td>\n",
              "      <td>0.860833</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>132</td>\n",
              "      <td>0.811000</td>\n",
              "      <td>0.610264</td>\n",
              "      <td>0.864167</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>136</td>\n",
              "      <td>0.792100</td>\n",
              "      <td>0.593863</td>\n",
              "      <td>0.869167</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>0.778500</td>\n",
              "      <td>0.586585</td>\n",
              "      <td>0.865000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>144</td>\n",
              "      <td>0.784300</td>\n",
              "      <td>0.601220</td>\n",
              "      <td>0.865833</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>148</td>\n",
              "      <td>0.753300</td>\n",
              "      <td>0.618683</td>\n",
              "      <td>0.865833</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>152</td>\n",
              "      <td>0.763300</td>\n",
              "      <td>0.592089</td>\n",
              "      <td>0.872500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>156</td>\n",
              "      <td>0.786300</td>\n",
              "      <td>0.574287</td>\n",
              "      <td>0.874167</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>160</td>\n",
              "      <td>0.760100</td>\n",
              "      <td>0.582599</td>\n",
              "      <td>0.868333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>164</td>\n",
              "      <td>0.748300</td>\n",
              "      <td>0.568143</td>\n",
              "      <td>0.884167</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>168</td>\n",
              "      <td>0.718000</td>\n",
              "      <td>0.561597</td>\n",
              "      <td>0.881667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>172</td>\n",
              "      <td>0.762500</td>\n",
              "      <td>0.560524</td>\n",
              "      <td>0.881667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>176</td>\n",
              "      <td>0.778100</td>\n",
              "      <td>0.576306</td>\n",
              "      <td>0.883333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>180</td>\n",
              "      <td>0.723000</td>\n",
              "      <td>0.567770</td>\n",
              "      <td>0.883333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>184</td>\n",
              "      <td>0.796700</td>\n",
              "      <td>0.564954</td>\n",
              "      <td>0.884167</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>188</td>\n",
              "      <td>0.780800</td>\n",
              "      <td>0.569764</td>\n",
              "      <td>0.889167</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>192</td>\n",
              "      <td>0.765400</td>\n",
              "      <td>0.561940</td>\n",
              "      <td>0.886667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>196</td>\n",
              "      <td>0.721400</td>\n",
              "      <td>0.561687</td>\n",
              "      <td>0.881667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.793300</td>\n",
              "      <td>0.559993</td>\n",
              "      <td>0.890000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>204</td>\n",
              "      <td>0.729800</td>\n",
              "      <td>0.565605</td>\n",
              "      <td>0.890000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>208</td>\n",
              "      <td>0.735200</td>\n",
              "      <td>0.552545</td>\n",
              "      <td>0.890000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>212</td>\n",
              "      <td>0.708800</td>\n",
              "      <td>0.550933</td>\n",
              "      <td>0.879167</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>216</td>\n",
              "      <td>0.749600</td>\n",
              "      <td>0.546954</td>\n",
              "      <td>0.887500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>220</td>\n",
              "      <td>0.774700</td>\n",
              "      <td>0.561924</td>\n",
              "      <td>0.889167</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>224</td>\n",
              "      <td>0.754600</td>\n",
              "      <td>0.567622</td>\n",
              "      <td>0.893333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>228</td>\n",
              "      <td>0.721600</td>\n",
              "      <td>0.549424</td>\n",
              "      <td>0.891667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>232</td>\n",
              "      <td>0.710800</td>\n",
              "      <td>0.541981</td>\n",
              "      <td>0.891667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>236</td>\n",
              "      <td>0.699000</td>\n",
              "      <td>0.542211</td>\n",
              "      <td>0.887500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>240</td>\n",
              "      <td>0.754900</td>\n",
              "      <td>0.544198</td>\n",
              "      <td>0.892500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>244</td>\n",
              "      <td>0.705500</td>\n",
              "      <td>0.547107</td>\n",
              "      <td>0.895833</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>248</td>\n",
              "      <td>0.705700</td>\n",
              "      <td>0.543887</td>\n",
              "      <td>0.895833</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>252</td>\n",
              "      <td>0.676900</td>\n",
              "      <td>0.538303</td>\n",
              "      <td>0.893333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>256</td>\n",
              "      <td>0.719000</td>\n",
              "      <td>0.537248</td>\n",
              "      <td>0.890000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>260</td>\n",
              "      <td>0.736500</td>\n",
              "      <td>0.540158</td>\n",
              "      <td>0.892500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>264</td>\n",
              "      <td>0.697900</td>\n",
              "      <td>0.541695</td>\n",
              "      <td>0.893333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>268</td>\n",
              "      <td>0.694300</td>\n",
              "      <td>0.540133</td>\n",
              "      <td>0.893333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>272</td>\n",
              "      <td>0.732500</td>\n",
              "      <td>0.540096</td>\n",
              "      <td>0.890833</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>276</td>\n",
              "      <td>0.703500</td>\n",
              "      <td>0.539693</td>\n",
              "      <td>0.890833</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>280</td>\n",
              "      <td>0.673100</td>\n",
              "      <td>0.538206</td>\n",
              "      <td>0.893333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>284</td>\n",
              "      <td>0.735500</td>\n",
              "      <td>0.537967</td>\n",
              "      <td>0.888333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>288</td>\n",
              "      <td>0.715800</td>\n",
              "      <td>0.538372</td>\n",
              "      <td>0.888333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>292</td>\n",
              "      <td>0.686000</td>\n",
              "      <td>0.538843</td>\n",
              "      <td>0.890833</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>296</td>\n",
              "      <td>0.705100</td>\n",
              "      <td>0.539096</td>\n",
              "      <td>0.890833</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.692700</td>\n",
              "      <td>0.539707</td>\n",
              "      <td>0.893333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>304</td>\n",
              "      <td>0.679300</td>\n",
              "      <td>0.539091</td>\n",
              "      <td>0.893333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>308</td>\n",
              "      <td>0.678000</td>\n",
              "      <td>0.537924</td>\n",
              "      <td>0.890833</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>312</td>\n",
              "      <td>0.725300</td>\n",
              "      <td>0.537242</td>\n",
              "      <td>0.890833</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>316</td>\n",
              "      <td>0.692200</td>\n",
              "      <td>0.537021</td>\n",
              "      <td>0.890833</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>320</td>\n",
              "      <td>0.685900</td>\n",
              "      <td>0.536810</td>\n",
              "      <td>0.890833</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>324</td>\n",
              "      <td>0.706600</td>\n",
              "      <td>0.536581</td>\n",
              "      <td>0.890833</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>328</td>\n",
              "      <td>0.702700</td>\n",
              "      <td>0.536583</td>\n",
              "      <td>0.891667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>332</td>\n",
              "      <td>0.682200</td>\n",
              "      <td>0.536670</td>\n",
              "      <td>0.891667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>336</td>\n",
              "      <td>0.718900</td>\n",
              "      <td>0.536652</td>\n",
              "      <td>0.891667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>340</td>\n",
              "      <td>0.676100</td>\n",
              "      <td>0.536508</td>\n",
              "      <td>0.891667</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=DataCollatorForMultipleChoice(tokenizer=tokenizer),\n",
        "    train_dataset=tokenized_dataset,\n",
        "    eval_dataset=tokenized_dataset_valid,\n",
        "    compute_metrics = compute_metrics,\n",
        "    callbacks=[SavePeftModelCallback] if USE_PEFT else None,\n",
        "    # resume\n",
        "    #callbacks=[EarlyStoppingCallback(early_stopping_patience=5)],\n",
        ")\n",
        "\n",
        "# trainer.train(resume_from_checkpoint = True)\n",
        "trainer.train()\n",
        "\n",
        "\n",
        "if USE_PEFT:\n",
        "    trainer.model.save_pretrained(f'model_v{VER}') # 我改了这个\n",
        "else:\n",
        "    trainer.save_model(f'model_v{VER}')\n",
        "\n",
        "# I think I read from some parts of the discussion that some length of input during training could be changed.\n",
        "# Training longer during training will hopefully cover the length of even the longest sentence in testing.\n",
        "# This is some problem with model extrapolation.\n",
        "# Basically, if the model is not using very good positional encoding, it will perform badly in sequences longer than what it has been trained on."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8VhfWy4jpOXb"
      },
      "source": [
        "# Verify Saved Model\n",
        "During training, we see the MAP@3 validation score above. Let's load the saved model and compute it again here to verify that our model is saved correctly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-02T02:42:35.053535Z",
          "iopub.status.idle": "2023-09-02T02:42:35.054036Z",
          "shell.execute_reply": "2023-09-02T02:42:35.053806Z",
          "shell.execute_reply.started": "2023-09-02T02:42:35.053782Z"
        },
        "id": "K6Ko_1V9pOXb",
        "outputId": "fff68db6-d5b1-41e8-f8d2-f32d9285940e",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loading peft\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of DebertaV2ForMultipleChoice were not initialized from the model checkpoint at microsoft/deberta-v3-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight', 'classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "/home/dijkstraz/anaconda3/envs/kaggle/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:470: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
            "  warnings.warn(\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: './checkpoints_repeat_V3/checkpoint-30/torch_model/pytorch_model.bin'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m/home/dijkstraz/ScienceExam/working_new/jason_how_to_train_open_book_model.ipynb Cell 25\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/dijkstraz/ScienceExam/working_new/jason_how_to_train_open_book_model.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m model \u001b[39m=\u001b[39m get_peft_model(model, peft_config)\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/dijkstraz/ScienceExam/working_new/jason_how_to_train_open_book_model.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m# checkpoint = torch.load(f'model_v{VER}/adapter_model.bin')\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/dijkstraz/ScienceExam/working_new/jason_how_to_train_open_book_model.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m# checkpoint = torch.load(f'./checkpoints_3/checkpoint-160/adapter_model.bin')\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/dijkstraz/ScienceExam/working_new/jason_how_to_train_open_book_model.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39m# checkpoint = torch.load(f'./checkpoints_3/checkpoint-224/adapter_model.bin')\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/dijkstraz/ScienceExam/working_new/jason_how_to_train_open_book_model.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m# checkpoint = torch.load(f'./checkpoints_5/checkpoint-3900/adapter_model.bin')\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/dijkstraz/ScienceExam/working_new/jason_how_to_train_open_book_model.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m checkpoint \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mload(\u001b[39mf\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m./checkpoints_\u001b[39;49m\u001b[39m{\u001b[39;49;00mVER\u001b[39m}\u001b[39;49;00m\u001b[39m/checkpoint-30/torch_model/pytorch_model.bin\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/dijkstraz/ScienceExam/working_new/jason_how_to_train_open_book_model.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m model\u001b[39m.\u001b[39mbase_model\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mload_state_dict(checkpoint)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/dijkstraz/ScienceExam/working_new/jason_how_to_train_open_book_model.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39m# print('loading state dict')\u001b[39;00m\n",
            "File \u001b[0;32m/home/dijkstraz/anaconda3/envs/kaggle/lib/python3.10/site-packages/torch/serialization.py:791\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[1;32m    788\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m'\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m pickle_load_args\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m    789\u001b[0m     pickle_load_args[\u001b[39m'\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m--> 791\u001b[0m \u001b[39mwith\u001b[39;00m _open_file_like(f, \u001b[39m'\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m opened_file:\n\u001b[1;32m    792\u001b[0m     \u001b[39mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m    793\u001b[0m         \u001b[39m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m    794\u001b[0m         \u001b[39m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m    795\u001b[0m         \u001b[39m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m    796\u001b[0m         orig_position \u001b[39m=\u001b[39m opened_file\u001b[39m.\u001b[39mtell()\n",
            "File \u001b[0;32m/home/dijkstraz/anaconda3/envs/kaggle/lib/python3.10/site-packages/torch/serialization.py:271\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    270\u001b[0m     \u001b[39mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 271\u001b[0m         \u001b[39mreturn\u001b[39;00m _open_file(name_or_buffer, mode)\n\u001b[1;32m    272\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    273\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mw\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m mode:\n",
            "File \u001b[0;32m/home/dijkstraz/anaconda3/envs/kaggle/lib/python3.10/site-packages/torch/serialization.py:252\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, name, mode):\n\u001b[0;32m--> 252\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39mopen\u001b[39;49m(name, mode))\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './checkpoints_repeat_V3/checkpoint-30/torch_model/pytorch_model.bin'"
          ]
        }
      ],
      "source": [
        "del model, trainer\n",
        "from peft import get_peft_model, set_peft_model_state_dict\n",
        "if USE_PEFT:\n",
        "    print('loading peft')\n",
        "    model = AutoModelForMultipleChoice.from_pretrained(MODEL)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
        "    model = get_peft_model(model, peft_config)\n",
        "    # checkpoint = torch.load(f'model_v{VER}/adapter_model.bin')\n",
        "    # checkpoint = torch.load(f'./checkpoints_3/checkpoint-160/adapter_model.bin')\n",
        "    # checkpoint = torch.load(f'./checkpoints_3/checkpoint-224/adapter_model.bin')\n",
        "    # checkpoint = torch.load(f'./checkpoints_5/checkpoint-3900/adapter_model.bin')\n",
        "\n",
        "    checkpoint = torch.load(f'./checkpoints_{VER}/checkpoint-30/torch_model/pytorch_model.bin')\n",
        "    model.base_model.model.load_state_dict(checkpoint)\n",
        "\n",
        "\n",
        "    # print('loading state dict')\n",
        "    set_peft_model_state_dict(model, checkpoint)\n",
        "    # if FREEZE_EMBEDDINGS:\n",
        "    #     print('Freezing embeddings.')\n",
        "    #     for param in model.deberta.embeddings.parameters():\n",
        "    #         param.requires_grad = False\n",
        "    # if FREEZE_LAYERS>0:\n",
        "    #     print(f'Freezing {FREEZE_LAYERS} layers.')\n",
        "    #     for layer in model.deberta.encoder.layer[:FREEZE_LAYERS]:\n",
        "    #         for param in layer.parameters():\n",
        "    #             param.requires_grad = False\n",
        "    #     # Newly added for v3\n",
        "    #     for layer in model.deberta.encoder.layer[FREEZE_LAYERS:]:\n",
        "    #         for param in layer.parameters():\n",
        "    #             param.requires_grad = True\n",
        "    model.eval()\n",
        "    model.print_trainable_parameters()\n",
        "else:\n",
        "    model = AutoModelForMultipleChoice.from_pretrained(f'model_v{VER}')\n",
        "trainer = Trainer(model=model,\n",
        "                data_collator=DataCollatorForMultipleChoice(tokenizer=tokenizer),\n",
        "                tokenizer=tokenizer,\n",
        "                train_dataset=tokenized_dataset,\n",
        "                eval_dataset=tokenized_dataset_valid,\n",
        "                compute_metrics = compute_metrics,)\n",
        "# trainer.evaluate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "ee0430024c5b4ceeb15a2e1048e074d9"
          ]
        },
        "execution": {
          "iopub.status.busy": "2023-09-02T02:42:35.055773Z",
          "iopub.status.idle": "2023-09-02T02:42:35.056299Z",
          "shell.execute_reply": "2023-09-02T02:42:35.056073Z",
          "shell.execute_reply.started": "2023-09-02T02:42:35.056015Z"
        },
        "id": "t9XwQPoLpOXb",
        "outputId": "2cd4121b-4fee-4d68-da11-9dbf006e4b1f",
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ee0430024c5b4ceeb15a2e1048e074d9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "MAX_INPUT = 512\n",
        "# MAX_INPUT = 256\n",
        "\n",
        "from functools import partial\n",
        "test_df = pd.read_csv('../input/60k-data-with-context-v2/train_with_context2.csv')\n",
        "\n",
        "# tokenized_dataset_valid = dataset_valid.map(preprocess, remove_columns=['prompt', 'context', 'A', 'B', 'C', 'D', 'E', 'answer'])\n",
        "tokenized_test_dataset = Dataset.from_pandas(test_df).map(\n",
        "        preprocess, remove_columns=['prompt', 'context', 'A', 'B', 'C', 'D', 'E'])\n",
        "\n",
        "# with torch.no_grad():\n",
        "test_predictions = trainer.predict(tokenized_test_dataset).predictions\n",
        "predictions_as_ids = np.argsort(-test_predictions, 1)\n",
        "predictions_as_answer_letters = np.array(list('ABCDE'))[predictions_as_ids]\n",
        "predictions_as_string = test_df['prediction'] = [\n",
        "    ' '.join(row) for row in predictions_as_answer_letters[:, :3]\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O1dkTbA7pOXb"
      },
      "source": [
        "法1：\n",
        "1. 通过 question 在 wiki 中 进行 context matching，用 sentence embedding 的 cosine similarity，得到 k 个 context，Ctx\n",
        "2. 通过 answer 在 Ctx 中进行 sentence similarity matching, 将每个答案的 similarity 进行 normalize, 当作先验概率（prior probability）\n",
        "3. 以训练过的方式，让模型给logits，然后用 sonormalize 得到后验概率（posterior probability）\n",
        "4. 先后 相乘，然后 softmax，准备ensemble\n",
        "\n",
        "法2：\n",
        "1. 通过 question 在 wiki 中 进行 context matching，用 sentence embedding 的 cosine similarity，得到 k 个 context，Ctx\n",
        "2. 通过某种方法分解提取答案，然后将分解出来的 entity 拿到 context 中，计算 context 每个句子的权重，挑出最重要的 s 个句子，以他们在wiki 中出现的顺序喂给模型"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oK-tb269pOXb",
        "outputId": "26017790-b467-45b1-8120-4230f8824004"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "512\n",
            "[CLS] Eunice Fay McKenzie (February 19, 1918 – April 16, 2019) was an American actress and singer. She also entertained the troops with her former screen partner, Gene Autry. ===Later career=== After World War II, McKenzie retired from films to raise her two children. She was briefly billed as Fay Shannon. ==Biography== ===Early life and silent film=== McKenzie was born on February 19, 1918, in Hollywood, California, to show business parents, film actor Eva (née Heazlitt) and Irish American actor/director Robert McKenzie.Mike Fitzgerald, \"An Interview with... She starred in silent films as a child, and then sound films as an adult, but perhaps she is best known for her leading roles opposite Gene Autry in the early 1940s in five horse opera features. Fay's sister Ida Mae McKenzie, cousin Ella McKenzie, and brother-in-law Billy Gilbert, were also actors. McKenzie sang duets with Autry in each of these films. Ida Mae also played the character of Sarah Lincoln in The Dramatic Life of Abraham Lincoln, in the part of the film where she had become a teenager. ===Schooling=== In the mid-1920s, McKenzie took a ten-year break from acting in order to focus on her education. Her father had a stock company called the McKenzie Merry Makers, and was both an actor and director in stage productions and films. McKenzie later recalled, ===Sound films=== McKenzie appeared in numerous uncredited roles throughout the 1930s, with occasional credited roles in films such as The Boss Cowboy (1934) as Sally Nolan, and the anti-cannabis propaganda film Assassin of Youth (1937) as Linda Clayton. She later remembered: ===Theater and touring=== During World War II, McKenzie left Republic Pictures to work in theater and pursue other projects. McKenzie went on to appear in four additional Autry films as his leading lady: Sierra Sue (1941) as Sue Larrabee, Cowboy Serenade (1942) as Stephanie Lock, Heart of the Rio Grande (1942) as Alice Bennett, and Home in Wyomin' (1942) as Clementine Benson. Fay McKenzie\", Western Clippings. In 1938, she began to appear mainly in Western films, such as Ghost Town Riders (1938) #### In relation to Eunice Fay McKenzie's career, which statement accurately reflects her most notable work?[SEP] McKenzie showcased her singing talents in numerous musical productions, garnering critical acclaim.[SEP]\n",
            "512\n",
            "[CLS] Eunice Fay McKenzie (February 19, 1918 – April 16, 2019) was an American actress and singer. She also entertained the troops with her former screen partner, Gene Autry. ===Later career=== After World War II, McKenzie retired from films to raise her two children. She was briefly billed as Fay Shannon. ==Biography== ===Early life and silent film=== McKenzie was born on February 19, 1918, in Hollywood, California, to show business parents, film actor Eva (née Heazlitt) and Irish American actor/director Robert McKenzie.Mike Fitzgerald, \"An Interview with... She starred in silent films as a child, and then sound films as an adult, but perhaps she is best known for her leading roles opposite Gene Autry in the early 1940s in five horse opera features. Fay's sister Ida Mae McKenzie, cousin Ella McKenzie, and brother-in-law Billy Gilbert, were also actors. McKenzie sang duets with Autry in each of these films. Ida Mae also played the character of Sarah Lincoln in The Dramatic Life of Abraham Lincoln, in the part of the film where she had become a teenager. ===Schooling=== In the mid-1920s, McKenzie took a ten-year break from acting in order to focus on her education. Her father had a stock company called the McKenzie Merry Makers, and was both an actor and director in stage productions and films. McKenzie later recalled, ===Sound films=== McKenzie appeared in numerous uncredited roles throughout the 1930s, with occasional credited roles in films such as The Boss Cowboy (1934) as Sally Nolan, and the anti-cannabis propaganda film Assassin of Youth (1937) as Linda Clayton. She later remembered: ===Theater and touring=== During World War II, McKenzie left Republic Pictures to work in theater and pursue other projects. McKenzie went on to appear in four additional Autry films as his leading lady: Sierra Sue (1941) as Sue Larrabee, Cowboy Serenade (1942) as Stephanie Lock, Heart of the Rio Grande (1942) as Alice Bennett, and Home in Wyomin' (1942) as Clementine Benson. Fay McKenzie\", Western Clippings. In 1938, she began to appear mainly in Western films, such as #### In relation to Eunice Fay McKenzie's career, which statement accurately reflects her most notable work?[SEP] McKenzie is primarily remembered for her starring roles opposite Gene Autry in popular Western films of the 1940s.[SEP]\n",
            "512\n",
            "[CLS] Eunice Fay McKenzie (February 19, 1918 – April 16, 2019) was an American actress and singer. She also entertained the troops with her former screen partner, Gene Autry. ===Later career=== After World War II, McKenzie retired from films to raise her two children. She was briefly billed as Fay Shannon. ==Biography== ===Early life and silent film=== McKenzie was born on February 19, 1918, in Hollywood, California, to show business parents, film actor Eva (née Heazlitt) and Irish American actor/director Robert McKenzie.Mike Fitzgerald, \"An Interview with... She starred in silent films as a child, and then sound films as an adult, but perhaps she is best known for her leading roles opposite Gene Autry in the early 1940s in five horse opera features. Fay's sister Ida Mae McKenzie, cousin Ella McKenzie, and brother-in-law Billy Gilbert, were also actors. McKenzie sang duets with Autry in each of these films. Ida Mae also played the character of Sarah Lincoln in The Dramatic Life of Abraham Lincoln, in the part of the film where she had become a teenager. ===Schooling=== In the mid-1920s, McKenzie took a ten-year break from acting in order to focus on her education. Her father had a stock company called the McKenzie Merry Makers, and was both an actor and director in stage productions and films. McKenzie later recalled, ===Sound films=== McKenzie appeared in numerous uncredited roles throughout the 1930s, with occasional credited roles in films such as The Boss Cowboy (1934) as Sally Nolan, and the anti-cannabis propaganda film Assassin of Youth (1937) as Linda Clayton. She later remembered: ===Theater and touring=== During World War II, McKenzie left Republic Pictures to work in theater and pursue other projects. McKenzie went on to appear in four additional Autry films as his leading lady: Sierra Sue (1941) as Sue Larrabee, Cowboy Serenade (1942) as Stephanie Lock, Heart of the Rio Grande (1942) as Alice Bennett, and Home in Wyomin' (1942) as Clementine Benson. Fay McKenzie\", Western Clippings. In 1938, she began to appear mainly in Western films, such as Ghost Town #### In relation to Eunice Fay McKenzie's career, which statement accurately reflects her most notable work?[SEP] McKenzie gained recognition for her role as a child actress in a series of iconic silent films.[SEP]\n",
            "512\n",
            "[CLS] Eunice Fay McKenzie (February 19, 1918 – April 16, 2019) was an American actress and singer. She also entertained the troops with her former screen partner, Gene Autry. ===Later career=== After World War II, McKenzie retired from films to raise her two children. She was briefly billed as Fay Shannon. ==Biography== ===Early life and silent film=== McKenzie was born on February 19, 1918, in Hollywood, California, to show business parents, film actor Eva (née Heazlitt) and Irish American actor/director Robert McKenzie.Mike Fitzgerald, \"An Interview with... She starred in silent films as a child, and then sound films as an adult, but perhaps she is best known for her leading roles opposite Gene Autry in the early 1940s in five horse opera features. Fay's sister Ida Mae McKenzie, cousin Ella McKenzie, and brother-in-law Billy Gilbert, were also actors. McKenzie sang duets with Autry in each of these films. Ida Mae also played the character of Sarah Lincoln in The Dramatic Life of Abraham Lincoln, in the part of the film where she had become a teenager. ===Schooling=== In the mid-1920s, McKenzie took a ten-year break from acting in order to focus on her education. Her father had a stock company called the McKenzie Merry Makers, and was both an actor and director in stage productions and films. McKenzie later recalled, ===Sound films=== McKenzie appeared in numerous uncredited roles throughout the 1930s, with occasional credited roles in films such as The Boss Cowboy (1934) as Sally Nolan, and the anti-cannabis propaganda film Assassin of Youth (1937) as Linda Clayton. She later remembered: ===Theater and touring=== During World War II, McKenzie left Republic Pictures to work in theater and pursue other projects. McKenzie went on to appear in four additional Autry films as his leading lady: Sierra Sue (1941) as Sue Larrabee, Cowboy Serenade (1942) as Stephanie Lock, Heart of the Rio Grande (1942) as Alice Bennett, and Home in Wyomin' (1942) as Clementine Benson. Fay McKenzie\", Western Clippings. In 1938, she began to appear mainly in Western films, such as Ghost Town Riders ( #### In relation to Eunice Fay McKenzie's career, which statement accurately reflects her most notable work?[SEP] McKenzie's collaborations with director Blake Edwards were instrumental in her rise to fame.[SEP]\n",
            "512\n",
            "[CLS] Eunice Fay McKenzie (February 19, 1918 – April 16, 2019) was an American actress and singer. She also entertained the troops with her former screen partner, Gene Autry. ===Later career=== After World War II, McKenzie retired from films to raise her two children. She was briefly billed as Fay Shannon. ==Biography== ===Early life and silent film=== McKenzie was born on February 19, 1918, in Hollywood, California, to show business parents, film actor Eva (née Heazlitt) and Irish American actor/director Robert McKenzie.Mike Fitzgerald, \"An Interview with... She starred in silent films as a child, and then sound films as an adult, but perhaps she is best known for her leading roles opposite Gene Autry in the early 1940s in five horse opera features. Fay's sister Ida Mae McKenzie, cousin Ella McKenzie, and brother-in-law Billy Gilbert, were also actors. McKenzie sang duets with Autry in each of these films. Ida Mae also played the character of Sarah Lincoln in The Dramatic Life of Abraham Lincoln, in the part of the film where she had become a teenager. ===Schooling=== In the mid-1920s, McKenzie took a ten-year break from acting in order to focus on her education. Her father had a stock company called the McKenzie Merry Makers, and was both an actor and director in stage productions and films. McKenzie later recalled, ===Sound films=== McKenzie appeared in numerous uncredited roles throughout the 1930s, with occasional credited roles in films such as The Boss Cowboy (1934) as Sally Nolan, and the anti-cannabis propaganda film Assassin of Youth (1937) as Linda Clayton. She later remembered: ===Theater and touring=== During World War II, McKenzie left Republic Pictures to work in theater and pursue other projects. McKenzie went on to appear in four additional Autry films as his leading lady: Sierra Sue (1941) as Sue Larrabee, Cowboy Serenade (1942) as Stephanie Lock, Heart of the Rio Grande (1942) as Alice Bennett, and Home in Wyomin' (1942) as Clementine Benson. Fay McKenzie\", Western Clippings. In 1938, she began to appear mainly in Western films, such as #### In relation to Eunice Fay McKenzie's career, which statement accurately reflects her most notable work?[SEP] McKenzie's successful career in sound films continued into adulthood, becoming known for her versatile acting abilities.[SEP]\n"
          ]
        }
      ],
      "source": [
        "for x in tokenized_dataset[0]['input_ids']:\n",
        "    print(len(x))\n",
        "    print(tokenizer.decode(x)) # 许需要加一个 句子修剪\n",
        "    # print(x)\n",
        "# I think tokenized_dataset[0]['input_ids'] is a good example for checking the model's prediction\n",
        "# 因为它的 information retrieval 刚好找到了正确的context，因此需要看看模型是否能从这个context中获取正确答案。\n",
        "# 还应该 check 一下其他的 retrieval 里面，有多少 context 是相关的。\n",
        "# 能不能做一个 multi-hop retrieval:\n",
        "#\n",
        "#"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DwlrU9JqpOXb"
      },
      "source": [
        "# Compute Validation Score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-02T02:42:35.05812Z",
          "iopub.status.idle": "2023-09-02T02:42:35.059239Z",
          "shell.execute_reply": "2023-09-02T02:42:35.058895Z",
          "shell.execute_reply.started": "2023-09-02T02:42:35.058829Z"
        },
        "id": "tWk5T_rTpOXc",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# https://www.kaggle.com/code/philippsinger/h2ogpt-perplexity-ranking\n",
        "import numpy as np\n",
        "def precision_at_k(r, k):\n",
        "    \"\"\"Precision at k\"\"\"\n",
        "    assert k <= len(r)\n",
        "    assert k != 0\n",
        "    return sum(int(x) for x in r[:k]) / k\n",
        "\n",
        "def MAP_at_3(predictions, true_items):\n",
        "    \"\"\"Score is mean average precision at 3\"\"\"\n",
        "    U = len(predictions)\n",
        "    map_at_3 = 0.0\n",
        "    for u in range(U):\n",
        "        user_preds = predictions[u].split()\n",
        "        user_true = true_items[u]\n",
        "        user_results = [1 if item == user_true else 0 for item in user_preds]\n",
        "        for k in range(min(len(user_preds), 3)):\n",
        "            map_at_3 += precision_at_k(user_results, k+1) * user_results[k]\n",
        "    return map_at_3 / U"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-02T02:42:35.062122Z",
          "iopub.status.idle": "2023-09-02T02:42:35.06267Z",
          "shell.execute_reply": "2023-09-02T02:42:35.062401Z",
          "shell.execute_reply.started": "2023-09-02T02:42:35.062377Z"
        },
        "id": "Z639mdHopOXc",
        "outputId": "9925c861-1281-456c-9063-fd5d9a465b1d",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CV MAP@3 = 0.6274999999999998\n"
          ]
        }
      ],
      "source": [
        "m = MAP_at_3(test_df.prediction.values, test_df.answer.values)\n",
        "print( 'CV MAP@3 =',m )"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
